<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://cppalliance.org/feed.xml" rel="self" type="application/atom+xml" /><link href="http://cppalliance.org/" rel="alternate" type="text/html" /><updated>2021-06-09T16:03:15+00:00</updated><id>http://cppalliance.org/feed.xml</id><title type="html">The C++ Alliance</title><subtitle>The C++ Alliance is dedicated to helping the C++ programming language evolve. We see it developing as an ecosystem of open source libraries and as a growing community of those who contribute to those libraries..</subtitle><entry><title type="html">Richard’s May 2021 Update</title><link href="http://cppalliance.org/richard/2021/05/30/RichardsMayUpdate.html" rel="alternate" type="text/html" title="Richard’s May 2021 Update" /><published>2021-05-30T00:00:00+00:00</published><updated>2021-05-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/05/30/RichardsMayUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/05/30/RichardsMayUpdate.html">&lt;h1 id=&quot;the-month-in-review&quot;&gt;The Month in Review&lt;/h1&gt;

&lt;p&gt;It’s been a month of minor maintenance fixes, and a fair amount of support requests via
the &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;C++ Alliance Slack workspace&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;property_tree&quot;&gt;Property_tree&lt;/h2&gt;

&lt;p&gt;On the maintenance front, there are a number of historic pull requests in the property_tree repo which need working
through. Some of these take some unravelling and a degree of care, as I am still new to this venerable library and I
have the impression that it is used fairly ubiquitously in many code bases of varying pedigree.&lt;/p&gt;

&lt;p&gt;Currently I have no way of reaching out to users (not knowing exactly who they are) so the only way to know whether a
change is going to break someone’s build is to release it, by which time it is too late.&lt;/p&gt;

&lt;p&gt;I think the answer here is to start building out more test cases. Property_tree only recently gained CI, and so far I
have not gotten around to adding test coverage. No doubt I’ll get to this in due course.&lt;/p&gt;

&lt;h2 id=&quot;beast&quot;&gt;Beast&lt;/h2&gt;

&lt;p&gt;There are a lot of eager developers out there keen to use Beast and Asio, which is encouraging. The less encouraging
thing is the amount of time I find myself spending giving ad-hoc support to people who have hit the Asio mental brick
wall (which I remember when learning this fantastic library all too well).&lt;/p&gt;

&lt;p&gt;I have written blogs in this series before covering some of the topics I think are important and developers often
misunderstand, but there is more to do.&lt;/p&gt;

&lt;p&gt;With this in mind, an idea has been germinating over the past few months, which finally started to develop into a new
library this month. I’ll come back to this later.&lt;/p&gt;

&lt;h2 id=&quot;asio&quot;&gt;Asio&lt;/h2&gt;

&lt;p&gt;A few months ago I attended a WG21 meeting where a formal means of providing cancellation to asynchronous operations was
proposed. A few people at that meeting, myself included, were concerned that the proposal in its current form would
constrain the development style of asynchronous programs, making the fundamental objects a little more complex than they
often need to be.&lt;/p&gt;

&lt;p&gt;I have recognised that Asio needs a formal task cancellation mechanism for some time, this being the basis of the async
cancellation_token mentioned in a previous blog.&lt;/p&gt;

&lt;p&gt;I have been able to get some of Chris Kohlhoff’s valuable time to discuss this to see whether there is a way to get
effortless cancellation into Asio without impacting performance or compiled size when cancellation is not required.&lt;/p&gt;

&lt;p&gt;Chris, as he is wont to do, made the rather brilliant connection that in Asio, a 1-shot cancellation token can be
associated with each asynchronous completion handler, with the default token type being a zero-cost abstraction of a
null cancellation token - i.e. one that will never invoke the stop callback.&lt;/p&gt;

&lt;p&gt;The general idea being that if you want an operation to be cancellable, you would invoke it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// This is the signal object that you would use to 
// cancel any operation that depends on one of its slots
asio::cancellation_signal sig; 

// some IO Object
timer t(ioc, chronons::seconds(5));

// perform an asynchronous operation bound to the cancellation signal
t.async_wait(
    bind_cancellation_slot(sig.slot(),
      [](system::error_code ec)
      {
        // if the signal is invoked, the timer's asynchronous operation will notice
        // and the operation will complete with ec equal to asio::errors::operation_aborted
      });
      
// signal the cancellation
sig.emit():
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The interesting thing about this is that the cancellation slot is associated with the asynchronous operation’s handler.
This is not only useful for a library-provided asynchronous operation such as a timer wait. Because of the existence of
a function called &lt;code&gt;get_associated_cancellation_slot(handler)&lt;/code&gt;, the current slot is available in any context where user
code has access to the current asynchronous completion handler.&lt;/p&gt;

&lt;p&gt;One such place is in a user-defined composed operation, and therefore by extension, a c++ coroutine running in the
context of an Asio executor.&lt;/p&gt;

&lt;p&gt;This now becomes possible:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;
asio::awaitable&amp;lt;void&amp;gt;
my_coro(some_async_op&amp;amp; op)
{
    // The cancellation state allowed us to detect whether cancellation has been requested
    // It also allows us to register our own cancellation slot
    auto cs = asio::this_coro::cancellation_state;
    
    // Create a new slot from the cancellation state and register a callback which will 
    // invoke our own custom cancel signal on the some_async_op
    // note: A
    auto slot = cs.slot();
    slot.emplace([&amp;amp;]{ op.cancel(); });

    // continue to wait on the some_async_op
    co_await op.wait_to_finish();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This coroutine could be invoked in a couple of ways:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;
// In this case the cancellation state is a no-op cancellation. 
// the code at note A above will do nothing. This coroutine is not cancellable.
co_await asio::co_spawn(exec, 
                        my_coro(op), 
                        asio::use_awaitable);

// In this case, the coroutine has become cancellable because the code at note A will actually
// create a functioning slot and register the lambda.
// The coroutine is cancellable through the cancellation signal sig.
asio::cancellation_signal sig;
co_await asio::co_spawn(exec, 
                        my_coro(op), 
                        asio::bind_cancellation_signal(
                            asio::use_awaitable, sig));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code is experimental at the moment, but is available&lt;br /&gt;
&lt;a href=&quot;https://github.com/boostorg/asio/tree/generic-associators&quot;&gt;on the generic-associators branch of Boost.Asio&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;the-project-du-jour&quot;&gt;The Project du Jour&lt;/h1&gt;

&lt;p&gt;Coming back to the “Asio is hard at the beginning” meme, I was speaking to my son recently. He works with a number of
languages, including Python, Go and C++.&lt;/p&gt;

&lt;p&gt;During a conversation about these he mentioned that Go was a very uninspiring language (to him) but it was very easy to
get fairly complex asynchronous programs functioning reliably in a short amount of time.&lt;/p&gt;

&lt;p&gt;I asked him what the single most effective feature of the language was, to which he replied, “channels”.&lt;/p&gt;

&lt;p&gt;For anyone who does not already know, a golang channel is simply a multi-producer, multi-consumer ring buffer with an
asynchronous interface.&lt;/p&gt;

&lt;p&gt;It has the following behaviour:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Producer coroutines will suspend when providing values to the channel if the ring buffer is full and there is no
consumer pending a consume operation.&lt;/li&gt;
  &lt;li&gt;Consumer coroutines will suspend when consuming if the ring buffer is empty and there is no pending producer operation
in progress.&lt;/li&gt;
  &lt;li&gt;The ring buffer capacity is specified upon construction, and may be zero. Producers and consumers of a zero-sized
channel will only make progress if there is a corresponding pair of producer and consumer pending at the same time. In
this way, the channel also acts as a coroutine synchronisation primitive.&lt;/li&gt;
  &lt;li&gt;Finally, the channel may be closed. A closed channel will allow a consumer to consume remaining values in the ring
buffer, but it will not allow a producer to provide more values, whether into the ring buffer or directly to a pending
consume operation. A consume operation against an empty, closed channel will yield a default-constructed object plus a
boolean false indicating that there are no more values to consume.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some other nice features in Go, such as the select keyword which interact with channels in a pleasing way, but
for now I’ll focus on how we might implement the channel in asynchronous C++.&lt;/p&gt;

&lt;p&gt;The rationale here being:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Channels make writing complex asynchronous interactions simple.&lt;/li&gt;
  &lt;li&gt;Make simple things simple is the mantra to which I subscribe.&lt;/li&gt;
  &lt;li&gt;Perhaps C++ enthusiasts would benefit from an implementation of channels.&lt;/li&gt;
  &lt;li&gt;Given the flexibility of C++, we might be able to do a better job than Go, at least in terms of giving the programmer
some choice over implementation tradeoffs.&lt;/li&gt;
  &lt;li&gt;Maybe a little library offering this functionality in a simple, reusable way would be a useful addition to Boost.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I put some feelers out in the CppLang slack. So far the response to the idea has been only positive. So I decided to
make a start.&lt;/p&gt;

&lt;p&gt;TLDR - you can monitor how far I am getting by looking at
the &lt;a href=&quot;https://github.com/madmongo1/boost_channels&quot;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;I wanted the channels library to be built on top of Asio. The reason for this is that I happen to think that the Asio
executor model is very elegant, and allows the programmer to transpose the same fundamental idea onto a number of
different concurrency strategies. For example, thread pools, IO loop, threads and futures, and so on.&lt;/p&gt;

&lt;p&gt;Asio’s completion tokens allow the adaptation of asynchronous initiating functions to any or all of these strategies and
I wanted to make sure that the library will provide this functionality.&lt;/p&gt;

&lt;p&gt;Furthermore, asynchronous programs become complex quickly. Asio is a natural fit for IO, but does not provide the
primitives that programmers often find they need to create rich programs.&lt;/p&gt;

&lt;p&gt;It is my hope that this channels library provides people with a useful tool to make high performance, highly concurrent
programs easier to write in C++.&lt;/p&gt;

&lt;h2 id=&quot;design-decisions&quot;&gt;Design Decisions&lt;/h2&gt;

&lt;p&gt;I have elected to write library in two sections. The first will contain the basic objects to handle the concurrent
communication and asynchronous completions. These objects will not be thread-safe, just like any other object in Asio.&lt;/p&gt;

&lt;p&gt;The second will be a thread-safe interface written in terms of the first. The truth is that Asio objects do not need to
be thread-safe if programmers use the correct discipline vis-a-vis strands and ensuring that work is dispatched to the
correct strand. Another truth is that many programmers just want things to be easy. So why not provide an easy-mode
interface too?&lt;/p&gt;

&lt;h2 id=&quot;comparison&quot;&gt;Comparison&lt;/h2&gt;

&lt;p&gt;OK, so let’s take a simple Go program and see how we could express that in terms of Asio and C++ coroutines. Now I’m no
expert, so I’m sure there are many ways to improve this program. It’s about the third Go program I’ve ever written.
Please by all means let me know.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;fmt&quot;
	&quot;sync&quot;
)

func produce(wg *sync.WaitGroup, c chan&amp;lt;- string) {
	defer wg.Done()
	c &amp;lt;- &quot;The&quot;
	c &amp;lt;- &quot;cat&quot;
	c &amp;lt;- &quot;sat&quot;
	c &amp;lt;- &quot;on&quot;
	c &amp;lt;- &quot;the&quot;
	c &amp;lt;- &quot;mat&quot;
	close(c)
}

func consume(wg *sync.WaitGroup, name string, c &amp;lt;-chan string) {
	defer wg.Done()
	for {
		s, more := &amp;lt;-c
		if more {
			fmt.Println(name, &quot;:&quot;, s)
		} else {
			fmt.Println(name, &quot;: Channel closed&quot;, name)
			break
		}
	}
}

// Main function
func main() {
	var wg sync.WaitGroup
	wg.Add(4)
	c := make(chan string)
	go consume(&amp;amp;wg, &quot;a&quot;, c)
	go consume(&amp;amp;wg, &quot;b&quot;, c)
	go consume(&amp;amp;wg, &quot;c&quot;, c)
	go produce(&amp;amp;wg, c)
	wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is how I would envision it would look in the first cut of the C++ version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;auto
produce(channels::channel&amp;lt; std::string &amp;gt; &amp;amp;c) 
    -&amp;gt; asio::awaitable&amp;lt; void &amp;gt;
{
    constexpr auto wait = asio::use_awaitable;
    co_await c.async_send(&quot;The&quot;, wait);
    co_await c.async_send(&quot;cat&quot;, wait);
    co_await c.async_send(&quot;sat&quot;, wait);
    co_await c.async_send(&quot;on&quot;, wait);
    co_await c.async_send(&quot;the&quot;, wait);
    co_await c.async_send(&quot;mat&quot;, wait);
    c.close();
}

auto
consume(std::string_view name, channels::channel&amp;lt; std::string &amp;gt; &amp;amp;c)
    -&amp;gt; asio::awaitable&amp;lt; void &amp;gt;
{
    auto ec  = channels::error_code();
    auto tok = asio::redirect_error(asio::use_awaitable, ec);
    for (;;)
    {
        auto s = co_await c.async_consume(tok);
        if (ec)
        {
            std::cout &amp;lt;&amp;lt; name &amp;lt;&amp;lt; &quot; : &quot; &amp;lt;&amp;lt; ec.message() &amp;lt;&amp;lt; &quot;\n&quot;;
            break;
        }
        else
            std::cout &amp;lt;&amp;lt; name &amp;lt;&amp;lt; &quot; : &quot; &amp;lt;&amp;lt; s &amp;lt;&amp;lt; &quot;\n&quot;;
    }
}

int
main()
{
    auto ioc = asio::io_context();
    auto c   = channels::channel&amp;lt; std::string &amp;gt;(ioc.get_executor());

    asio::co_spawn(ioc, consume(&quot;a&quot;, c), asio::detached);
    asio::co_spawn(ioc, consume(&quot;b&quot;, c), asio::detached);
    asio::co_spawn(ioc, consume(&quot;c&quot;, c), asio::detached);
    asio::co_spawn(ioc, produce(c), asio::detached);

    ioc.run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One example of the output of the Go program (the order is actually nondeterministic) is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;a : The
a : cat
b : sat
b : mat
b : Channel closed b
a : on
a : Channel closed a
c : the
c : Channel closed c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;while the output of the C++ program is a more deterministic:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;a : The
b : cat
c : sat
a : on
b : the
c : mat
a : Channel is closed
b : Channel is closed
c : Channel is closed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I’m not an expert in Go by any means but I imagine the nondeterminism in the Go program is in part due to the fact that
the goroutine implementation is allowed to take shortcuts to consume data synchronously if it’s available. The Asio
model requires that each completion handler is invoked as-if by a call to &lt;code&gt;post(handler)&lt;/code&gt;. In this program, these posts
are being made to a single-threaded io_context and so are being executed sequentially, preserving the order of
invocation during execution.&lt;/p&gt;

&lt;p&gt;If this program were multi-threaded, it might be a different story. But this will have to wait until the basic
single-threaded implementation is complete.&lt;/p&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation Details&lt;/h2&gt;

&lt;p&gt;The implementation of the channel is actually fairly straightforward. The asynchronous initiation interfaces are
standard asio, e.g.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template &amp;lt; class ValueType, class Executor &amp;gt;
template &amp;lt; BOOST_ASIO_COMPLETION_TOKEN_FOR(void(error_code)) SendHandler &amp;gt;
BOOST_ASIO_INITFN_RESULT_TYPE(SendHandler, void(error_code))
channel&amp;lt; ValueType, Executor &amp;gt;::async_send(value_type    value,
                                           SendHandler &amp;amp;&amp;amp;token)
{
    if (!impl_) [[unlikely]]
        BOOST_THROW_EXCEPTION(std::logic_error(&quot;channel is null&quot;));

    return asio::async_initiate&amp;lt; SendHandler, void(error_code) &amp;gt;(
        [value = std::move(value), this](auto &amp;amp;&amp;amp;handler) {
            auto send_op = detail::create_channel_send_op(
                std::move(value),
                this-&amp;gt;impl_-&amp;gt;get_executor(),
                std::forward&amp;lt; decltype(handler) &amp;gt;(handler));
            impl_-&amp;gt;notify_send(send_op);
        },
        token);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The macros are supplied by Asio and simply ensure that the most up-to-date compiler facilities are used to ensure that
the completion token/handler has the correct signature. &lt;code&gt;BOOST_ASIO_INITFN_RESULT_TYPE&lt;/code&gt; deduces the return type of the
selected specialisation of &lt;code&gt;async_initiate&lt;/code&gt;. It is what ensures that &lt;code&gt;async_send&lt;/code&gt; returns an awaitable when the
completion token is of type &lt;code&gt;asio::use_awaitable&lt;/code&gt;, or a &lt;code&gt;std::future&lt;/code&gt; if we were to pass in &lt;code&gt;asio::use_future&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The actual work of the send is performed in the implementation class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    notify_send(detail::channel_send_op_concept&amp;lt; ValueType &amp;gt; *send_op)
    {
        // behaviour of send depends on the state of the implementation.
        // There are two states, running and closed. We will be in the closed
        // state if someone has called `close` on the channel.
        // Note that even if the channel is closed, consumers may still consume
        // values stored in the circular buffer. However, new values may not
        // be send into the channel.
        switch (state_)
        {
        case state_running:
            [[likely]] if (consumers_.empty())
            {
                // In the case that there is no consumer already waiting,
                // then behaviour depends on whether there is space in the
                // circular buffer. If so, we store the value in the send_op
                // there and allow the send_op to complete.
                // Otherwise, we store the send_op in the queue of pending
                // send operations for later processing when there is space in
                // the circular buffer or a pending consume is available.
                if (free())
                    push(send_op-&amp;gt;consume());
                else
                    senders_.push(send_op);
            }
            else
            {
                // A consumer is waiting, so we can unblock the consumer
                // by passing it the value in the send_op, causing both
                // send and consume to complete.
                auto my_receiver = std::move(consumers_.front());
                consumers_.pop();
                my_receiver-&amp;gt;notify_value(send_op-&amp;gt;consume());
            }
            break;
        case state_closed:
            // If the channel is closed, then all send operations result in
            // an error
            [[unlikely]] send_op-&amp;gt;notify_error(errors::channel_closed);
            break;
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An interesting feature of the send operation class is that when it is instructed to complete, it must:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Move the value out of itself,&lt;/li&gt;
  &lt;li&gt;Move the completion handler out of itself,&lt;/li&gt;
  &lt;li&gt;Destroy itself, returning memory back to the allocator.&lt;/li&gt;
  &lt;li&gt;Post the completion handler to the correct executor.&lt;/li&gt;
  &lt;li&gt;Return the value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The order is important. Later on we will be adding Asio allocator awareness. In order to maximise efficiency, Asio
asynchronous operations must free their memory back to the allocator before completing. This is so that during the
execution of the completion handler, the same memory that was just freed into asio’s special purpose allocators will be
allocated and used to compose the next completion handler. This memory will be at the head of the allocator’s list of
free blocks (and therefore found first) and it will be in cached memory, having just been touched.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template &amp;lt; class ValueType, class Executor, class Handler &amp;gt;
auto
basic_channel_send_op&amp;lt; ValueType, Executor, Handler &amp;gt;::consume() -&amp;gt; ValueType
{
    // move the result value to the local scope
    auto result  = std::move(this-&amp;gt;value_);
    
    // move the handler to local scope and transform it to be associated with
    // the correct executor.
    auto handler = ::boost::asio::bind_executor(
        std::move(exec_),
        [handler = std::move(handler_)]() mutable { handler(error_code()); });
    
    // then destroy this object (equivalent to delete this)
    destroy();
    
    // post the modified handler to its associated executor
    asio::post(std::move(handler));
    
    // return the value from the local scope to the caller (but note that NRVO
    // will guarantee that there is not actually a second move)
    return result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That’s all for now. I’ll add extra blog entries as and when I make any significant progress to the library.&lt;/p&gt;

&lt;p&gt;In the meantime, I’m always happy to receive queries by email or as issues in the github repo.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;

&lt;p&gt;Richard Hodges&lt;br /&gt;
for C++ Alliance&lt;br /&gt;
&lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;hodges.r@gmail.com&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">The Month in Review It’s been a month of minor maintenance fixes, and a fair amount of support requests via the C++ Alliance Slack workspace. Property_tree On the maintenance front, there are a number of historic pull requests in the property_tree repo which need working through. Some of these take some unravelling and a degree of care, as I am still new to this venerable library and I have the impression that it is used fairly ubiquitously in many code bases of varying pedigree. Currently I have no way of reaching out to users (not knowing exactly who they are) so the only way to know whether a change is going to break someone’s build is to release it, by which time it is too late. I think the answer here is to start building out more test cases. Property_tree only recently gained CI, and so far I have not gotten around to adding test coverage. No doubt I’ll get to this in due course. Beast There are a lot of eager developers out there keen to use Beast and Asio, which is encouraging. The less encouraging thing is the amount of time I find myself spending giving ad-hoc support to people who have hit the Asio mental brick wall (which I remember when learning this fantastic library all too well). I have written blogs in this series before covering some of the topics I think are important and developers often misunderstand, but there is more to do. With this in mind, an idea has been germinating over the past few months, which finally started to develop into a new library this month. I’ll come back to this later. Asio A few months ago I attended a WG21 meeting where a formal means of providing cancellation to asynchronous operations was proposed. A few people at that meeting, myself included, were concerned that the proposal in its current form would constrain the development style of asynchronous programs, making the fundamental objects a little more complex than they often need to be. I have recognised that Asio needs a formal task cancellation mechanism for some time, this being the basis of the async cancellation_token mentioned in a previous blog. I have been able to get some of Chris Kohlhoff’s valuable time to discuss this to see whether there is a way to get effortless cancellation into Asio without impacting performance or compiled size when cancellation is not required. Chris, as he is wont to do, made the rather brilliant connection that in Asio, a 1-shot cancellation token can be associated with each asynchronous completion handler, with the default token type being a zero-cost abstraction of a null cancellation token - i.e. one that will never invoke the stop callback. The general idea being that if you want an operation to be cancellable, you would invoke it like this: // This is the signal object that you would use to // cancel any operation that depends on one of its slots asio::cancellation_signal sig; // some IO Object timer t(ioc, chronons::seconds(5)); // perform an asynchronous operation bound to the cancellation signal t.async_wait( bind_cancellation_slot(sig.slot(), [](system::error_code ec) { // if the signal is invoked, the timer's asynchronous operation will notice // and the operation will complete with ec equal to asio::errors::operation_aborted }); // signal the cancellation sig.emit(): The interesting thing about this is that the cancellation slot is associated with the asynchronous operation’s handler. This is not only useful for a library-provided asynchronous operation such as a timer wait. Because of the existence of a function called get_associated_cancellation_slot(handler), the current slot is available in any context where user code has access to the current asynchronous completion handler. One such place is in a user-defined composed operation, and therefore by extension, a c++ coroutine running in the context of an Asio executor. This now becomes possible: asio::awaitable&amp;lt;void&amp;gt; my_coro(some_async_op&amp;amp; op) { // The cancellation state allowed us to detect whether cancellation has been requested // It also allows us to register our own cancellation slot auto cs = asio::this_coro::cancellation_state; // Create a new slot from the cancellation state and register a callback which will // invoke our own custom cancel signal on the some_async_op // note: A auto slot = cs.slot(); slot.emplace([&amp;amp;]{ op.cancel(); }); // continue to wait on the some_async_op co_await op.wait_to_finish(); } This coroutine could be invoked in a couple of ways: // In this case the cancellation state is a no-op cancellation. // the code at note A above will do nothing. This coroutine is not cancellable. co_await asio::co_spawn(exec, my_coro(op), asio::use_awaitable); // In this case, the coroutine has become cancellable because the code at note A will actually // create a functioning slot and register the lambda. // The coroutine is cancellable through the cancellation signal sig. asio::cancellation_signal sig; co_await asio::co_spawn(exec, my_coro(op), asio::bind_cancellation_signal( asio::use_awaitable, sig)); This code is experimental at the moment, but is available on the generic-associators branch of Boost.Asio. The Project du Jour Coming back to the “Asio is hard at the beginning” meme, I was speaking to my son recently. He works with a number of languages, including Python, Go and C++. During a conversation about these he mentioned that Go was a very uninspiring language (to him) but it was very easy to get fairly complex asynchronous programs functioning reliably in a short amount of time. I asked him what the single most effective feature of the language was, to which he replied, “channels”. For anyone who does not already know, a golang channel is simply a multi-producer, multi-consumer ring buffer with an asynchronous interface. It has the following behaviour: Producer coroutines will suspend when providing values to the channel if the ring buffer is full and there is no consumer pending a consume operation. Consumer coroutines will suspend when consuming if the ring buffer is empty and there is no pending producer operation in progress. The ring buffer capacity is specified upon construction, and may be zero. Producers and consumers of a zero-sized channel will only make progress if there is a corresponding pair of producer and consumer pending at the same time. In this way, the channel also acts as a coroutine synchronisation primitive. Finally, the channel may be closed. A closed channel will allow a consumer to consume remaining values in the ring buffer, but it will not allow a producer to provide more values, whether into the ring buffer or directly to a pending consume operation. A consume operation against an empty, closed channel will yield a default-constructed object plus a boolean false indicating that there are no more values to consume. There are some other nice features in Go, such as the select keyword which interact with channels in a pleasing way, but for now I’ll focus on how we might implement the channel in asynchronous C++. The rationale here being: Channels make writing complex asynchronous interactions simple. Make simple things simple is the mantra to which I subscribe. Perhaps C++ enthusiasts would benefit from an implementation of channels. Given the flexibility of C++, we might be able to do a better job than Go, at least in terms of giving the programmer some choice over implementation tradeoffs. Maybe a little library offering this functionality in a simple, reusable way would be a useful addition to Boost. I put some feelers out in the CppLang slack. So far the response to the idea has been only positive. So I decided to make a start. TLDR - you can monitor how far I am getting by looking at the Github repository. Approach I wanted the channels library to be built on top of Asio. The reason for this is that I happen to think that the Asio executor model is very elegant, and allows the programmer to transpose the same fundamental idea onto a number of different concurrency strategies. For example, thread pools, IO loop, threads and futures, and so on. Asio’s completion tokens allow the adaptation of asynchronous initiating functions to any or all of these strategies and I wanted to make sure that the library will provide this functionality. Furthermore, asynchronous programs become complex quickly. Asio is a natural fit for IO, but does not provide the primitives that programmers often find they need to create rich programs. It is my hope that this channels library provides people with a useful tool to make high performance, highly concurrent programs easier to write in C++. Design Decisions I have elected to write library in two sections. The first will contain the basic objects to handle the concurrent communication and asynchronous completions. These objects will not be thread-safe, just like any other object in Asio. The second will be a thread-safe interface written in terms of the first. The truth is that Asio objects do not need to be thread-safe if programmers use the correct discipline vis-a-vis strands and ensuring that work is dispatched to the correct strand. Another truth is that many programmers just want things to be easy. So why not provide an easy-mode interface too? Comparison OK, so let’s take a simple Go program and see how we could express that in terms of Asio and C++ coroutines. Now I’m no expert, so I’m sure there are many ways to improve this program. It’s about the third Go program I’ve ever written. Please by all means let me know. package main import ( &quot;fmt&quot; &quot;sync&quot; ) func produce(wg *sync.WaitGroup, c chan&amp;lt;- string) { defer wg.Done() c &amp;lt;- &quot;The&quot; c &amp;lt;- &quot;cat&quot; c &amp;lt;- &quot;sat&quot; c &amp;lt;- &quot;on&quot; c &amp;lt;- &quot;the&quot; c &amp;lt;- &quot;mat&quot; close(c) } func consume(wg *sync.WaitGroup, name string, c &amp;lt;-chan string) { defer wg.Done() for { s, more := &amp;lt;-c if more { fmt.Println(name, &quot;:&quot;, s) } else { fmt.Println(name, &quot;: Channel closed&quot;, name) break } } } // Main function func main() { var wg sync.WaitGroup wg.Add(4) c := make(chan string) go consume(&amp;amp;wg, &quot;a&quot;, c) go consume(&amp;amp;wg, &quot;b&quot;, c) go consume(&amp;amp;wg, &quot;c&quot;, c) go produce(&amp;amp;wg, c) wg.Wait() } And this is how I would envision it would look in the first cut of the C++ version: auto produce(channels::channel&amp;lt; std::string &amp;gt; &amp;amp;c) -&amp;gt; asio::awaitable&amp;lt; void &amp;gt; { constexpr auto wait = asio::use_awaitable; co_await c.async_send(&quot;The&quot;, wait); co_await c.async_send(&quot;cat&quot;, wait); co_await c.async_send(&quot;sat&quot;, wait); co_await c.async_send(&quot;on&quot;, wait); co_await c.async_send(&quot;the&quot;, wait); co_await c.async_send(&quot;mat&quot;, wait); c.close(); } auto consume(std::string_view name, channels::channel&amp;lt; std::string &amp;gt; &amp;amp;c) -&amp;gt; asio::awaitable&amp;lt; void &amp;gt; { auto ec = channels::error_code(); auto tok = asio::redirect_error(asio::use_awaitable, ec); for (;;) { auto s = co_await c.async_consume(tok); if (ec) { std::cout &amp;lt;&amp;lt; name &amp;lt;&amp;lt; &quot; : &quot; &amp;lt;&amp;lt; ec.message() &amp;lt;&amp;lt; &quot;\n&quot;; break; } else std::cout &amp;lt;&amp;lt; name &amp;lt;&amp;lt; &quot; : &quot; &amp;lt;&amp;lt; s &amp;lt;&amp;lt; &quot;\n&quot;; } } int main() { auto ioc = asio::io_context(); auto c = channels::channel&amp;lt; std::string &amp;gt;(ioc.get_executor()); asio::co_spawn(ioc, consume(&quot;a&quot;, c), asio::detached); asio::co_spawn(ioc, consume(&quot;b&quot;, c), asio::detached); asio::co_spawn(ioc, consume(&quot;c&quot;, c), asio::detached); asio::co_spawn(ioc, produce(c), asio::detached); ioc.run(); } One example of the output of the Go program (the order is actually nondeterministic) is: a : The a : cat b : sat b : mat b : Channel closed b a : on a : Channel closed a c : the c : Channel closed c while the output of the C++ program is a more deterministic: a : The b : cat c : sat a : on b : the c : mat a : Channel is closed b : Channel is closed c : Channel is closed I’m not an expert in Go by any means but I imagine the nondeterminism in the Go program is in part due to the fact that the goroutine implementation is allowed to take shortcuts to consume data synchronously if it’s available. The Asio model requires that each completion handler is invoked as-if by a call to post(handler). In this program, these posts are being made to a single-threaded io_context and so are being executed sequentially, preserving the order of invocation during execution. If this program were multi-threaded, it might be a different story. But this will have to wait until the basic single-threaded implementation is complete. Implementation Details The implementation of the channel is actually fairly straightforward. The asynchronous initiation interfaces are standard asio, e.g.: template &amp;lt; class ValueType, class Executor &amp;gt; template &amp;lt; BOOST_ASIO_COMPLETION_TOKEN_FOR(void(error_code)) SendHandler &amp;gt; BOOST_ASIO_INITFN_RESULT_TYPE(SendHandler, void(error_code)) channel&amp;lt; ValueType, Executor &amp;gt;::async_send(value_type value, SendHandler &amp;amp;&amp;amp;token) { if (!impl_) [[unlikely]] BOOST_THROW_EXCEPTION(std::logic_error(&quot;channel is null&quot;)); return asio::async_initiate&amp;lt; SendHandler, void(error_code) &amp;gt;( [value = std::move(value), this](auto &amp;amp;&amp;amp;handler) { auto send_op = detail::create_channel_send_op( std::move(value), this-&amp;gt;impl_-&amp;gt;get_executor(), std::forward&amp;lt; decltype(handler) &amp;gt;(handler)); impl_-&amp;gt;notify_send(send_op); }, token); } The macros are supplied by Asio and simply ensure that the most up-to-date compiler facilities are used to ensure that the completion token/handler has the correct signature. BOOST_ASIO_INITFN_RESULT_TYPE deduces the return type of the selected specialisation of async_initiate. It is what ensures that async_send returns an awaitable when the completion token is of type asio::use_awaitable, or a std::future if we were to pass in asio::use_future. The actual work of the send is performed in the implementation class: void notify_send(detail::channel_send_op_concept&amp;lt; ValueType &amp;gt; *send_op) { // behaviour of send depends on the state of the implementation. // There are two states, running and closed. We will be in the closed // state if someone has called `close` on the channel. // Note that even if the channel is closed, consumers may still consume // values stored in the circular buffer. However, new values may not // be send into the channel. switch (state_) { case state_running: [[likely]] if (consumers_.empty()) { // In the case that there is no consumer already waiting, // then behaviour depends on whether there is space in the // circular buffer. If so, we store the value in the send_op // there and allow the send_op to complete. // Otherwise, we store the send_op in the queue of pending // send operations for later processing when there is space in // the circular buffer or a pending consume is available. if (free()) push(send_op-&amp;gt;consume()); else senders_.push(send_op); } else { // A consumer is waiting, so we can unblock the consumer // by passing it the value in the send_op, causing both // send and consume to complete. auto my_receiver = std::move(consumers_.front()); consumers_.pop(); my_receiver-&amp;gt;notify_value(send_op-&amp;gt;consume()); } break; case state_closed: // If the channel is closed, then all send operations result in // an error [[unlikely]] send_op-&amp;gt;notify_error(errors::channel_closed); break; } } An interesting feature of the send operation class is that when it is instructed to complete, it must: Move the value out of itself, Move the completion handler out of itself, Destroy itself, returning memory back to the allocator. Post the completion handler to the correct executor. Return the value. The order is important. Later on we will be adding Asio allocator awareness. In order to maximise efficiency, Asio asynchronous operations must free their memory back to the allocator before completing. This is so that during the execution of the completion handler, the same memory that was just freed into asio’s special purpose allocators will be allocated and used to compose the next completion handler. This memory will be at the head of the allocator’s list of free blocks (and therefore found first) and it will be in cached memory, having just been touched. template &amp;lt; class ValueType, class Executor, class Handler &amp;gt; auto basic_channel_send_op&amp;lt; ValueType, Executor, Handler &amp;gt;::consume() -&amp;gt; ValueType { // move the result value to the local scope auto result = std::move(this-&amp;gt;value_); // move the handler to local scope and transform it to be associated with // the correct executor. auto handler = ::boost::asio::bind_executor( std::move(exec_), [handler = std::move(handler_)]() mutable { handler(error_code()); }); // then destroy this object (equivalent to delete this) destroy(); // post the modified handler to its associated executor asio::post(std::move(handler)); // return the value from the local scope to the caller (but note that NRVO // will guarantee that there is not actually a second move) return result; } That’s all for now. I’ll add extra blog entries as and when I make any significant progress to the library. In the meantime, I’m always happy to receive queries by email or as issues in the github repo. Thanks for reading. Richard Hodges for C++ Alliance hodges.r@gmail.com</summary></entry><entry><title type="html">Richard’s April Update</title><link href="http://cppalliance.org/richard/2021/04/30/RichardsAprilUpdate.html" rel="alternate" type="text/html" title="Richard’s April Update" /><published>2021-04-30T00:00:00+00:00</published><updated>2021-04-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/04/30/RichardsAprilUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/04/30/RichardsAprilUpdate.html">&lt;h1 id=&quot;a-new-responsibility&quot;&gt;A new responsibility&lt;/h1&gt;

&lt;p&gt;Last month I took over the maintenance of the Boost Property Tree library.&lt;/p&gt;

&lt;p&gt;This library is in ubiquitous use but has been without a full time maintainer for some time. 
There was a backlog of commits on develop that had not made it to master for the boost releases. 
This is changing in upcoming 1.76 release. Property Tree PRs and issues will get more attention going forward.
In addition, Property Tree has gained CI courtesy of GitHub Actions. The test status can be seen 
&lt;a href=&quot;https://github.com/boostorg/property_tree&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;beast-ci&quot;&gt;Beast CI&lt;/h1&gt;

&lt;p&gt;Beast has acquired two new continuous integration pipelines. Drone and Github Actions.
Drone is the canonical source of truth for the Linux/Windows badge on the
&lt;a href=&quot;https://github.com/boostorg/beast&quot;&gt;readme page&lt;/a&gt; while GitHub Actions provides a more comprehensive matrix of
targets, C++ standards and compiler versions which I use during the development cycle.&lt;/p&gt;

&lt;h1 id=&quot;boost-version-176&quot;&gt;Boost Version 1.76&lt;/h1&gt;

&lt;p&gt;The 1.76 release of boost is imminent. As far as Beast and Property Tree is concerned, this is a maintenance release.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">A new responsibility Last month I took over the maintenance of the Boost Property Tree library. This library is in ubiquitous use but has been without a full time maintainer for some time. There was a backlog of commits on develop that had not made it to master for the boost releases. This is changing in upcoming 1.76 release. Property Tree PRs and issues will get more attention going forward. In addition, Property Tree has gained CI courtesy of GitHub Actions. The test status can be seen here Beast CI Beast has acquired two new continuous integration pipelines. Drone and Github Actions. Drone is the canonical source of truth for the Linux/Windows badge on the readme page while GitHub Actions provides a more comprehensive matrix of targets, C++ standards and compiler versions which I use during the development cycle. Boost Version 1.76 The 1.76 release of boost is imminent. As far as Beast and Property Tree is concerned, this is a maintenance release.</summary></entry><entry><title type="html">Richard’s February/March Update</title><link href="http://cppalliance.org/richard/2021/03/30/RichardsMarchUpdate.html" rel="alternate" type="text/html" title="Richard’s February/March Update" /><published>2021-03-30T00:00:00+00:00</published><updated>2021-03-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/03/30/RichardsMarchUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/03/30/RichardsMarchUpdate.html">&lt;h1 id=&quot;boostpropertytree-is-back-in-the-saddle&quot;&gt;Boost.PropertyTree is back in the saddle!&lt;/h1&gt;

&lt;p&gt;Last month I took over the maintenance of the Boost Property Tree library.&lt;/p&gt;

&lt;p&gt;This library is in ubiquitous use but has been without a full time maintainer for some time. 
There was a backlog of commits on develop that had not made it to master for the boost releases. 
This is changing in upcoming 1.76 release. Property Tree PRs and issues will get more attention going forward.
In addition, Property Tree has gained CI courtesy of GitHub Actions. The test status can be seen 
&lt;a href=&quot;https://github.com/boostorg/property_tree&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;beast-ci&quot;&gt;Beast CI&lt;/h1&gt;

&lt;p&gt;Beast has acquired two new continuous integration pipelines. Drone and Github Actions.
Drone is the canonical source of truth for the Linux/Windows badge on the
&lt;a href=&quot;https://github.com/boostorg/beast&quot;&gt;readme page&lt;/a&gt; while GitHub Actions provides a more comprehensive matrix of
targets, C++ standards and compiler versions which I use during the development cycle.&lt;/p&gt;

&lt;h1 id=&quot;boost-version-176&quot;&gt;Boost Version 1.76&lt;/h1&gt;

&lt;p&gt;The 1.76 release of boost is imminent. As far as Beast and Property Tree is concerned, this is a maintenance release.&lt;/p&gt;

&lt;h1 id=&quot;other-activities&quot;&gt;Other Activities&lt;/h1&gt;

&lt;p&gt;Much of the past two months has been devoted to resolving user queries, and examining suggestions for changes to Beast 
and Property Tree.&lt;/p&gt;

&lt;p&gt;Changes to Property tree are not taken likely. It is used in a great deal of legacy code in the wild and there are few 
tests.
It would be a shame to break existing code noisily, or worse, silently.
For better or worse, the behaviour is probably not going to change very much going forward until there are more tests in 
place.&lt;/p&gt;

&lt;p&gt;Accepting changes to beast’s behaviour or interface is something we consider very carefully for a different reason.
Beast is already a complex library. Like Asio upon which it is built, it is already extremely configurable, with a
myriad (actually potentially an unbounded set) of completion tokens, completion handlers and buffer types.&lt;/p&gt;

&lt;p&gt;Something I am often asked is whether we would consider a “multi-send” interface. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;std::vector&amp;lt;std::string&amp;gt; messages;
ws.write(buffer, messages);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Arguably there are some efficiencies available here, since we could potentially build all the resulting websocket frames
in one pass, and send as one IO operation.&lt;/p&gt;

&lt;p&gt;There are some issues however. One is buffer space. What do we do if the buffer runs out of space while be are half way 
through building the frames? Fail the operation, return a partial failure? Block?
Also, what do we do if the ensuing IO operation(s) partially fail(s)? Asio currently has no protocol to report a partial 
failure. We would have to create such a protocol, test it and then teach it.&lt;/p&gt;

&lt;p&gt;In reality people’s requirements are often different. Some people may require confirmation of each frame within a time 
period, some will want all-or-nothing delivery and some are happy with partial delivery.&lt;/p&gt;

&lt;p&gt;One approach is to start providing options for such things as timeouts, partial completion, etc. But the feeling here is
that this starts to dictate to users of the library how they write code, which we feel is outside the remit of Beast.&lt;/p&gt;

&lt;p&gt;Perhaps there is a case for a function that separates out the building of a websocket frame from the sending of it. 
I’ll give it some thought.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Boost.PropertyTree is back in the saddle! Last month I took over the maintenance of the Boost Property Tree library. This library is in ubiquitous use but has been without a full time maintainer for some time. There was a backlog of commits on develop that had not made it to master for the boost releases. This is changing in upcoming 1.76 release. Property Tree PRs and issues will get more attention going forward. In addition, Property Tree has gained CI courtesy of GitHub Actions. The test status can be seen here Beast CI Beast has acquired two new continuous integration pipelines. Drone and Github Actions. Drone is the canonical source of truth for the Linux/Windows badge on the readme page while GitHub Actions provides a more comprehensive matrix of targets, C++ standards and compiler versions which I use during the development cycle. Boost Version 1.76 The 1.76 release of boost is imminent. As far as Beast and Property Tree is concerned, this is a maintenance release. Other Activities Much of the past two months has been devoted to resolving user queries, and examining suggestions for changes to Beast and Property Tree. Changes to Property tree are not taken likely. It is used in a great deal of legacy code in the wild and there are few tests. It would be a shame to break existing code noisily, or worse, silently. For better or worse, the behaviour is probably not going to change very much going forward until there are more tests in place. Accepting changes to beast’s behaviour or interface is something we consider very carefully for a different reason. Beast is already a complex library. Like Asio upon which it is built, it is already extremely configurable, with a myriad (actually potentially an unbounded set) of completion tokens, completion handlers and buffer types. Something I am often asked is whether we would consider a “multi-send” interface. For example: std::vector&amp;lt;std::string&amp;gt; messages; ws.write(buffer, messages); Arguably there are some efficiencies available here, since we could potentially build all the resulting websocket frames in one pass, and send as one IO operation. There are some issues however. One is buffer space. What do we do if the buffer runs out of space while be are half way through building the frames? Fail the operation, return a partial failure? Block? Also, what do we do if the ensuing IO operation(s) partially fail(s)? Asio currently has no protocol to report a partial failure. We would have to create such a protocol, test it and then teach it. In reality people’s requirements are often different. Some people may require confirmation of each frame within a time period, some will want all-or-nothing delivery and some are happy with partial delivery. One approach is to start providing options for such things as timeouts, partial completion, etc. But the feeling here is that this starts to dictate to users of the library how they write code, which we feel is outside the remit of Beast. Perhaps there is a case for a function that separates out the building of a websocket frame from the sending of it. I’ll give it some thought.</summary></entry><entry><title type="html">Emil’s January Update</title><link href="http://cppalliance.org/emil/2021/02/20/EmilsJanuaryUpdate.html" rel="alternate" type="text/html" title="Emil’s January Update" /><published>2021-02-20T00:00:00+00:00</published><updated>2021-02-20T00:00:00+00:00</updated><id>http://cppalliance.org/emil/2021/02/20/EmilsJanuaryUpdate</id><content type="html" xml:base="http://cppalliance.org/emil/2021/02/20/EmilsJanuaryUpdate.html">&lt;h1 id=&quot;history&quot;&gt;History&lt;/h1&gt;

&lt;p&gt;This is my first entry on the C++ Alliance web site. I am happy to have been invited to join the organization.&lt;/p&gt;

&lt;p&gt;I first talked to Vinnie on Slack, and was surprised to find that he is practically a neighbor. So we got in touch, had breakfast together, talked about C++ and life, the universe and everything. We became friends, he took care of our cats when we flew to Europe (pre-Covid), and I brought back some &lt;a href=&quot;https://en.wikipedia.org/wiki/Rakia#Bulgaria&quot;&gt;Rakia&lt;/a&gt; (next time I’ll bring him some of my father’s home-made stuff!)&lt;/p&gt;

&lt;h1 id=&quot;boost-url&quot;&gt;(Boost) URL&lt;/h1&gt;

&lt;p&gt;My immediate focus after joining the C++ Alliance is to work on what will hopefully become Boost.URL. C++ has long been missing a solid standard-conforming library for working with URLs, we often get contacted by impatient users asking about the state of things.&lt;/p&gt;

&lt;p&gt;Building on the foundation already in place and requiring only C++11, the library is being developed to the highest standards of portability, safety and efficiency. We’re also careful to keep the design simple, with a lot of attention paid to minimizing physical coupling so that compilation is as fast as possible.&lt;/p&gt;

&lt;h1 id=&quot;about&quot;&gt;About&lt;/h1&gt;

&lt;p&gt;I started coding in middle school on the Bulgarian-made Apple ][ clone &lt;a href=&quot;https://www.zdnet.com/article/how-these-communist-era-apple-ii-clones-helped-shape-central-europes-it-sector/&quot;&gt;Правец-82&lt;/a&gt; (named after – you guessed it – the birth place of the leader of the Bulgarian Communist Party at the time).  This has resulted in a lot of useless information being engraved on my brain, I still remember some of the opcodes of the 6502 CPU. :)&lt;/p&gt;

&lt;p&gt;After graduating the Sofia University I moved to L.A. to work in the video game industry where I’ve spent most of my professional career, starting with the original Playstation all the way to modern consoles and handhelds. For the last few years I’ve shifted to projects that utilize my background in realtime graphics engines in areas that are not directly connected to gaming.&lt;/p&gt;

&lt;p&gt;Pretty much all of my projects have been written in C++. This includes several Boost libraries, the latest one being LEAF, a Lightweight Error Augmentation Framework for C++11. I’ll probably post about error handling at some later time, as this is one of my key areas of interest.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;

&lt;p&gt;Emil Dotchevski&lt;/p&gt;</content><author><name></name></author><category term="emil" /><summary type="html">History This is my first entry on the C++ Alliance web site. I am happy to have been invited to join the organization. I first talked to Vinnie on Slack, and was surprised to find that he is practically a neighbor. So we got in touch, had breakfast together, talked about C++ and life, the universe and everything. We became friends, he took care of our cats when we flew to Europe (pre-Covid), and I brought back some Rakia (next time I’ll bring him some of my father’s home-made stuff!) (Boost) URL My immediate focus after joining the C++ Alliance is to work on what will hopefully become Boost.URL. C++ has long been missing a solid standard-conforming library for working with URLs, we often get contacted by impatient users asking about the state of things. Building on the foundation already in place and requiring only C++11, the library is being developed to the highest standards of portability, safety and efficiency. We’re also careful to keep the design simple, with a lot of attention paid to minimizing physical coupling so that compilation is as fast as possible. About I started coding in middle school on the Bulgarian-made Apple ][ clone Правец-82 (named after – you guessed it – the birth place of the leader of the Bulgarian Communist Party at the time). This has resulted in a lot of useless information being engraved on my brain, I still remember some of the opcodes of the 6502 CPU. :) After graduating the Sofia University I moved to L.A. to work in the video game industry where I’ve spent most of my professional career, starting with the original Playstation all the way to modern consoles and handhelds. For the last few years I’ve shifted to projects that utilize my background in realtime graphics engines in areas that are not directly connected to gaming. Pretty much all of my projects have been written in C++. This includes several Boost libraries, the latest one being LEAF, a Lightweight Error Augmentation Framework for C++11. I’ll probably post about error handling at some later time, as this is one of my key areas of interest. Thanks for reading. Emil Dotchevski</summary></entry><entry><title type="html">Dmitry’s January Update</title><link href="http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update.html" rel="alternate" type="text/html" title="Dmitry’s January Update" /><published>2021-02-15T00:00:00+00:00</published><updated>2021-02-15T00:00:00+00:00</updated><id>http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update</id><content type="html" xml:base="http://cppalliance.org/dmitry/2021/02/15/dmitrys-january-update.html">&lt;h1 id=&quot;dmitrys-january-update&quot;&gt;Dmitry’s January Update&lt;/h1&gt;

&lt;p&gt;In January 2021 I started working on improving Boost.JSON for The C++ Alliance.
The time during this first month was mostly spent on getting familiar with the
project, particularly with its list of issues on GitHub.&lt;/p&gt;

&lt;p&gt;It turns out, half of the open issues are related to documentation. For
example, the section about conversions might need a rewrite, and related
entries in the reference need to provide more information. There should be more
examples for parsing customizations. There also needs to be a separate section
dedicated to library’s named requirements. There was also a bug in coversion
customization logic that was fixed by me this month.&lt;/p&gt;

&lt;p&gt;The next two large blocks are issues related to optimization opportunities and
dealing with floating point numbers (some issues are present in both groups).
The next group is issues related to build and continuous integration. A couple
of build bugs were fixed in January.&lt;/p&gt;

&lt;p&gt;The final group consists of feature requests, mostly for convenient ways to
access items inside &lt;code&gt;json::value&lt;/code&gt;. And this month I started implementing one
such feature – Json.Pointer support. The work is still in the early stages,
though.&lt;/p&gt;</content><author><name></name></author><category term="dmitry" /><summary type="html">Dmitry’s January Update In January 2021 I started working on improving Boost.JSON for The C++ Alliance. The time during this first month was mostly spent on getting familiar with the project, particularly with its list of issues on GitHub. It turns out, half of the open issues are related to documentation. For example, the section about conversions might need a rewrite, and related entries in the reference need to provide more information. There should be more examples for parsing customizations. There also needs to be a separate section dedicated to library’s named requirements. There was also a bug in coversion customization logic that was fixed by me this month. The next two large blocks are issues related to optimization opportunities and dealing with floating point numbers (some issues are present in both groups). The next group is issues related to build and continuous integration. A couple of build bugs were fixed in January. The final group consists of feature requests, mostly for convenient ways to access items inside json::value. And this month I started implementing one such feature – Json.Pointer support. The work is still in the early stages, though.</summary></entry><entry><title type="html">Richard’s January Update</title><link href="http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate.html" rel="alternate" type="text/html" title="Richard’s January Update" /><published>2021-01-31T00:00:00+00:00</published><updated>2021-01-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/01/31/RichardsJanuaryUpdate.html">&lt;h1 id=&quot;a-year-in-the-c-alliance&quot;&gt;A year in the C++ Alliance&lt;/h1&gt;

&lt;p&gt;January marks one year since I joined the C++ Alliance and started maintaining the Boost.Beast library.&lt;/p&gt;

&lt;p&gt;It’s a pleasure to work with interesting and fun people who are passionate about writing C++, and in particular good 
C++.&lt;/p&gt;

&lt;p&gt;During the past year I have spent some time attending ISO WG21 meetings online as an alternate representative of the 
C++ Alliance. Prior to joining the organisation, during my life as a developer I always felt that the standards 
committee did the developer community a disservice. Without knowing much about the inner workings, it seemed to me that 
the committee lived in an Ivory Tower. So my intention was to see if there was a way to bring something useful to the table 
as a keen a prolific writer of C++ in the financial sector. In particular I had a personal interest in the 
standardisation of asynchronous networking, forked from the wonderful Asio library.&lt;/p&gt;

&lt;p&gt;I ended the year feeling no less jaded with the entire standards process, concluding that there is not much I can do to 
help.&lt;/p&gt;

&lt;p&gt;I feel it’s important to say that the committee is attended by very bright, passionate people who clearly enjoy the C++
language as much as I do.&lt;/p&gt;

&lt;p&gt;What I think does not work, at least from the point of view of delivering useful progress, is the process of committee 
itself. During my commercial life there has been one fundamental truth, which is that things go well when there is focus
of attention and the taking of personal responsibility. It seems to me that committees in general undermine these
important fundamentals. The upshot of this is that in my mind, C++ developers are not going to get the tools they need, 
in the timescales they need, if they wait for the slow grind of WG21’s wheels of stone.&lt;/p&gt;

&lt;p&gt;It is to me noteworthy that many of the libraries I actually use (fmt, spdlog, boost, jwt, openssl, and so on) have been 
in some way standardised or are in the process of being standardised, but always in a lesser form than the original, 
created by a small, passionate team of individuals who enjoyed autonomy and freedom of design.&lt;/p&gt;

&lt;p&gt;Even now, if a feature is available in the standard and as a 3rd party library, I will almost always choose the third 
party version. It will generally have more features and a design undamaged by a process that externalises costs.&lt;/p&gt;

&lt;p&gt;Which brings me back to my old C++ mantra, proven true for me over the past 15 years or so, &lt;em&gt;Boost is the 
Standard&lt;/em&gt;. Having said this, I must in fairness mention the wonderful fmtlib and spdlog libraries, and the gargantuan
Qt, without which a back-end developer like me would never be able to get anything to display on a screen in a cross-
platform manner.&lt;/p&gt;

&lt;p&gt;In the end I find myself in the same place I was a year ago: My view is that the only thing C++ needs is a credible 
dependency management tool. Given that, the developer community will produce and distribute all needed libraries, and 
the most popular will naturally become the standard ones.&lt;/p&gt;

&lt;h1 id=&quot;the-year-ahead&quot;&gt;The Year Ahead&lt;/h1&gt;

&lt;p&gt;Therefore, it is my intention this year to do what I can to bring more utility to the Boost ecosystem, where one 
person can make a useful impact on the lives of developers, and taking personal responsibility for libraries is the 
norm.&lt;/p&gt;

&lt;h2 id=&quot;the-big-three&quot;&gt;The Big Three&lt;/h2&gt;

&lt;p&gt;It is my view that there are a number of areas where common use cases have not been well served in Boost. These are of 
course JSON, Databases and HTTP clients.&lt;/p&gt;

&lt;h3 id=&quot;json&quot;&gt;JSON&lt;/h3&gt;

&lt;p&gt;At the end of 2020, Vinnie and the team finally brought a very credible JSON library to Boost, which I have used to
write some cryptocurrency exchange connectors. On the whole, it’s proven to be a very pleasant and 
intuitive API. In particular the methods to simultaneously query the presence of a value and return a pointer on success
with &lt;code&gt;if_contains&lt;/code&gt;, &lt;code&gt;if_string&lt;/code&gt; etc. have seen a lot of use and result in code that is readable and neat.&lt;/p&gt;

&lt;p&gt;Currently, Boost.JSON favours performance over accuracy with respect to parsing floating point numbers (sometimes a 
number is 1 ULP different to that which you’d expect from &lt;code&gt;std::strtod&lt;/code&gt;). I had a little look to see if there was 
a way to address this easily. It transpired not. Parsing decimal representations of floating point numbers into a binary
representation that will round-trip correctly is a hard problem, and beyond my level of mathematical skill. 
There is currently work underway to address this in the JSON repo.&lt;/p&gt;

&lt;p&gt;There is also work in progress to provide JSON-pointer lookups. This will be a welcome addition as it will mean I can
throw away my jury-rigged “path” code and use something robust.&lt;/p&gt;

&lt;h3 id=&quot;mysql&quot;&gt;MySQL&lt;/h3&gt;

&lt;p&gt;A very common database in use in the web world is of course MySQL. I have always found the official connectors somewhat
uninteresting, particularly as there is no asynchronous connector compatible with Asio.&lt;/p&gt;

&lt;p&gt;That was until a certain Rubén Pérez decided to write an asio-compatible mysql connector from scratch, using 
none of the code in the Oracle connector. Rubén has started the arduous journey of getting his 
&lt;a href=&quot;https://anarthal.github.io/boost-mysql/index.html&quot;&gt;library&lt;/a&gt; ready for Boost review.&lt;/p&gt;

&lt;p&gt;I have been asked to be the review manager for this work, something I am happy to do as, whether or not the admission is
ultimately accepted, I think the general principle of producing simple, self-contained libraries that meet a 
common requirement is to be encouraged.&lt;/p&gt;

&lt;p&gt;If this library is successful, I would hope that others will rise to the challenge and provide native connectors for
other common database systems.&lt;/p&gt;

&lt;h3 id=&quot;http-client&quot;&gt;HTTP Client&lt;/h3&gt;

&lt;p&gt;I mentioned in an earlier blog that an HTTP Client with a similar interface to Python’s Requests library woudl be worked 
on. As it happens, other priorities took over last year. This year I will be focussing efforts on getting this library 
in shape for a proof of concept.&lt;/p&gt;

&lt;h3 id=&quot;i-mean-big-four&quot;&gt;I mean Big Four&lt;/h3&gt;

&lt;p&gt;Redis is ubiquitous in server environments. A quick search for Redis C++ clients turned up this 
&lt;a href=&quot;https://github.com/basiliscos/cpp-bredis&quot;&gt;little gem&lt;/a&gt;. I’d like to find time to give this a try at some point.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">A year in the C++ Alliance January marks one year since I joined the C++ Alliance and started maintaining the Boost.Beast library. It’s a pleasure to work with interesting and fun people who are passionate about writing C++, and in particular good C++. During the past year I have spent some time attending ISO WG21 meetings online as an alternate representative of the C++ Alliance. Prior to joining the organisation, during my life as a developer I always felt that the standards committee did the developer community a disservice. Without knowing much about the inner workings, it seemed to me that the committee lived in an Ivory Tower. So my intention was to see if there was a way to bring something useful to the table as a keen a prolific writer of C++ in the financial sector. In particular I had a personal interest in the standardisation of asynchronous networking, forked from the wonderful Asio library. I ended the year feeling no less jaded with the entire standards process, concluding that there is not much I can do to help. I feel it’s important to say that the committee is attended by very bright, passionate people who clearly enjoy the C++ language as much as I do. What I think does not work, at least from the point of view of delivering useful progress, is the process of committee itself. During my commercial life there has been one fundamental truth, which is that things go well when there is focus of attention and the taking of personal responsibility. It seems to me that committees in general undermine these important fundamentals. The upshot of this is that in my mind, C++ developers are not going to get the tools they need, in the timescales they need, if they wait for the slow grind of WG21’s wheels of stone. It is to me noteworthy that many of the libraries I actually use (fmt, spdlog, boost, jwt, openssl, and so on) have been in some way standardised or are in the process of being standardised, but always in a lesser form than the original, created by a small, passionate team of individuals who enjoyed autonomy and freedom of design. Even now, if a feature is available in the standard and as a 3rd party library, I will almost always choose the third party version. It will generally have more features and a design undamaged by a process that externalises costs. Which brings me back to my old C++ mantra, proven true for me over the past 15 years or so, Boost is the Standard. Having said this, I must in fairness mention the wonderful fmtlib and spdlog libraries, and the gargantuan Qt, without which a back-end developer like me would never be able to get anything to display on a screen in a cross- platform manner. In the end I find myself in the same place I was a year ago: My view is that the only thing C++ needs is a credible dependency management tool. Given that, the developer community will produce and distribute all needed libraries, and the most popular will naturally become the standard ones. The Year Ahead Therefore, it is my intention this year to do what I can to bring more utility to the Boost ecosystem, where one person can make a useful impact on the lives of developers, and taking personal responsibility for libraries is the norm. The Big Three It is my view that there are a number of areas where common use cases have not been well served in Boost. These are of course JSON, Databases and HTTP clients. JSON At the end of 2020, Vinnie and the team finally brought a very credible JSON library to Boost, which I have used to write some cryptocurrency exchange connectors. On the whole, it’s proven to be a very pleasant and intuitive API. In particular the methods to simultaneously query the presence of a value and return a pointer on success with if_contains, if_string etc. have seen a lot of use and result in code that is readable and neat. Currently, Boost.JSON favours performance over accuracy with respect to parsing floating point numbers (sometimes a number is 1 ULP different to that which you’d expect from std::strtod). I had a little look to see if there was a way to address this easily. It transpired not. Parsing decimal representations of floating point numbers into a binary representation that will round-trip correctly is a hard problem, and beyond my level of mathematical skill. There is currently work underway to address this in the JSON repo. There is also work in progress to provide JSON-pointer lookups. This will be a welcome addition as it will mean I can throw away my jury-rigged “path” code and use something robust. MySQL A very common database in use in the web world is of course MySQL. I have always found the official connectors somewhat uninteresting, particularly as there is no asynchronous connector compatible with Asio. That was until a certain Rubén Pérez decided to write an asio-compatible mysql connector from scratch, using none of the code in the Oracle connector. Rubén has started the arduous journey of getting his library ready for Boost review. I have been asked to be the review manager for this work, something I am happy to do as, whether or not the admission is ultimately accepted, I think the general principle of producing simple, self-contained libraries that meet a common requirement is to be encouraged. If this library is successful, I would hope that others will rise to the challenge and provide native connectors for other common database systems. HTTP Client I mentioned in an earlier blog that an HTTP Client with a similar interface to Python’s Requests library woudl be worked on. As it happens, other priorities took over last year. This year I will be focussing efforts on getting this library in shape for a proof of concept. I mean Big Four Redis is ubiquitous in server environments. A quick search for Redis C++ clients turned up this little gem. I’d like to find time to give this a try at some point.</summary></entry><entry><title type="html">Drone CI</title><link href="http://cppalliance.org/sam/2021/01/15/DroneCI.html" rel="alternate" type="text/html" title="Drone CI" /><published>2021-01-15T00:00:00+00:00</published><updated>2021-01-15T00:00:00+00:00</updated><id>http://cppalliance.org/sam/2021/01/15/DroneCI</id><content type="html" xml:base="http://cppalliance.org/sam/2021/01/15/DroneCI.html">&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;A message currently appears (mid-January 2021) at the top of the travis-ci.org website.&lt;/p&gt;

&lt;p&gt;“Please be aware travis-ci.org will be shutting down in several weeks, with all accounts migrating to travis-ci.com. Please stay tuned here for more information.”&lt;/p&gt;

&lt;p&gt;The transition has not been a smooth one, with long, disruptive delays occurring on existing builds, and lack of clear communication from the company. Many were unaware of the impending change. Some informative posts about the topic are &lt;a href=&quot;https://www.jeffgeerling.com/blog/2020/travis-cis-new-pricing-plan-threw-wrench-my-open-source-works&quot;&gt;Travis CI’s new pricing plan threw a wrench in my open source works&lt;/a&gt; and &lt;a href=&quot;https://travis-ci.community/t/extremely-poor-official-communication-of-the-org-shutdown/10568&quot;&gt;Extremely poor official communication of the .org shutdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The C++ Alliance has decided to implement an in-house CI solution and make the new service available for Boost libraries also.&lt;/p&gt;

&lt;h1 id=&quot;selection-process&quot;&gt;Selection Process&lt;/h1&gt;

&lt;p&gt;The first step was choosing which software to use. There are truly a surprising number of alternatives. An extensive review was conducted, including many Continuous Integration services from &lt;a href=&quot;https://github.com/ligurio/awesome-ci&quot;&gt;awesome-ci&lt;/a&gt; which lists more than 50.  Coincidentally, Rene Rivera had recently done an analysis as well for &lt;a href=&quot;https://github.com/bfgroup/ci_playground&quot;&gt;ci_playground&lt;/a&gt;, and his example config files eventually became the basis for our new config files.&lt;/p&gt;

&lt;p&gt;The top choices:&lt;br /&gt;
Appveyor&lt;br /&gt;
Azure Pipelines&lt;br /&gt;
BuildBot&lt;br /&gt;
CircleCI&lt;br /&gt;
CirrusCI&lt;br /&gt;
Drone&lt;br /&gt;
Github Actions&lt;br /&gt;
Semaphore&lt;br /&gt;
Shippable&lt;br /&gt;
TeamCity&lt;/p&gt;

&lt;p&gt;From this list, Appveyor and Drone seemed the most promising to start with.  Both allow for 100% self-hosting.&lt;/p&gt;

&lt;h2 id=&quot;appveyor&quot;&gt;Appveyor&lt;/h2&gt;

&lt;p&gt;The appeal of Appveyor is that the config files are basic yaml, and everything runs in a Docker container. It sounds perfect. However, after experimentation there were a few issues.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Appveyor was originally designed for Microsoft Windows, with .NET and Powershell being key ingredients. While it can run on Linux, it’s not a native Linux application.  Most of the CI testing done by Boost and CPPAlliance runs on Linux.&lt;/li&gt;
  &lt;li&gt;Specifically, the Docker experience on Windows is not nearly as smooth as Linux. I encountered numerous complexities when setting up Appveyor Windows Docker containers.&lt;/li&gt;
  &lt;li&gt;Bugs in their app.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Due to a combination of those reasons, Appveyor was not the best choice for this project.&lt;/p&gt;
&lt;h2 id=&quot;drone&quot;&gt;Drone&lt;/h2&gt;

&lt;p&gt;Within the first day or two experimenting with Drone, it became clear that this was an excellent CI framework:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Simple, usable UI&lt;/li&gt;
  &lt;li&gt;Easy installation&lt;/li&gt;
  &lt;li&gt;Linux and Docker native&lt;/li&gt;
  &lt;li&gt;Small number of processes to run&lt;/li&gt;
  &lt;li&gt;Integrates with PostgreSQL, MySQL, Amazon S3&lt;/li&gt;
  &lt;li&gt;Autoscale the agents&lt;/li&gt;
  &lt;li&gt;Badges on github repos&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The main drawback with Drone is the absence of “matrix” builds. Their alternative for matrices is jsonnet or starlark, which are flexible scripting languages. I was apprehensive about this point, thinking that the end-user would prefer simple yaml files - exactly like Travis. And, in fact, that is probably the case. Basic yaml files are easier to understand. However, on balance, this was the only noticable problem with Drone, and everything else seemed to be in order. The resulting starlark files do have a matrix-like configuration where each job can be customized.&lt;/p&gt;

&lt;p&gt;To discuss &lt;a href=&quot;https://docs.bazel.build/versions/master/skylark/language.html&quot;&gt;Starlark&lt;/a&gt; for a moment - it’s a subset of python, and therefore easy to learn. &lt;a href=&quot;https://pypi.org/project/pystarlark/&quot;&gt;Why would I use Starlark instead of just Python?&lt;/a&gt; “Sandboxing. The primary reason this was written is for the “hermetic execution” feature of Starlark. Python is notoriously difficult to sandbox and there didn’t appear to be any sandboxing solutions that could run within Python to run Python or Python-like code. While Starlark isn’t exactly Python it is very very close to it. You can think of this as a secure way to run very simplistic Python functions. Note that this library itself doesn’t really provide any security guarantees and your program may crash while using it (PRs welcome). Starlark itself is providing the security guarantees.”&lt;/p&gt;

&lt;h3 id=&quot;running-a-drone-server&quot;&gt;Running a drone server&lt;/h3&gt;

&lt;p&gt;Create a script startdrone.sh with these contents:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run \
  --volume=/var/lib/drone:/data \
  --env=DRONE_GITHUB_CLIENT_ID= \
  --env=DRONE_GITHUB_CLIENT_SECRET= \
  --env=DRONE_RPC_SECRET= \
  --env=DRONE_TLS_AUTOCERT= \
  --env=DRONE_SERVER_HOST= \
  --env=DRONE_SERVER_PROTO= \
  --env=DRONE_CONVERT_PLUGIN_ENDPOINT= \
  --env=DRONE_CONVERT_PLUGIN_SECRET= \
  --env=DRONE_HTTP_SSL_REDIRECT= \
  --env=DRONE_HTTP_SSL_TEMPORARY_REDIRECT= \
  --env=DRONE_S3_BUCKET= \
  --env=DRONE_LOGS_PRETTY= \
  --env=AWS_ACCESS_KEY_ID= \
  --env=AWS_SECRET_ACCESS_KEY= \
  --env=AWS_DEFAULT_REGION= \
  --env=AWS_REGION= \
  --env=DRONE_DATABASE_DRIVER= \
  --env=DRONE_DATABASE_DATASOURCE= \
  --env=DRONE_USER_CREATE= \
  --env=DRONE_REPOSITORY_FILTER= \
  --env=DRONE_GITHUB_SCOPE= \
  --publish=80:80 \
  --publish=443:443 \
  --restart=always \
  --detach=true \
  --name=drone \
  drone/drone:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Fill in the variables. (Many of those are secure keys which shouldn’t be published on a public webpage.)&lt;br /&gt;
Then, run the script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./startdrone.sh  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Drone is up and running.&lt;/p&gt;

&lt;p&gt;Next, the starlark plugin. Edit startstarlark.sh:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run -d \
  --volume=/var/lib/starlark:/data \
  --env= \
  --publish= \
  --env=DRONE_DEBUG= \
  --env=DRONE_SECRET= \
  --restart=always \
  --name=starlark drone/drone-convert-starlark
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and run it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./startstarlark.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Starlark is up and running. Finally, the autoscaler.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash
  
docker run -d \
  -v /var/lib/autoscaler:/data \
  -e DRONE_POOL_MIN= \
  -e DRONE_POOL_MAX= \
  -e DRONE_SERVER_PROTO= \
  -e DRONE_SERVER_HOST= \
  -e DRONE_SERVER_TOKEN= \
  -e DRONE_AGENT_TOKEN= \
  -e DRONE_AMAZON_REGION= \
  -e DRONE_AMAZON_SUBNET_ID= \
  -e DRONE_AMAZON_SECURITY_GROUP= \
  -e AWS_ACCESS_KEY_ID= \
  -e AWS_SECRET_ACCESS_KEY= \
  -e DRONE_CAPACITY_BUFFER= \
  -e DRONE_REAPER_INTERVAL= \
  -e DRONE_REAPER_ENABLED= \
  -e DRONE_ENABLE_REAPER= \
  -e DRONE_AMAZON_INSTANCE= \
  -e DRONE_AMAZON_VOLUME_TYPE= \
  -e DRONE_AMAZON_VOLUME_IOPS= \
  -p  \
  --restart=always \
  --name=autoscaler \
  drone/autoscaler
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Start the autoscaler.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./startautoscaler.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Windows autoscaler is still experimental. For now, both Windows and Mac servers have been installed manually and will be scaled individually. Because they are less common operating systems, with most boost builds running in Linux, the CPU load on these other machines is not as significant.&lt;/p&gt;

&lt;h1 id=&quot;configs&quot;&gt;Configs&lt;/h1&gt;

&lt;p&gt;The real complexities appear when composing the config files for each repository. After manually porting .travis.yml for &lt;a href=&quot;https://github.com/boostorg/beast&quot;&gt;https://github.com/boostorg/beast&lt;/a&gt; and &lt;a href=&quot;https://github.com/boostorg/json&quot;&gt;https://github.com/boostorg/json&lt;/a&gt;, the next step was creating a Python script which automates the entire process.&lt;/p&gt;

&lt;h2 id=&quot;drone-converter&quot;&gt;Drone Converter&lt;/h2&gt;

&lt;p&gt;A copy of the script can be viewed at &lt;a href=&quot;https://github.com/CPPAlliance/droneconverter-demo&quot;&gt;https://github.com/CPPAlliance/droneconverter-demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The idea is to be able to go into any directory with a .travis.yml file, and migrate to Drone by executing a single command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd boostorg/accumulators
droneconverter
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The converter ingests a source file, parses it with PyYAML, and dumps the output in Jinja2 templates. The method to write the script was by beginning with any library, such as &lt;a href=&quot;https://github.com/boostorg/array&quot;&gt;boostorg/array&lt;/a&gt;, and just get that one working. Then, move on to others, &lt;a href=&quot;https://github.com/boostorg/assign&quot;&gt;boostorg/assign&lt;/a&gt;,  &lt;a href=&quot;https://github.com/boostorg/bind&quot;&gt;boostorg/bind&lt;/a&gt;, etc. Each library contains a mix of travis jobs which are both similar and different to the previously translated libraries. Thus, each new travis file presents a new puzzle to solve, but hopefully in a generalized way that will also work for all repositories.&lt;/p&gt;

&lt;p&gt;Versions of &lt;a href=&quot;https://clang.llvm.org/&quot;&gt;clang&lt;/a&gt; ranging from 3.3 to 11 are targeted in the tests. While later releases such as clang-6 or clang-7 usually build right away without errors, the earlier versions in the 3.x series were failing to build for a variety of reasons. First of all, those versions are not available on Ubuntu 16.04 and later, which means being stuck on Ubuntu 14.04, preventing an across-the-board upgrade to a newer Ubuntu.  Then, if a standard “base” clang is simultaneously installed, such as clang-7 or 9, this seems to cause other dependent packages and libraries to be installed, which conflict with clang-3. The solution was to figure out what travis does, and copy it. Travis images have downloaded and installed clang-7 into a completely separate directory, not using the ordinary system packages. Then the extra directory /usr/local/clang-7.0.0/bin has been added to the $PATH.&lt;/p&gt;

&lt;p&gt;Some .travis.yml files have a “matrix” section. Others have “jobs”. Some .travis files place all the logic in one main “install” script. Others refer to a variety of script sections including “install”, “script”, “before_install”, “before_script”, “after_success”, which may or may not be shared between the 20 jobs contained in the file. Some job in travis were moving (with the mv command) their source directory, which is baffling and not usually permitted in Jenkins or Drone.  This must be converted to a copy command instead. “travis_wait” and “travis_retry” must be deleted, they are travis-specific.  Many travis variables such as TRAVIS_OS_NAME or TRAVIS_BRANCH need to be set and/or replaced in the resulting scripts. Environment variables in the travis file might be presented as a list, or a string, without quotes, or with single quotes, or double quotes, or single quotes embedded in double quotes, or double quotes embedded in single quotes, or missing entirely and included as a global env section at the top of the file.  The CXX variable, which determines the compiler used by boost build, isn’t always apparent and could be derived from a variety of sources, including the COMPILER variable or the list of packages. The list of these types of conversion fixes goes on and on, you can see them in the droneconverter program.&lt;/p&gt;

&lt;p&gt;Apple Macs Developer Tools depends on Xcode, with an array of possible Xcode version numbers from 6 to 12, and counting.  Catalina only runs 10 and above. High Sierra will run 7-9. None of these will operate without accepting the license agreement, however the license agreement might reset if switching to a newer version of Xcode. The presence of “Command Line Tools” seems to break some builds during testing, however everything works if “Command Line Tools” is missing and the build directly accesses Xcode in the /Applications/Xcode-11.7.app/Contents/Developer directory. On the other hand, the package manager “brew” needs “Command Line Tools” (on High Sierra) or it can’t install packages. A solution which appears to be sufficiently effective is to “mv CommandLineTools CommandLineTools.bck”, or the reverse, when needed.&lt;/p&gt;

&lt;p&gt;Drone will not start on a Mac during reboot unless the Drone user has a window console sessions running too. So, Apple is not an ideal command-line-only remote server environment.&lt;/p&gt;

&lt;h1 id=&quot;drone-deployments&quot;&gt;Drone Deployments&lt;/h1&gt;

&lt;p&gt;Pull requests with the new drone configs were rolled out to all those boost repositories with 100% travis build success rate and 100% drone build success rate, which accounts for about half of boost libraries. The other half of boost libraries are either missing a .travis.yml file, or they have a couple of travis/drone jobs which are failing. These should be addressed individually, even if only to post details about it in the pull requests. Ideally, attention should be focused on these failing tests, one by one, until the jobs attain a 100% build success rate and the badge displays green instead of red.  The &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_tail&quot;&gt;long tail&lt;/a&gt; distribution of edge-cases require individual attention and rewritten tests.&lt;/p&gt;

&lt;h1 id=&quot;github-actions&quot;&gt;Github Actions&lt;/h1&gt;

&lt;p&gt;The droneconverter script is being enhanced to also generate Github Actions config files. A first draft, tested on a few boost libraries, is building Linux jobs successfully. Ongoing.&lt;/p&gt;</content><author><name></name></author><category term="sam" /><summary type="html">Overview A message currently appears (mid-January 2021) at the top of the travis-ci.org website. “Please be aware travis-ci.org will be shutting down in several weeks, with all accounts migrating to travis-ci.com. Please stay tuned here for more information.” The transition has not been a smooth one, with long, disruptive delays occurring on existing builds, and lack of clear communication from the company. Many were unaware of the impending change. Some informative posts about the topic are Travis CI’s new pricing plan threw a wrench in my open source works and Extremely poor official communication of the .org shutdown. The C++ Alliance has decided to implement an in-house CI solution and make the new service available for Boost libraries also. Selection Process The first step was choosing which software to use. There are truly a surprising number of alternatives. An extensive review was conducted, including many Continuous Integration services from awesome-ci which lists more than 50. Coincidentally, Rene Rivera had recently done an analysis as well for ci_playground, and his example config files eventually became the basis for our new config files. The top choices: Appveyor Azure Pipelines BuildBot CircleCI CirrusCI Drone Github Actions Semaphore Shippable TeamCity From this list, Appveyor and Drone seemed the most promising to start with. Both allow for 100% self-hosting. Appveyor The appeal of Appveyor is that the config files are basic yaml, and everything runs in a Docker container. It sounds perfect. However, after experimentation there were a few issues. Appveyor was originally designed for Microsoft Windows, with .NET and Powershell being key ingredients. While it can run on Linux, it’s not a native Linux application. Most of the CI testing done by Boost and CPPAlliance runs on Linux. Specifically, the Docker experience on Windows is not nearly as smooth as Linux. I encountered numerous complexities when setting up Appveyor Windows Docker containers. Bugs in their app. Due to a combination of those reasons, Appveyor was not the best choice for this project. Drone Within the first day or two experimenting with Drone, it became clear that this was an excellent CI framework: Simple, usable UI Easy installation Linux and Docker native Small number of processes to run Integrates with PostgreSQL, MySQL, Amazon S3 Autoscale the agents Badges on github repos The main drawback with Drone is the absence of “matrix” builds. Their alternative for matrices is jsonnet or starlark, which are flexible scripting languages. I was apprehensive about this point, thinking that the end-user would prefer simple yaml files - exactly like Travis. And, in fact, that is probably the case. Basic yaml files are easier to understand. However, on balance, this was the only noticable problem with Drone, and everything else seemed to be in order. The resulting starlark files do have a matrix-like configuration where each job can be customized. To discuss Starlark for a moment - it’s a subset of python, and therefore easy to learn. Why would I use Starlark instead of just Python? “Sandboxing. The primary reason this was written is for the “hermetic execution” feature of Starlark. Python is notoriously difficult to sandbox and there didn’t appear to be any sandboxing solutions that could run within Python to run Python or Python-like code. While Starlark isn’t exactly Python it is very very close to it. You can think of this as a secure way to run very simplistic Python functions. Note that this library itself doesn’t really provide any security guarantees and your program may crash while using it (PRs welcome). Starlark itself is providing the security guarantees.” Running a drone server Create a script startdrone.sh with these contents: #!/bin/bash docker run \ --volume=/var/lib/drone:/data \ --env=DRONE_GITHUB_CLIENT_ID= \ --env=DRONE_GITHUB_CLIENT_SECRET= \ --env=DRONE_RPC_SECRET= \ --env=DRONE_TLS_AUTOCERT= \ --env=DRONE_SERVER_HOST= \ --env=DRONE_SERVER_PROTO= \ --env=DRONE_CONVERT_PLUGIN_ENDPOINT= \ --env=DRONE_CONVERT_PLUGIN_SECRET= \ --env=DRONE_HTTP_SSL_REDIRECT= \ --env=DRONE_HTTP_SSL_TEMPORARY_REDIRECT= \ --env=DRONE_S3_BUCKET= \ --env=DRONE_LOGS_PRETTY= \ --env=AWS_ACCESS_KEY_ID= \ --env=AWS_SECRET_ACCESS_KEY= \ --env=AWS_DEFAULT_REGION= \ --env=AWS_REGION= \ --env=DRONE_DATABASE_DRIVER= \ --env=DRONE_DATABASE_DATASOURCE= \ --env=DRONE_USER_CREATE= \ --env=DRONE_REPOSITORY_FILTER= \ --env=DRONE_GITHUB_SCOPE= \ --publish=80:80 \ --publish=443:443 \ --restart=always \ --detach=true \ --name=drone \ drone/drone:1 Fill in the variables. (Many of those are secure keys which shouldn’t be published on a public webpage.) Then, run the script. ./startdrone.sh Drone is up and running. Next, the starlark plugin. Edit startstarlark.sh: #!/bin/bash docker run -d \ --volume=/var/lib/starlark:/data \ --env= \ --publish= \ --env=DRONE_DEBUG= \ --env=DRONE_SECRET= \ --restart=always \ --name=starlark drone/drone-convert-starlark and run it: ./startstarlark.sh Starlark is up and running. Finally, the autoscaler. #!/bin/bash docker run -d \ -v /var/lib/autoscaler:/data \ -e DRONE_POOL_MIN= \ -e DRONE_POOL_MAX= \ -e DRONE_SERVER_PROTO= \ -e DRONE_SERVER_HOST= \ -e DRONE_SERVER_TOKEN= \ -e DRONE_AGENT_TOKEN= \ -e DRONE_AMAZON_REGION= \ -e DRONE_AMAZON_SUBNET_ID= \ -e DRONE_AMAZON_SECURITY_GROUP= \ -e AWS_ACCESS_KEY_ID= \ -e AWS_SECRET_ACCESS_KEY= \ -e DRONE_CAPACITY_BUFFER= \ -e DRONE_REAPER_INTERVAL= \ -e DRONE_REAPER_ENABLED= \ -e DRONE_ENABLE_REAPER= \ -e DRONE_AMAZON_INSTANCE= \ -e DRONE_AMAZON_VOLUME_TYPE= \ -e DRONE_AMAZON_VOLUME_IOPS= \ -p \ --restart=always \ --name=autoscaler \ drone/autoscaler Start the autoscaler. ./startautoscaler.sh Windows autoscaler is still experimental. For now, both Windows and Mac servers have been installed manually and will be scaled individually. Because they are less common operating systems, with most boost builds running in Linux, the CPU load on these other machines is not as significant. Configs The real complexities appear when composing the config files for each repository. After manually porting .travis.yml for https://github.com/boostorg/beast and https://github.com/boostorg/json, the next step was creating a Python script which automates the entire process. Drone Converter A copy of the script can be viewed at https://github.com/CPPAlliance/droneconverter-demo The idea is to be able to go into any directory with a .travis.yml file, and migrate to Drone by executing a single command: cd boostorg/accumulators droneconverter The converter ingests a source file, parses it with PyYAML, and dumps the output in Jinja2 templates. The method to write the script was by beginning with any library, such as boostorg/array, and just get that one working. Then, move on to others, boostorg/assign, boostorg/bind, etc. Each library contains a mix of travis jobs which are both similar and different to the previously translated libraries. Thus, each new travis file presents a new puzzle to solve, but hopefully in a generalized way that will also work for all repositories. Versions of clang ranging from 3.3 to 11 are targeted in the tests. While later releases such as clang-6 or clang-7 usually build right away without errors, the earlier versions in the 3.x series were failing to build for a variety of reasons. First of all, those versions are not available on Ubuntu 16.04 and later, which means being stuck on Ubuntu 14.04, preventing an across-the-board upgrade to a newer Ubuntu. Then, if a standard “base” clang is simultaneously installed, such as clang-7 or 9, this seems to cause other dependent packages and libraries to be installed, which conflict with clang-3. The solution was to figure out what travis does, and copy it. Travis images have downloaded and installed clang-7 into a completely separate directory, not using the ordinary system packages. Then the extra directory /usr/local/clang-7.0.0/bin has been added to the $PATH. Some .travis.yml files have a “matrix” section. Others have “jobs”. Some .travis files place all the logic in one main “install” script. Others refer to a variety of script sections including “install”, “script”, “before_install”, “before_script”, “after_success”, which may or may not be shared between the 20 jobs contained in the file. Some job in travis were moving (with the mv command) their source directory, which is baffling and not usually permitted in Jenkins or Drone. This must be converted to a copy command instead. “travis_wait” and “travis_retry” must be deleted, they are travis-specific. Many travis variables such as TRAVIS_OS_NAME or TRAVIS_BRANCH need to be set and/or replaced in the resulting scripts. Environment variables in the travis file might be presented as a list, or a string, without quotes, or with single quotes, or double quotes, or single quotes embedded in double quotes, or double quotes embedded in single quotes, or missing entirely and included as a global env section at the top of the file. The CXX variable, which determines the compiler used by boost build, isn’t always apparent and could be derived from a variety of sources, including the COMPILER variable or the list of packages. The list of these types of conversion fixes goes on and on, you can see them in the droneconverter program. Apple Macs Developer Tools depends on Xcode, with an array of possible Xcode version numbers from 6 to 12, and counting. Catalina only runs 10 and above. High Sierra will run 7-9. None of these will operate without accepting the license agreement, however the license agreement might reset if switching to a newer version of Xcode. The presence of “Command Line Tools” seems to break some builds during testing, however everything works if “Command Line Tools” is missing and the build directly accesses Xcode in the /Applications/Xcode-11.7.app/Contents/Developer directory. On the other hand, the package manager “brew” needs “Command Line Tools” (on High Sierra) or it can’t install packages. A solution which appears to be sufficiently effective is to “mv CommandLineTools CommandLineTools.bck”, or the reverse, when needed. Drone will not start on a Mac during reboot unless the Drone user has a window console sessions running too. So, Apple is not an ideal command-line-only remote server environment. Drone Deployments Pull requests with the new drone configs were rolled out to all those boost repositories with 100% travis build success rate and 100% drone build success rate, which accounts for about half of boost libraries. The other half of boost libraries are either missing a .travis.yml file, or they have a couple of travis/drone jobs which are failing. These should be addressed individually, even if only to post details about it in the pull requests. Ideally, attention should be focused on these failing tests, one by one, until the jobs attain a 100% build success rate and the badge displays green instead of red. The long tail distribution of edge-cases require individual attention and rewritten tests. Github Actions The droneconverter script is being enhanced to also generate Github Actions config files. A first draft, tested on a few boost libraries, is building Linux jobs successfully. Ongoing.</summary></entry><entry><title type="html">Richard’s New Year Update - Reusable HTTP Connections</title><link href="http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate.html" rel="alternate" type="text/html" title="Richard’s New Year Update - Reusable HTTP Connections" /><published>2021-01-01T00:00:00+00:00</published><updated>2021-01-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2021/01/01/RichardsNewYearUpdate.html">&lt;h1 id=&quot;reusable-https-connections&quot;&gt;Reusable HTTP(S) Connections&lt;/h1&gt;

&lt;p&gt;Something I am often asked by users of Boost Beast is how to code a client which effectively re-uses a pool of HTTP 
connections, in the same way a web browser does.&lt;/p&gt;

&lt;p&gt;The premise is straightforward - if our client is going to be making multiple calls to a web server (or several of them)
then it makes sense that once a connection has been used for one request, it is returned to a connection pool so that 
a subsequent request can make use of it.&lt;/p&gt;

&lt;p&gt;It also makes sense to have a limit on the number of concurrent connections that can be open against any one host.
Otherwise, if the client needs to make multiple requests at the same time, it will end up creating new connections in
parallel and lose the efficiency of re-using an existing connection.&lt;/p&gt;

&lt;p&gt;From these requirements, we can start to think about a design.&lt;/p&gt;

&lt;p&gt;Firstly, we can imagine a connection cache, with connections kept in a map keyed on host + scheme + port (we can’t 
re-use an HTTP port for an HTTPS request!).&lt;/p&gt;

&lt;p&gt;When a request needs a connection, it will either create a new one (connection limit per host not met) or will wait
for an existing connection to become available (which implies a condition variable).&lt;/p&gt;

&lt;p&gt;Once a request has a connection to use, it will send the HTTP request and wait for the response.&lt;/p&gt;

&lt;p&gt;At this stage, there is a possibility that the active connection which has been allocated could have been idle since 
the last time it was used. In TCP there is no way to atomically check whether the remote host has closed the 
connection (or died). The only way to know is to actually read from the socket with a timeout. If the remote host has 
shutdown the socket, we will be notified as soon as the RST frame arrives at our host. If the remote host has stopped
working or the network is bad, we’ll be notified by the timeout.&lt;/p&gt;

&lt;p&gt;Thus if our read operation results in an error and we have inherited the connection from the cache, we ought to re-try
by reopening the connection to the remote host and repeating the write/read operations. However, if there is an error
reported on the subsequent attempt, then we can conclude that this is a legitimate error to be reported back to the
caller.&lt;/p&gt;

&lt;p&gt;In simplified pesudocode, the operation might look something like this (assuming we report transport errors as 
exceptions):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;response 
read_write(connection&amp;amp; conn)
{
  response resp;
  
  auto retry = false;
   
  if(conn.is_initialised())
    retry = true;
  else
    conn.connect(...);
  
  for(;;)
  {
    conn.write(request);
    auto err = conn.read(resp);
    if (err)
    {
      if(!std::exchange(retry, false))
        throw system_error(err);
      request.clear();
      conn.disconnect();
    }
    else
      break;
  }

  return resp;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;structuring-the-code&quot;&gt;Structuring the Code&lt;/h1&gt;

&lt;h2 id=&quot;general-principles&quot;&gt;General Principles&lt;/h2&gt;

&lt;p&gt;In my previous &lt;a href=&quot;https://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html&quot;&gt;blog&lt;/a&gt; I mentioned my preference
for writing the implementation of a class in such a way that it does not need to take care of its own lifetime or
mutual exclusion. These concerns are deferred to a wrapper or handle. Methods on the handle class take care of 
marshalling the call to the correct executor (or thread) and preserving the implementation’s lifetime. The 
implementation need only concern itself with the logic of handling the request.&lt;/p&gt;

&lt;p&gt;Here’s an example about which I’ll go into more detail later:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    // DRY - define an operation that performs the inner call.
    auto op = [&amp;amp;] {
        return impl_-&amp;gt;call(method,
                           url,
                           std::move(data),
                           std::move(headers),
                           std::move(options));
    };

    // deduce the current executor
    auto my_executor = co_await net::this_coro::executor;

    // either call directly or via a spawned coroutine
    co_return impl_-&amp;gt;get_executor() != my_executor
        ? co_await op()
        : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;coroutines&quot;&gt;Coroutines&lt;/h2&gt;

&lt;p&gt;In this implementation I will be providing a C++20 coroutine interface over Asio executors once again. I am using 
coroutines because they are easier to write when compared to Asio’s composed operations, but are fundamentally the 
same thing in a prettier package.&lt;/p&gt;

&lt;h2 id=&quot;mutual-exclusion&quot;&gt;Mutual Exclusion&lt;/h2&gt;

&lt;p&gt;For mutual exclusion I will be embedded an asio strand into each active object. The advantage of doing so means that no
thread of execution is ever blocked which means we can limit the number of threads in the program to the number of 
free CPUs giving us maximum throughput of work. In reality of course, one thread is more than enough computing power
for almost all asynchronous programs. It’s therefore better to think in terms of one &lt;em&gt;executor&lt;/em&gt; per program component,
with the implicit guarantee that a given executor will only perform work on one thread at a time.
Thinking this way allows us to write code in a way that is agnostic of whether the final program is compiled to be 
single or multi-threaded.&lt;/p&gt;

&lt;h2 id=&quot;but-what-about-single-threaded-programs&quot;&gt;But What About Single-threaded Programs?&lt;/h2&gt;

&lt;p&gt;In order that I don’t need to rewrite code when should I decide to make a single-threaded program multi-threaded or vice 
versa, I have a couple of little utility functions and types defined in 
&lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021/blob/master/src/config.hpp&quot;&gt;config.hpp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Specifically:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace net
{
using namespace asio;

using io_executor = io_context::executor_type;

#ifdef MULTI_THREADED

using io_strand = strand&amp;lt; io_executor &amp;gt;;

inline io_strand
new_strand(io_executor const &amp;amp;src);

inline io_strand
new_strand(io_strand const &amp;amp;src);

#else

using io_strand = io_context::executor_type;

inline io_strand
new_strand(io_executor const &amp;amp;src);

#endif

inline io_executor
to_io_executor(any_io_executor const &amp;amp;src);
}   // namespace net
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Any object type in the program which &lt;em&gt;would require its own strand&lt;/em&gt; in a multi-threaded program will simply use the type
&lt;code&gt;io_strand&lt;/code&gt; whether the program is compiled for single-threaded operation or not. Any code that would notionally 
need to construct a new strand simply calls &lt;code&gt;new_strand(e)&lt;/code&gt; where &lt;code&gt;e&lt;/code&gt; is either a strand or a naked executor.&lt;/p&gt;

&lt;p&gt;Any code that needs access to the notional underlying executor would call &lt;code&gt;to_io_executor(e)&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;determinism&quot;&gt;Determinism&lt;/h2&gt;

&lt;p&gt;Since we’re using asio’s executors for scheduling, it means that we can use asio’s timer as a deterministic, ordered 
asynchronous condition variable, which means that requests waiting on the connection pool will be offered free 
connections in the order that they were requested. This guarantee is implicit in the way that the timers’ &lt;code&gt;cancel_one()&lt;/code&gt;
method is specified.&lt;/p&gt;

&lt;p&gt;As we’ll see later, asio’s timers also make it trivial to implement an asynchronous semaphore. In this case we use one
to ensure that requests are handled concurrently but no more than some upper limit at any one time.&lt;/p&gt;

&lt;h2 id=&quot;interface&quot;&gt;Interface&lt;/h2&gt;

&lt;p&gt;I’m going to create a high-level concept called a &lt;code&gt;connection_cache&lt;/code&gt;. The interface will be something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct connection_cache
{
  using response_ptr = std::unique_ptr&amp;lt; response &amp;gt;;
  
  net::awaitable&amp;lt; response_ptr &amp;gt;
  rest_call(
    verb method, 
    std::string const&amp;amp; url, 
    std::optional&amp;lt;std::string&amp;gt; data = std::nullopt,
    headers hdrs = {},
    options opts = {});
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The return type of the coroutine is a unique_ptr to a response. A natural question might be whether the response 
should simply be returned by value. However, in practice I have found that there are a number of practical reasons
why it’s often better to return the response as a pointer. Firstly it allows conversion to a 
&lt;code&gt;shared_ptr&amp;lt;const response&amp;gt;&lt;/code&gt; in environments where the response might be passed through a directed acyclic graph.
Secondly, would allow a further enhancement in that having finished with the response, the client could post it back
to the cache, meaning that it could be cached for re-use.&lt;/li&gt;
  &lt;li&gt;The only two required arguments are the method and url. All others can be defaulted.&lt;/li&gt;
  &lt;li&gt;An optional string may be passed which contains the payload of a POST request. This is passed by value because, 
as we’ll see later,the implementation will want to move this into the request object prior to initiating 
communications. I have chosen a string type for two reasons
    &lt;ul&gt;
      &lt;li&gt;text in the form of JSON or XML is the most common form of messaging in REST calls.&lt;/li&gt;
      &lt;li&gt;strings can contain binary data with no loss of efficiency.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code&gt;hdrs&lt;/code&gt; is simply a list of headers which should be set in the request. Again, these are passed by value as they will
be moved into the request object.&lt;/li&gt;
  &lt;li&gt;The last parameter of the call is an as-yet undefined type called &lt;code&gt;options&lt;/code&gt;. This will allow us to add niceties like
timeout arguments, a stop_token, redirect policies, a reference to a cookie store and so on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When called, &lt;code&gt;rest_call&lt;/code&gt; will attempt to reuse an existing connection. If a connection is not available, it will create
a new one if we are under the connection threshold for the deduced host and if not, it will wait until a connection is
available.&lt;/p&gt;

&lt;p&gt;Furthermore, the number of concurrent requests will be throttled to some upper limit.&lt;/p&gt;

&lt;p&gt;Transport failures will be reported as an exception (of type &lt;code&gt;system_error&lt;/code&gt;) and a successful response (even if a 400 
or 500) will be returned in the &lt;code&gt;response_ptr&lt;/code&gt;. That is to say, as long as the transport layer works out, the code will 
take the non-exceptional path.&lt;/p&gt;

&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h2&gt;

&lt;h3 id=&quot;url-parsing&quot;&gt;URL Parsing&lt;/h3&gt;

&lt;p&gt;Among the things I am often asked about in the Beast slack channel and in the 
&lt;a href=&quot;https://github.com/boostorg/beast/issues&quot;&gt;Issue Tracker&lt;/a&gt; is why there is no URL support in Beast. 
The is that Beast is a reasonably low level library that concerns itself with the HTTP (and WebSocket) 
protocols, plus as much buffer and stream management as is necessary to implement HTTP over Asio.
The concept of a URL the subject of its own RFCs and is a higher level concern.
The C++ Alliance is working on &lt;a href=&quot;https://github.com/CPPAlliance/url&quot;&gt;Boost.URL&lt;/a&gt; but it is not ready for publishing yet.
In the meantime, I found a nifty regex on the internet that more-or-less suffices for our needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    std::tuple&amp;lt; connection_key, std::string &amp;gt;
    parse_url(std::string const &amp;amp;url)
    {
        static const auto url_regex = std::regex(
            R&quot;regex((http|https)://([^/ :]+):?([^/ ]*)((/?[^ #?]*)\x3f?([^ #]*)#?([^ ]*)))regex&quot;,
            std::regex_constants::icase | std::regex_constants::optimize);
        auto match = std::smatch();
        if (not std::regex_match(url, match, url_regex))
            throw system_error(net::error::invalid_argument, &quot;invalid url&quot;);

        auto &amp;amp;protocol = match[1];
        auto &amp;amp;host     = match[2];
        auto &amp;amp;port_ind = match[3];
        auto &amp;amp;target   = match[4];
        /*
        auto &amp;amp;path     = match[5];
        auto &amp;amp;query    = match[6];
        auto &amp;amp;fragment = match[7];
        */
        return std::make_tuple(
            connection_key { .hostname = host.str(),
                             .port     = deduce_port(protocol, port_ind),
                             .scheme   = deduce_scheme(protocol, port_ind) },
            target.str());
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&quot;exceptions-in-asynchronous-code-considered-harmful&quot;&gt;Exceptions in Asynchronous Code Considered Harmful&lt;/h3&gt;

&lt;p&gt;I hate to admit this, because I am a huge fan of propagating errors as exceptions. This is because the combination of 
RAII and exception behaviour handling makes error handling very slick in C++. However, coroutines have two rather 
unpleasant limitations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You can’t call a coroutine in a destructor.&lt;/li&gt;
  &lt;li&gt;You can’t call a coroutine in an exception handling block.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are workarounds. Consider:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;my_component::~my_component()
{
  // destructor
  net::co_spawn(get_executor(), 
  [impl = impl_]()-&amp;gt;net::awaitable&amp;lt;void&amp;gt;
  {
    co_await impl-&amp;gt;shutdown();
    // destructor of *impl happens here
  }, net::detached);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the destructor of a wrapper object that contains a &lt;code&gt;shared_ptr&lt;/code&gt; to its implementation, &lt;code&gt;impl_&lt;/code&gt;. In this case
we can detect the destruction of &lt;code&gt;my_component&lt;/code&gt; and use this to spawn a new coroutine of indeterminate lifetime that
takes care of shutting down the actual implementation and then destroying it.&lt;/p&gt;

&lt;p&gt;This solves the problem of RAII but it mandates that we must author objects that will be used in coroutines as 
a handle-body pair.&lt;/p&gt;

&lt;p&gt;We can similarly get around the “no coroutine calls in exception handlers” limitation if we’re prepared to stomach code 
like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt; 
my_coro()
{
  std::function&amp;lt;net::awaitable(void)&amp;gt; on_error;
  try {
    co_await something();
  }
  catch(...)
  {
    // set up error_handler
    on_error = [ep = std::current_exception] {
      return handler_error_coro(ep);
    };
  }
  // perhaps handle the error here
  if (on_error)
      co_await on_error();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I think you’ll agree that this is a revolting solution to an unforgivable omission in the language. Not only is it 
untidy, confusing, difficult to teach and error-prone, it also turns exception handling into same fiasco that is 
enforced checking of return codes.&lt;/p&gt;

&lt;p&gt;To add insult to injury, the error handling code in this function takes up 5 times as many lines as the logic!&lt;/p&gt;

&lt;p&gt;Therefore my recommendation is that in asynchronous coroutine code, it’s better to avoid exceptions and have coroutines
either return a tuple of (error_code, possible_value) or a variant containing error-or-value.&lt;/p&gt;

&lt;p&gt;For example, here is some code from the &lt;code&gt;connection_impl&lt;/code&gt; in my example project:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; std::tuple&amp;lt; error_code, response_type &amp;gt; &amp;gt;
connection_impl::rest_call(request_class const &amp;amp;  request,
                           request_options const &amp;amp;options)
{
    auto response = std::make_unique&amp;lt; response_class &amp;gt;();

    auto ec = stream_.is_open()
                  ? co_await rest_call(request, *response, options)
                  : net::error::basic_errors::not_connected;

    if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted)
    {
        ec = co_await connect(options.stop);
        if (!ec)
            ec = co_await rest_call(request, *response, options);
    }

    if (ec || response-&amp;gt;need_eof())
        stream_.close();

    co_return std::make_tuple(ec, std::move(response));
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ensuring-that-a-coroutine-executes-on-the-correct-executor&quot;&gt;Ensuring that a Coroutine Executes on the Correct Executor&lt;/h3&gt;

&lt;p&gt;In playing with asio coroutines I stumbled upon something that has become an idiom.&lt;/p&gt;

&lt;p&gt;Consider the situation where a &lt;code&gt;connection&lt;/code&gt; class is implemented in terms of a handle and body. The body contains its 
own executor. In a multi-threaded build, this executor will be a &lt;code&gt;strand&lt;/code&gt; while in a single threaded-build we would want 
it to be simply an &lt;code&gt;io_context::executor_type&lt;/code&gt; since there will be no need for any of the thread guards implicit in a 
strand.&lt;/p&gt;

&lt;p&gt;Now consider that the implementation has a member coroutine called (say) &lt;code&gt;call&lt;/code&gt;. There are two scenarios in which 
this member will be called. The first is where the caller is executing in the same executor that is associated with 
the implementation, the second is where the caller is in its own different executor.
In the latter case, we must &lt;code&gt;post&lt;/code&gt; or &lt;code&gt;spawn&lt;/code&gt; the execution of the coroutine onto the implementation’s executor in order 
to ensure that it runs in the correct sequence with respect to other coroutines initiated against it.&lt;/p&gt;

&lt;p&gt;The idiom that occurred to me originally was to recursively spawn a coroutine to ensure the call happened on the 
correct executor:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    auto my_executor = co_await net::this_coro::executor;

    if (impl_-&amp;gt;get_executor() == my_executor)
    {
        co_return co_await impl_-&amp;gt;call(method,
                                       url,
                                       std::move(data),
                                       std::move(headers),
                                       std::move(options));
    }
    else
    {
        // spawn a coroutine which recurses on the correct executor.
        // wait for this coroutine to finish
        co_return co_await net::co_spawn(
            impl_-&amp;gt;get_executor(),
            [&amp;amp;]() -&amp;gt; net::awaitable&amp;lt; response_type &amp;gt; {
                return call(method,
                            url,
                            std::move(data),
                            std::move(headers),
                            std::move(options));
            },
            net::use_awaitable);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, this does have the drawback that a code analyser might see the possibility of infinite recursion.&lt;/p&gt;

&lt;p&gt;After discussing this with &lt;a href=&quot;https://github.com/chriskohlhoff/asio/&quot;&gt;Chris&lt;/a&gt;, Asio’s author, a better solution was found:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; response_type &amp;gt;
connection_cache::call(beast::http::verb   method,
                       const std::string &amp;amp; url,
                       std::string         data,
                       beast::http::fields headers,
                       request_options     options)
{
    // DRY - define an operation that performs the inner call.
    auto op = [&amp;amp;] {
        return impl_-&amp;gt;call(method,
                           url,
                           std::move(data),
                           std::move(headers),
                           std::move(options));
    };

    // deduce the current executor
    auto my_executor = co_await net::this_coro::executor;

    // either call directly or via a spawned coroutine
    co_return impl_-&amp;gt;get_executor() != my_executor
        ? co_await op()
        : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few things to note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;All lifetimes are correct even though the &lt;code&gt;op&lt;/code&gt; takes arguments by reference. This is because whichever path we take, 
our outer coroutine will suspend on the call to &lt;code&gt;op&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Note that in the slow path, &lt;code&gt;op&lt;/code&gt; is captured by value in the call to &lt;code&gt;co_spawn&lt;/code&gt;. Had we written:
&lt;code&gt;: co_await net::co_spawn(impl_-&amp;gt;get_executor(), op(), net::use_awaitable);&lt;/code&gt; then &lt;code&gt;op&lt;/code&gt; would have been called &lt;em&gt;during&lt;/em&gt;
the setup of the call to &lt;code&gt;co_spawn&lt;/code&gt;, which would result in &lt;code&gt;impl_-&amp;gt;call(...)&lt;/code&gt; being called on the wrong 
executor/thread.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;talk-is-cheap&quot;&gt;Talk is Cheap&lt;/h1&gt;

&lt;p&gt;TL;DR. Enough talk, where’s the code?&lt;/p&gt;

&lt;p&gt;It’s on github at &lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021&quot;&gt;https://github.com/madmongo1/blog-new-year-2021&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;final-notes&quot;&gt;Final Notes.&lt;/h2&gt;

&lt;p&gt;Before signing off I just wanted to cover a few of the features I have completed in this example and a few that I 
have not.&lt;/p&gt;

&lt;h3 id=&quot;whats-done&quot;&gt;What’s Done&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is a limit on the number of concurrent connections to a single host. Host here is defined as a tuple of the 
transport scheme (i.e. http or https), port and hostname. This is currently defaulted to two, but would be trivial to 
change.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is a limit on the number of concurrent requests across all hosts. This defaults to 1000. Simultaneous requests
numbering more than this will be processed through what is an asynchronous semaphore, implemented like this:&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;  while (request_count_ &amp;gt;= max_concurrent_requests_)
  {
      error_code ec;
      co_await concurrent_requests_available_.async_wait(
          net::redirect_error(net::use_awaitable, ec));
  }

  ++request_count_;

  ...
  ... request takes place here
  ...
  
  if (--request_count_ &amp;lt; max_concurrent_requests_)
  {
      concurrent_requests_available_.cancel_one();
  }

&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;whats-not-done&quot;&gt;What’s Not Done&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;HTTP 300 Redirect handling. I consider this to be a higher level concern than connection caching.&lt;/li&gt;
  &lt;li&gt;LRU Connection recycling. At the moment, the program will allow the number of connections to grow without limit if
an unlimited number of different hosts are contacted. In a production system we would need to add more active state to
each connection and have some logic to destroy old unused connections to make way for new ones.&lt;/li&gt;
  &lt;li&gt;The &lt;code&gt;stop_token&lt;/code&gt; is not executor-aware. I have left the stop_token in for exposition but it should not be activated 
by a different executor to the one where the connection to it has been created at present. If you’re interested to 
see how this will look when complete, please submit an issue against the github repo and I’ll update the code and add
a demonstration of it.&lt;/li&gt;
  &lt;li&gt;A Cookie Jar and HTTP session management. Again, these are higher level concerns. The next layer up would take care of
these plus redirect handling.&lt;/li&gt;
  &lt;li&gt;The CMakeLists project in the repo has been tested with GCC 10.2 and Clang-11 on Fedora Linux. Microsoft developers
may need to make the odd tweak to get things working. I’m more than happy to accept PRs.&lt;/li&gt;
  &lt;li&gt;Setting up of the &lt;code&gt;ssl::context&lt;/code&gt; to check peer certificates. Doing this properly is complex enough to warrant another
 blog in its own right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks for reading. I hope you’ve found blog useful. Please by all means get in touch by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;raising an &lt;a href=&quot;https://github.com/madmongo1/blog-new-year-2021/issues&quot;&gt;issue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Contacting me in the #beast channel of &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;CppLang Slack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;email &lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;hodges.r@gmail.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Reusable HTTP(S) Connections Something I am often asked by users of Boost Beast is how to code a client which effectively re-uses a pool of HTTP connections, in the same way a web browser does. The premise is straightforward - if our client is going to be making multiple calls to a web server (or several of them) then it makes sense that once a connection has been used for one request, it is returned to a connection pool so that a subsequent request can make use of it. It also makes sense to have a limit on the number of concurrent connections that can be open against any one host. Otherwise, if the client needs to make multiple requests at the same time, it will end up creating new connections in parallel and lose the efficiency of re-using an existing connection. From these requirements, we can start to think about a design. Firstly, we can imagine a connection cache, with connections kept in a map keyed on host + scheme + port (we can’t re-use an HTTP port for an HTTPS request!). When a request needs a connection, it will either create a new one (connection limit per host not met) or will wait for an existing connection to become available (which implies a condition variable). Once a request has a connection to use, it will send the HTTP request and wait for the response. At this stage, there is a possibility that the active connection which has been allocated could have been idle since the last time it was used. In TCP there is no way to atomically check whether the remote host has closed the connection (or died). The only way to know is to actually read from the socket with a timeout. If the remote host has shutdown the socket, we will be notified as soon as the RST frame arrives at our host. If the remote host has stopped working or the network is bad, we’ll be notified by the timeout. Thus if our read operation results in an error and we have inherited the connection from the cache, we ought to re-try by reopening the connection to the remote host and repeating the write/read operations. However, if there is an error reported on the subsequent attempt, then we can conclude that this is a legitimate error to be reported back to the caller. In simplified pesudocode, the operation might look something like this (assuming we report transport errors as exceptions): response read_write(connection&amp;amp; conn) { response resp; auto retry = false; if(conn.is_initialised()) retry = true; else conn.connect(...); for(;;) { conn.write(request); auto err = conn.read(resp); if (err) { if(!std::exchange(retry, false)) throw system_error(err); request.clear(); conn.disconnect(); } else break; } return resp; } Structuring the Code General Principles In my previous blog I mentioned my preference for writing the implementation of a class in such a way that it does not need to take care of its own lifetime or mutual exclusion. These concerns are deferred to a wrapper or handle. Methods on the handle class take care of marshalling the call to the correct executor (or thread) and preserving the implementation’s lifetime. The implementation need only concern itself with the logic of handling the request. Here’s an example about which I’ll go into more detail later: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { // DRY - define an operation that performs the inner call. auto op = [&amp;amp;] { return impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); }; // deduce the current executor auto my_executor = co_await net::this_coro::executor; // either call directly or via a spawned coroutine co_return impl_-&amp;gt;get_executor() != my_executor ? co_await op() : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable); } Coroutines In this implementation I will be providing a C++20 coroutine interface over Asio executors once again. I am using coroutines because they are easier to write when compared to Asio’s composed operations, but are fundamentally the same thing in a prettier package. Mutual Exclusion For mutual exclusion I will be embedded an asio strand into each active object. The advantage of doing so means that no thread of execution is ever blocked which means we can limit the number of threads in the program to the number of free CPUs giving us maximum throughput of work. In reality of course, one thread is more than enough computing power for almost all asynchronous programs. It’s therefore better to think in terms of one executor per program component, with the implicit guarantee that a given executor will only perform work on one thread at a time. Thinking this way allows us to write code in a way that is agnostic of whether the final program is compiled to be single or multi-threaded. But What About Single-threaded Programs? In order that I don’t need to rewrite code when should I decide to make a single-threaded program multi-threaded or vice versa, I have a couple of little utility functions and types defined in config.hpp Specifically: namespace net { using namespace asio; using io_executor = io_context::executor_type; #ifdef MULTI_THREADED using io_strand = strand&amp;lt; io_executor &amp;gt;; inline io_strand new_strand(io_executor const &amp;amp;src); inline io_strand new_strand(io_strand const &amp;amp;src); #else using io_strand = io_context::executor_type; inline io_strand new_strand(io_executor const &amp;amp;src); #endif inline io_executor to_io_executor(any_io_executor const &amp;amp;src); } // namespace net Any object type in the program which would require its own strand in a multi-threaded program will simply use the type io_strand whether the program is compiled for single-threaded operation or not. Any code that would notionally need to construct a new strand simply calls new_strand(e) where e is either a strand or a naked executor. Any code that needs access to the notional underlying executor would call to_io_executor(e). Determinism Since we’re using asio’s executors for scheduling, it means that we can use asio’s timer as a deterministic, ordered asynchronous condition variable, which means that requests waiting on the connection pool will be offered free connections in the order that they were requested. This guarantee is implicit in the way that the timers’ cancel_one() method is specified. As we’ll see later, asio’s timers also make it trivial to implement an asynchronous semaphore. In this case we use one to ensure that requests are handled concurrently but no more than some upper limit at any one time. Interface I’m going to create a high-level concept called a connection_cache. The interface will be something like this: struct connection_cache { using response_ptr = std::unique_ptr&amp;lt; response &amp;gt;; net::awaitable&amp;lt; response_ptr &amp;gt; rest_call( verb method, std::string const&amp;amp; url, std::optional&amp;lt;std::string&amp;gt; data = std::nullopt, headers hdrs = {}, options opts = {}); }; There are a few things to note here. The return type of the coroutine is a unique_ptr to a response. A natural question might be whether the response should simply be returned by value. However, in practice I have found that there are a number of practical reasons why it’s often better to return the response as a pointer. Firstly it allows conversion to a shared_ptr&amp;lt;const response&amp;gt; in environments where the response might be passed through a directed acyclic graph. Secondly, would allow a further enhancement in that having finished with the response, the client could post it back to the cache, meaning that it could be cached for re-use. The only two required arguments are the method and url. All others can be defaulted. An optional string may be passed which contains the payload of a POST request. This is passed by value because, as we’ll see later,the implementation will want to move this into the request object prior to initiating communications. I have chosen a string type for two reasons text in the form of JSON or XML is the most common form of messaging in REST calls. strings can contain binary data with no loss of efficiency. hdrs is simply a list of headers which should be set in the request. Again, these are passed by value as they will be moved into the request object. The last parameter of the call is an as-yet undefined type called options. This will allow us to add niceties like timeout arguments, a stop_token, redirect policies, a reference to a cookie store and so on. When called, rest_call will attempt to reuse an existing connection. If a connection is not available, it will create a new one if we are under the connection threshold for the deduced host and if not, it will wait until a connection is available. Furthermore, the number of concurrent requests will be throttled to some upper limit. Transport failures will be reported as an exception (of type system_error) and a successful response (even if a 400 or 500) will be returned in the response_ptr. That is to say, as long as the transport layer works out, the code will take the non-exceptional path. Implementation details URL Parsing Among the things I am often asked about in the Beast slack channel and in the Issue Tracker is why there is no URL support in Beast. The is that Beast is a reasonably low level library that concerns itself with the HTTP (and WebSocket) protocols, plus as much buffer and stream management as is necessary to implement HTTP over Asio. The concept of a URL the subject of its own RFCs and is a higher level concern. The C++ Alliance is working on Boost.URL but it is not ready for publishing yet. In the meantime, I found a nifty regex on the internet that more-or-less suffices for our needs: std::tuple&amp;lt; connection_key, std::string &amp;gt; parse_url(std::string const &amp;amp;url) { static const auto url_regex = std::regex( R&quot;regex((http|https)://([^/ :]+):?([^/ ]*)((/?[^ #?]*)\x3f?([^ #]*)#?([^ ]*)))regex&quot;, std::regex_constants::icase | std::regex_constants::optimize); auto match = std::smatch(); if (not std::regex_match(url, match, url_regex)) throw system_error(net::error::invalid_argument, &quot;invalid url&quot;); auto &amp;amp;protocol = match[1]; auto &amp;amp;host = match[2]; auto &amp;amp;port_ind = match[3]; auto &amp;amp;target = match[4]; /* auto &amp;amp;path = match[5]; auto &amp;amp;query = match[6]; auto &amp;amp;fragment = match[7]; */ return std::make_tuple( connection_key { .hostname = host.str(), .port = deduce_port(protocol, port_ind), .scheme = deduce_scheme(protocol, port_ind) }, target.str()); } Exceptions in Asynchronous Code Considered Harmful I hate to admit this, because I am a huge fan of propagating errors as exceptions. This is because the combination of RAII and exception behaviour handling makes error handling very slick in C++. However, coroutines have two rather unpleasant limitations: You can’t call a coroutine in a destructor. You can’t call a coroutine in an exception handling block. There are workarounds. Consider: my_component::~my_component() { // destructor net::co_spawn(get_executor(), [impl = impl_]()-&amp;gt;net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;shutdown(); // destructor of *impl happens here }, net::detached); } This is the destructor of a wrapper object that contains a shared_ptr to its implementation, impl_. In this case we can detect the destruction of my_component and use this to spawn a new coroutine of indeterminate lifetime that takes care of shutting down the actual implementation and then destroying it. This solves the problem of RAII but it mandates that we must author objects that will be used in coroutines as a handle-body pair. We can similarly get around the “no coroutine calls in exception handlers” limitation if we’re prepared to stomach code like this: net::awaitable&amp;lt;void&amp;gt; my_coro() { std::function&amp;lt;net::awaitable(void)&amp;gt; on_error; try { co_await something(); } catch(...) { // set up error_handler on_error = [ep = std::current_exception] { return handler_error_coro(ep); }; } // perhaps handle the error here if (on_error) co_await on_error(); } I think you’ll agree that this is a revolting solution to an unforgivable omission in the language. Not only is it untidy, confusing, difficult to teach and error-prone, it also turns exception handling into same fiasco that is enforced checking of return codes. To add insult to injury, the error handling code in this function takes up 5 times as many lines as the logic! Therefore my recommendation is that in asynchronous coroutine code, it’s better to avoid exceptions and have coroutines either return a tuple of (error_code, possible_value) or a variant containing error-or-value. For example, here is some code from the connection_impl in my example project: net::awaitable&amp;lt; std::tuple&amp;lt; error_code, response_type &amp;gt; &amp;gt; connection_impl::rest_call(request_class const &amp;amp; request, request_options const &amp;amp;options) { auto response = std::make_unique&amp;lt; response_class &amp;gt;(); auto ec = stream_.is_open() ? co_await rest_call(request, *response, options) : net::error::basic_errors::not_connected; if (ec &amp;amp;&amp;amp; ec != net::error::operation_aborted) { ec = co_await connect(options.stop); if (!ec) ec = co_await rest_call(request, *response, options); } if (ec || response-&amp;gt;need_eof()) stream_.close(); co_return std::make_tuple(ec, std::move(response)); } Ensuring that a Coroutine Executes on the Correct Executor In playing with asio coroutines I stumbled upon something that has become an idiom. Consider the situation where a connection class is implemented in terms of a handle and body. The body contains its own executor. In a multi-threaded build, this executor will be a strand while in a single threaded-build we would want it to be simply an io_context::executor_type since there will be no need for any of the thread guards implicit in a strand. Now consider that the implementation has a member coroutine called (say) call. There are two scenarios in which this member will be called. The first is where the caller is executing in the same executor that is associated with the implementation, the second is where the caller is in its own different executor. In the latter case, we must post or spawn the execution of the coroutine onto the implementation’s executor in order to ensure that it runs in the correct sequence with respect to other coroutines initiated against it. The idiom that occurred to me originally was to recursively spawn a coroutine to ensure the call happened on the correct executor: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { auto my_executor = co_await net::this_coro::executor; if (impl_-&amp;gt;get_executor() == my_executor) { co_return co_await impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); } else { // spawn a coroutine which recurses on the correct executor. // wait for this coroutine to finish co_return co_await net::co_spawn( impl_-&amp;gt;get_executor(), [&amp;amp;]() -&amp;gt; net::awaitable&amp;lt; response_type &amp;gt; { return call(method, url, std::move(data), std::move(headers), std::move(options)); }, net::use_awaitable); } } However, this does have the drawback that a code analyser might see the possibility of infinite recursion. After discussing this with Chris, Asio’s author, a better solution was found: net::awaitable&amp;lt; response_type &amp;gt; connection_cache::call(beast::http::verb method, const std::string &amp;amp; url, std::string data, beast::http::fields headers, request_options options) { // DRY - define an operation that performs the inner call. auto op = [&amp;amp;] { return impl_-&amp;gt;call(method, url, std::move(data), std::move(headers), std::move(options)); }; // deduce the current executor auto my_executor = co_await net::this_coro::executor; // either call directly or via a spawned coroutine co_return impl_-&amp;gt;get_executor() != my_executor ? co_await op() : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op, net::use_awaitable); } There are a few things to note: All lifetimes are correct even though the op takes arguments by reference. This is because whichever path we take, our outer coroutine will suspend on the call to op. Note that in the slow path, op is captured by value in the call to co_spawn. Had we written: : co_await net::co_spawn(impl_-&amp;gt;get_executor(), op(), net::use_awaitable); then op would have been called during the setup of the call to co_spawn, which would result in impl_-&amp;gt;call(...) being called on the wrong executor/thread. Talk is Cheap TL;DR. Enough talk, where’s the code? It’s on github at https://github.com/madmongo1/blog-new-year-2021. Final Notes. Before signing off I just wanted to cover a few of the features I have completed in this example and a few that I have not. What’s Done There is a limit on the number of concurrent connections to a single host. Host here is defined as a tuple of the transport scheme (i.e. http or https), port and hostname. This is currently defaulted to two, but would be trivial to change. There is a limit on the number of concurrent requests across all hosts. This defaults to 1000. Simultaneous requests numbering more than this will be processed through what is an asynchronous semaphore, implemented like this: while (request_count_ &amp;gt;= max_concurrent_requests_) { error_code ec; co_await concurrent_requests_available_.async_wait( net::redirect_error(net::use_awaitable, ec)); } ++request_count_; ... ... request takes place here ... if (--request_count_ &amp;lt; max_concurrent_requests_) { concurrent_requests_available_.cancel_one(); } What’s Not Done HTTP 300 Redirect handling. I consider this to be a higher level concern than connection caching. LRU Connection recycling. At the moment, the program will allow the number of connections to grow without limit if an unlimited number of different hosts are contacted. In a production system we would need to add more active state to each connection and have some logic to destroy old unused connections to make way for new ones. The stop_token is not executor-aware. I have left the stop_token in for exposition but it should not be activated by a different executor to the one where the connection to it has been created at present. If you’re interested to see how this will look when complete, please submit an issue against the github repo and I’ll update the code and add a demonstration of it. A Cookie Jar and HTTP session management. Again, these are higher level concerns. The next layer up would take care of these plus redirect handling. The CMakeLists project in the repo has been tested with GCC 10.2 and Clang-11 on Fedora Linux. Microsoft developers may need to make the odd tweak to get things working. I’m more than happy to accept PRs. Setting up of the ssl::context to check peer certificates. Doing this properly is complex enough to warrant another blog in its own right. Thanks for reading. I hope you’ve found blog useful. Please by all means get in touch by: raising an issue Contacting me in the #beast channel of CppLang Slack email hodges.r@gmail.com</summary></entry><entry><title type="html">Richard’s November/December Update</title><link href="http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html" rel="alternate" type="text/html" title="Richard’s November/December Update" /><published>2020-12-22T00:00:00+00:00</published><updated>2020-12-22T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/12/22/RichardsDecemberUpdate.html">&lt;h1 id=&quot;a-coroutine-websocket-using-boost-beast&quot;&gt;A Coroutine Websocket Using Boost Beast&lt;/h1&gt;

&lt;p&gt;This month I thought I would present a little idea that I had a few months ago.&lt;/p&gt;

&lt;p&gt;Boost.Beast is a very comprehensive and competent websocket implementation, but it is not what you might call
“straightforward” to use unless you are already wise in the ways of Asio.&lt;/p&gt;

&lt;p&gt;Beast’s documentation and design makes no apology for this. There is a disclaimer in the 
&lt;a href=&quot;https://www.boost.org/doc/libs/1_75_0/libs/beast/doc/html/beast/using_io/asio_refresher.html&quot;&gt;documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To use Beast effectively, a prior understanding of Networking is required.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is worth taking seriously (and if you are not fully aware of how Asio works with respect to the posting of 
completion handlers onto the associated executor, this page is worth studying).&lt;/p&gt;

&lt;h2 id=&quot;the-interface&quot;&gt;The Interface&lt;/h2&gt;

&lt;p&gt;I wanted to model my websocket object’s interface roughly on the javascript websocket connection interface. There will
be a few differences of course, because the javascript version uses callbacks (or continuations) and I will be using 
C++ coroutines that execute on an Asio executor. In underlying implementation, these concepts are not actually that far
apart, since Asio awaitables are actually implemented in terms of the normal asio completion token/handler interaction.&lt;/p&gt;

&lt;p&gt;Furthermore, I want my WebSocket’s connection phase to be cancellable.&lt;/p&gt;

&lt;p&gt;My websocket interface will look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace websocket
{
    struct message 
    {
        bool is_binary() const;
        bool is_text() const;
        std::string_view operator*() const;
    }; 
    
    struct event 
    {
        bool is_error() const;
        bool is_message() const;
        error_code const&amp;amp; error() const; 
        message const&amp;amp; message() const;
    };
    
    struct connection
    {
        /// Schedule a frame to be sent at some point in the near future
        void 
        send(std::string_view data, bool as_text = true);

        /// Suspend and wait until either there is a message available or an error        
        net::awaitable&amp;lt;event&amp;gt; 
        consume();
        
        /// Close the websocket and suspend until it is closed.
        net::awaitable&amp;lt;void&amp;gt; 
        close(beast::websocket::close_reason = /* a sensible default */); 

        /// Send the close frame to the server but don't hang around to wait
        /// for the confirmation.
        void 
        drop(beast::websocket::close_reason = /* a sensible default */);

        /// If consume() exits with an error of beast::websocket::error::closed then this
        /// will return the reason for the closure as sent by the server.
        /// Otherwise undefined.        
        beast::websocket::close_reason
        reason() const;
    };
    
    net::awaitable&amp;lt;connection&amp;gt; 
    connect(std::string url, 
            connect_options options /* = some default */);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea here is to keep the interface as lightweight and as simple as possible. The websocket connection will run on 
the executor of the coroutine that created it. Any commands sent to the websocket will be executor safe. That is, 
internally their work will be dispatched to the websocket connection’s executor. The exception to this will be the 
close_reason method, which must only be called once the connection’s consume coroutine has returned an error event.
It is a guarantee that once &lt;code&gt;consume&lt;/code&gt; returns an event that is an error, it will never return anything else, and no
other method on the connection will mutate its internal state. In this condition, it is legal to call the &lt;code&gt;reason&lt;/code&gt; 
method.&lt;/p&gt;

&lt;p&gt;A typical use would look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // default options
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); 

    for(;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error())
            break;
        else
            on_message(std::move(event.message()));
    }    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code example does not provide any means to write to the websocket. But it would be trivial to either spawn
another coroutine to handle the writer, or call a function in order to signal some orthogonal process that the websocket 
was ready.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // default options
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); 
    
    on_connected(ws); // The websocket object should be shared-copyable

    for(;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error()) {
            on_close();
            break;
        }
        else
            on_message(std::move(event.message()));
    }    
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another way to visualise a websocket is exactly as javascript’s websocket connection does, using callbacks or 
continuations in order to notify user code that the websocket has received data or closed. It would be trivial to wrap
our coroutine version in order to provide this functionality. We would need to spawn a coroutine in order to run 
the &lt;code&gt;consume()&lt;/code&gt; loop and then somehow signal it to stop if the websocket was disposed of.&lt;/p&gt;

&lt;p&gt;User code might then start to look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    websocket::connect(&quot;wss://echo.websocket.org&quot;, options)
        .on_connect([](websocket::connection ws)
        {
            run_my_loop(ws);
        });
        
void run_my_loop(websocket::connection ws)
{
    bool closed = false;
    ws.on_close([&amp;amp;]{ closed = true; });
    ws.on_error([&amp;amp;]{ closed = true; });
    ws.on_message([&amp;amp;](websocket::message msg){ process_message(msg); });

    // some time later
    ws.send(&quot;Hello, World!&quot;);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this style of interface we would need some means of passing the executor on which the continuations would be 
invoked. A reasonable place to do this might be the &lt;code&gt;options&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;In the JavaScript style interface, it would be important to be able to detect when the websocket has gone out of scope
and ensure that it closes correctly, otherwise we’ll have a rogue resource out there with no handle by which we can 
close it. This argues that the actual &lt;code&gt;websocket::connection&lt;/code&gt; class should be a handle to an internal implementation
and that the destructor of the handle should ensure that the implementation is signalled so that it can &lt;code&gt;drop&lt;/code&gt; the 
connection and shutdown cleanly. Under the covers, we’re implementing this websocket in Boost.Beast. As with all Asio
objects, there could be (probably will be) asynchronous operations in progress at the time the websocket handle goes 
out of scope.&lt;/p&gt;

&lt;p&gt;Thinking this through, it means that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The implementation is going to live longer than the lifetime of the last copy of the handle owning the implementation.&lt;/li&gt;
  &lt;li&gt;There needs to be some mechanism to cancel the underlying implementation’s operations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Coroutines can be visualised as threads of execution. In the world of threads (e.g. &lt;code&gt;std::thread&lt;/code&gt;) we have primitives
such as &lt;code&gt;std::stop_token&lt;/code&gt; and &lt;code&gt;std::condition_variable&lt;/code&gt;. The C++ Standard Library does not yet have these primitives
for coroutines. And if it did it would be questionable whether they would be suitable for networking code where 
coroutines are actually built on top of Asio composed operations. Does Asio itself provide anything we can use?&lt;/p&gt;

&lt;h2 id=&quot;asios-hidden-asynchronous-condition_variable&quot;&gt;Asio’s Hidden Asynchronous condition_variable&lt;/h2&gt;

&lt;p&gt;The answer is surprisingly, yes. But not in the form I was expecting when I asked Chris Kohlhoff (Asio’s author and 
maintainer) about it. It turns out that asio’s timer models an asynchronous version of a condition variable perfectly. 
Consider:&lt;/p&gt;

&lt;p&gt;Given:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;auto t = net::steady_timer(co_await net::this_coro::executor);
t.expires_at(std::chrono::stready_clock::time_point::max());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can write:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;template&amp;lt;class Pred&amp;gt;
net::awaitable&amp;lt;void&amp;gt;
wait(net::steady_timer&amp;amp; t, Pred predicate)
{
    error_code ec;
    while(!ec &amp;amp;&amp;amp; !predicate())
    {
        co_await t.async_wait(net::redirect_error(net::use_awaitable, ec));
        if (ec == net::error::operation_aborted)
            // timer cancelled
            continue;
        else
            throw std::logic_error(&quot;unexpected error&quot;);
    } 
}

void
notify(net::steady_timer&amp;amp; t)
{
    // assuming we are executing on the same executor as the wait()
    t.cancel();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which gives us a simple asynchronous condition_variable (this one does not implement timeouts, but it would be trivial
to extend this code to accommodate them).&lt;/p&gt;

&lt;h2 id=&quot;asynchronous-stop-token&quot;&gt;Asynchronous Stop Token&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;std::stop_token&lt;/code&gt; is a welcome addition to the standard, but it is a little heavyweight for asynchronous code that 
runs in an executor, which is already thread-safe by design. A simple in-executor stop source can be implemented 
something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace async {
namespace detail {
struct shared_state {
    void stop()
    {
        if (!std::exchange(stopped_, true))
        {
            auto sigs = std::move(signals_);
            signals_.clear();
            for(auto&amp;amp; s : sigs)
                s();
        }
    }
    
    std::list&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; signals_;
    bool stopped_ =  false;
};
}
struct stop_source {
    void stop() {
        impl_-&amp;gt;stop();
    }
    std::shared_ptr&amp;lt;shared_state&amp;gt; impl_;
}

struct stop_connection
{
};

struct stop_token
{
    stop_token(stop_source&amp;amp; source)
    : impl_(source.impl_) {
    }
    
    bool 
    stopped() const { return !impl_ || impl_-&amp;gt;stopped_; }
    
    stop_connection 
    connect(std::function&amp;lt;void()&amp;gt; slot);
     
    std::shared_ptr&amp;lt;shared_state&amp;gt; impl_;
}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The use case would look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;
net::awaitable&amp;lt;void&amp;gt;
something_with_websocket(async::stop_token token)
{
    // passing the stop token allows the connect call to abort early
    // if the owner of the stop_source wants to end the use of the
    // websocket before it is connected
    auto ws = websocket::connect(&quot;wss://echo.websocket.org&quot;, 
        websocket::connect_options { .stop_token = token });

    // connect a slot to the stop token which drops the connection        
    auto stopconn = token.connect([&amp;amp;] { ws.drop(); };
    
    for(;;} {
        auto event = co_await ws.consume();
        // ...etc
    }
    
}   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, armed with both a &lt;code&gt;stop_token&lt;/code&gt; and a &lt;code&gt;condition_variable&lt;/code&gt;, we gain a great deal of flexibility with programs 
running on an Asio-style executor.&lt;/p&gt;

&lt;p&gt;So let’s build a little chat app to talk the the echo bot.&lt;/p&gt;

&lt;h2 id=&quot;coding-style-when-using-asio-coroutines&quot;&gt;Coding style when using Asio coroutines.&lt;/h2&gt;

&lt;p&gt;I mentioned earlier that I like to decompose objects with complex lifetimes into an impl and handle. My personal 
programming style for naming the components is as follows:&lt;/p&gt;

&lt;h3 id=&quot;the-implementation&quot;&gt;The implementation&lt;/h3&gt;

&lt;p&gt;This is the class that implements the complex functionality that we want. I generally give this class an &lt;code&gt;_impl&lt;/code&gt; suffix
and apply the following guidelines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The impl does not control its own lifetime.&lt;/li&gt;
  &lt;li&gt;Calls to the impl are expected to already be executing on the correct thread or strand, and in the case of 
multi-threaded code, are expected to have already taken a lock on any mutex.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a personal preference which I find tends to lower the complexity of the object, since the interface functions
do not have to manage more than one concern, and deadlocks etc are not possible.&lt;/p&gt;

&lt;h3 id=&quot;the-lifetime&quot;&gt;The lifetime&lt;/h3&gt;

&lt;p&gt;When holding an object in shared_ptr, we get a chance to intercept the destruction of the last handle. At this point
we do not have to destroy the implementation, but can allow it to shut down gracefully before destruction.
In order to do this, particularly with an object that is referenced by internal coroutines, I have found that it’s 
useful to separate the public lifetime of the object, and its internal lifetime, which may be longer than the public 
one.&lt;/p&gt;

&lt;p&gt;A convenient, if not especially efficient way to do this is to hold two shared_ptr’s in the handle. One being a
shared_ptr&lt;void&gt; which has a custom destructor - the lifetime ptr, and one being a normal shared_ptr to the 
implementation which can be copied in order to extend its private lifetime while it shuts down.
It is the responsibility of the custom deleter to signal the implementation that it should start shutting down.&lt;/void&gt;&lt;/p&gt;

&lt;p&gt;In this case, the websocket connection’s public handle may look something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;namespace websocket {

struct connection_lifetime
{
    connection_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt;&amp;amp;&amp;amp; adopted)
    : impl_(std::move(adopted))
    , lifetime_(new_lifetime(impl_))
    {
    }
    
    static std::shared_ptr&amp;lt;void&amp;gt;
    new_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt; const&amp;amp; impl)
    {
        static int useful_address;
        auto deleter = [impl](int*) noexcept
        {
            net::co_spawn(impl-&amp;gt;get_executor(), 
                [impl]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt;
                { 
                    co_await impl-&amp;gt;stop();
                }, net::detached);
        };
        
        return std::shared_ptr&amp;lt;void&amp;gt;(&amp;amp;useful_address, deleter);
    }
    
    std::shared_ptr&amp;lt;connection_impl&amp;gt; impl_;
    std::shared_ptr&amp;lt;void&amp;gt; lifetime_;
};

struct connection
{
    connection(connection_lifetime l);
};
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The interesting part here is in the function &lt;code&gt;new_lifetime&lt;/code&gt;. There are a few things going on here.
First, we are capturing the internal lifetime of our &lt;code&gt;connection_impl&lt;/code&gt; and storing it in the deleter of the lifetime 
pointer. This of course means that the private implementation will live at least as long as the public lifetime.
Secondly, the deleter does not actually delete anything. It merely captures a copy of the impl pointer and runs a 
coroutine on the impl to completion before releasing the impl pointer. The idea is that this coroutine will not complete
until all internal coroutines within the implementation have completed. The provides the fortunate side effect that
operations running inside the impl do not have to capture the impl’s lifetime via shared_from_this.
It turns out that this aids composability, since subordinate coroutines within the implementation can be written as free
functions, and ported to other implementations that may not involve a shared_ptr.
It also means that the impl itself can be composed, since it has no restrictions on lifetime semantics.
i.e. If I wanted to implement a JSON-RPC connection by deriving from the websocket::connection_impl, I do not have to 
be concerned about translating shared_ptrs internally in the derived class.&lt;/p&gt;

&lt;h1 id=&quot;once-its-all-put-together&quot;&gt;Once it’s all put together&lt;/h1&gt;

&lt;p&gt;Finally, having created all the primitives (which I really should start collating into a library), we can test our
little websocket chat client, which becomes a very simple program:&lt;/p&gt;

&lt;p&gt;Here’s main:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int
main()
{
    net::io_context ioctx;

    net::co_spawn(
        ioctx.get_executor(), [] { return chat(); }, net::detached);

    ioctx.run();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here’s the chat() coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; void &amp;gt;
chat()
{
    // connect the websocket
    auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;);

    // spawn the coroutine to read console input and send it to the websocket
    auto stop_children = async::stop_source();
    net::co_spawn(
        co_await net::this_coro::executor,
        [stop = async::stop_token(stop_children), ws]() mutable {
            return do_console(std::move(stop), std::move(ws));
        },
        net::detached);

    // read events from the websocket connection.
    for (;;)
    {
        auto event = co_await ws.consume();
        if (event.is_error())
        {
            if (event.error() == beast::websocket::error::closed)
                std::cerr &amp;lt;&amp;lt; &quot;peer closed connection: &quot; &amp;lt;&amp;lt; ws.reason()
                          &amp;lt;&amp;lt; std::endl;
            else
                std::cerr &amp;lt;&amp;lt; &quot;connection error: &quot; &amp;lt;&amp;lt; event.error() &amp;lt;&amp;lt; std::endl;
            break;
        }
        else
        {
            std::cout &amp;lt;&amp;lt; &quot;message received: &quot; &amp;lt;&amp;lt; event.message() &amp;lt;&amp;lt; std::endl;
        }
    }
    
    // at this point, the stop_source goes out of scope, 
    // which will cause the console coroutine to exit.
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, the do_console() coroutine. Note that I have used asio’s posix interface to collect console input. 
In order to run compile in a WIN32 environment, we’d need to do something different (suggestions welcome via PR!).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt; void &amp;gt;
do_console(async::stop_token stop, websocket::connection ws)
try
{
    auto console = asio::posix::stream_descriptor(
        co_await net::this_coro::executor, ::dup(STDIN_FILENO));
    auto stopconn = stop.connect([&amp;amp;] { console.cancel(); });

    std::string console_chars;
    while (!stop.stopped())
    {
        auto line_len =
            co_await net::async_read_until(console,
                                           net::dynamic_buffer(console_chars),
                                           '\n',
                                           net::use_awaitable);
        auto line = console_chars.substr(0, line_len - 1);
        console_chars.erase(0, line_len);
        std::cout &amp;lt;&amp;lt; &quot;you typed this: &quot; &amp;lt;&amp;lt; line &amp;lt;&amp;lt; std::endl;
        ws.send(line);
    }
}
catch(...) {
  // error handling here
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’d like to look into the complete code, submit a PR or offer some (probably well-deserved) criticism, you will
find the &lt;a href=&quot;https://github.com/madmongo1/blog-december-2020&quot;&gt;code repository here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">A Coroutine Websocket Using Boost Beast This month I thought I would present a little idea that I had a few months ago. Boost.Beast is a very comprehensive and competent websocket implementation, but it is not what you might call “straightforward” to use unless you are already wise in the ways of Asio. Beast’s documentation and design makes no apology for this. There is a disclaimer in the documentation: To use Beast effectively, a prior understanding of Networking is required. This is worth taking seriously (and if you are not fully aware of how Asio works with respect to the posting of completion handlers onto the associated executor, this page is worth studying). The Interface I wanted to model my websocket object’s interface roughly on the javascript websocket connection interface. There will be a few differences of course, because the javascript version uses callbacks (or continuations) and I will be using C++ coroutines that execute on an Asio executor. In underlying implementation, these concepts are not actually that far apart, since Asio awaitables are actually implemented in terms of the normal asio completion token/handler interaction. Furthermore, I want my WebSocket’s connection phase to be cancellable. My websocket interface will look something like this: namespace websocket { struct message { bool is_binary() const; bool is_text() const; std::string_view operator*() const; }; struct event { bool is_error() const; bool is_message() const; error_code const&amp;amp; error() const; message const&amp;amp; message() const; }; struct connection { /// Schedule a frame to be sent at some point in the near future void send(std::string_view data, bool as_text = true); /// Suspend and wait until either there is a message available or an error net::awaitable&amp;lt;event&amp;gt; consume(); /// Close the websocket and suspend until it is closed. net::awaitable&amp;lt;void&amp;gt; close(beast::websocket::close_reason = /* a sensible default */); /// Send the close frame to the server but don't hang around to wait /// for the confirmation. void drop(beast::websocket::close_reason = /* a sensible default */); /// If consume() exits with an error of beast::websocket::error::closed then this /// will return the reason for the closure as sent by the server. /// Otherwise undefined. beast::websocket::close_reason reason() const; }; net::awaitable&amp;lt;connection&amp;gt; connect(std::string url, connect_options options /* = some default */); } The idea here is to keep the interface as lightweight and as simple as possible. The websocket connection will run on the executor of the coroutine that created it. Any commands sent to the websocket will be executor safe. That is, internally their work will be dispatched to the websocket connection’s executor. The exception to this will be the close_reason method, which must only be called once the connection’s consume coroutine has returned an error event. It is a guarantee that once consume returns an event that is an error, it will never return anything else, and no other method on the connection will mutate its internal state. In this condition, it is legal to call the reason method. A typical use would look like this: // default options auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); for(;;) { auto event = co_await ws.consume(); if (event.is_error()) break; else on_message(std::move(event.message())); } The above code example does not provide any means to write to the websocket. But it would be trivial to either spawn another coroutine to handle the writer, or call a function in order to signal some orthogonal process that the websocket was ready. // default options auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); on_connected(ws); // The websocket object should be shared-copyable for(;;) { auto event = co_await ws.consume(); if (event.is_error()) { on_close(); break; } else on_message(std::move(event.message())); } Another way to visualise a websocket is exactly as javascript’s websocket connection does, using callbacks or continuations in order to notify user code that the websocket has received data or closed. It would be trivial to wrap our coroutine version in order to provide this functionality. We would need to spawn a coroutine in order to run the consume() loop and then somehow signal it to stop if the websocket was disposed of. User code might then start to look something like this: websocket::connect(&quot;wss://echo.websocket.org&quot;, options) .on_connect([](websocket::connection ws) { run_my_loop(ws); }); void run_my_loop(websocket::connection ws) { bool closed = false; ws.on_close([&amp;amp;]{ closed = true; }); ws.on_error([&amp;amp;]{ closed = true; }); ws.on_message([&amp;amp;](websocket::message msg){ process_message(msg); }); // some time later ws.send(&quot;Hello, World!&quot;); } With this style of interface we would need some means of passing the executor on which the continuations would be invoked. A reasonable place to do this might be the options parameter. In the JavaScript style interface, it would be important to be able to detect when the websocket has gone out of scope and ensure that it closes correctly, otherwise we’ll have a rogue resource out there with no handle by which we can close it. This argues that the actual websocket::connection class should be a handle to an internal implementation and that the destructor of the handle should ensure that the implementation is signalled so that it can drop the connection and shutdown cleanly. Under the covers, we’re implementing this websocket in Boost.Beast. As with all Asio objects, there could be (probably will be) asynchronous operations in progress at the time the websocket handle goes out of scope. Thinking this through, it means that: The implementation is going to live longer than the lifetime of the last copy of the handle owning the implementation. There needs to be some mechanism to cancel the underlying implementation’s operations. Coroutines can be visualised as threads of execution. In the world of threads (e.g. std::thread) we have primitives such as std::stop_token and std::condition_variable. The C++ Standard Library does not yet have these primitives for coroutines. And if it did it would be questionable whether they would be suitable for networking code where coroutines are actually built on top of Asio composed operations. Does Asio itself provide anything we can use? Asio’s Hidden Asynchronous condition_variable The answer is surprisingly, yes. But not in the form I was expecting when I asked Chris Kohlhoff (Asio’s author and maintainer) about it. It turns out that asio’s timer models an asynchronous version of a condition variable perfectly. Consider: Given: auto t = net::steady_timer(co_await net::this_coro::executor); t.expires_at(std::chrono::stready_clock::time_point::max()); Then we can write: template&amp;lt;class Pred&amp;gt; net::awaitable&amp;lt;void&amp;gt; wait(net::steady_timer&amp;amp; t, Pred predicate) { error_code ec; while(!ec &amp;amp;&amp;amp; !predicate()) { co_await t.async_wait(net::redirect_error(net::use_awaitable, ec)); if (ec == net::error::operation_aborted) // timer cancelled continue; else throw std::logic_error(&quot;unexpected error&quot;); } } void notify(net::steady_timer&amp;amp; t) { // assuming we are executing on the same executor as the wait() t.cancel(); } Which gives us a simple asynchronous condition_variable (this one does not implement timeouts, but it would be trivial to extend this code to accommodate them). Asynchronous Stop Token The std::stop_token is a welcome addition to the standard, but it is a little heavyweight for asynchronous code that runs in an executor, which is already thread-safe by design. A simple in-executor stop source can be implemented something like this: namespace async { namespace detail { struct shared_state { void stop() { if (!std::exchange(stopped_, true)) { auto sigs = std::move(signals_); signals_.clear(); for(auto&amp;amp; s : sigs) s(); } } std::list&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; signals_; bool stopped_ = false; }; } struct stop_source { void stop() { impl_-&amp;gt;stop(); } std::shared_ptr&amp;lt;shared_state&amp;gt; impl_; } struct stop_connection { }; struct stop_token { stop_token(stop_source&amp;amp; source) : impl_(source.impl_) { } bool stopped() const { return !impl_ || impl_-&amp;gt;stopped_; } stop_connection connect(std::function&amp;lt;void()&amp;gt; slot); std::shared_ptr&amp;lt;shared_state&amp;gt; impl_; } } The use case would look something like this: net::awaitable&amp;lt;void&amp;gt; something_with_websocket(async::stop_token token) { // passing the stop token allows the connect call to abort early // if the owner of the stop_source wants to end the use of the // websocket before it is connected auto ws = websocket::connect(&quot;wss://echo.websocket.org&quot;, websocket::connect_options { .stop_token = token }); // connect a slot to the stop token which drops the connection auto stopconn = token.connect([&amp;amp;] { ws.drop(); }; for(;;} { auto event = co_await ws.consume(); // ...etc } } Now, armed with both a stop_token and a condition_variable, we gain a great deal of flexibility with programs running on an Asio-style executor. So let’s build a little chat app to talk the the echo bot. Coding style when using Asio coroutines. I mentioned earlier that I like to decompose objects with complex lifetimes into an impl and handle. My personal programming style for naming the components is as follows: The implementation This is the class that implements the complex functionality that we want. I generally give this class an _impl suffix and apply the following guidelines: The impl does not control its own lifetime. Calls to the impl are expected to already be executing on the correct thread or strand, and in the case of multi-threaded code, are expected to have already taken a lock on any mutex. This is a personal preference which I find tends to lower the complexity of the object, since the interface functions do not have to manage more than one concern, and deadlocks etc are not possible. The lifetime When holding an object in shared_ptr, we get a chance to intercept the destruction of the last handle. At this point we do not have to destroy the implementation, but can allow it to shut down gracefully before destruction. In order to do this, particularly with an object that is referenced by internal coroutines, I have found that it’s useful to separate the public lifetime of the object, and its internal lifetime, which may be longer than the public one. A convenient, if not especially efficient way to do this is to hold two shared_ptr’s in the handle. One being a shared_ptr which has a custom destructor - the lifetime ptr, and one being a normal shared_ptr to the implementation which can be copied in order to extend its private lifetime while it shuts down. It is the responsibility of the custom deleter to signal the implementation that it should start shutting down. In this case, the websocket connection’s public handle may look something like this: namespace websocket { struct connection_lifetime { connection_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt;&amp;amp;&amp;amp; adopted) : impl_(std::move(adopted)) , lifetime_(new_lifetime(impl_)) { } static std::shared_ptr&amp;lt;void&amp;gt; new_lifetime(std::shared_ptr&amp;lt;connection_impl&amp;gt; const&amp;amp; impl) { static int useful_address; auto deleter = [impl](int*) noexcept { net::co_spawn(impl-&amp;gt;get_executor(), [impl]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;stop(); }, net::detached); }; return std::shared_ptr&amp;lt;void&amp;gt;(&amp;amp;useful_address, deleter); } std::shared_ptr&amp;lt;connection_impl&amp;gt; impl_; std::shared_ptr&amp;lt;void&amp;gt; lifetime_; }; struct connection { connection(connection_lifetime l); }; } The interesting part here is in the function new_lifetime. There are a few things going on here. First, we are capturing the internal lifetime of our connection_impl and storing it in the deleter of the lifetime pointer. This of course means that the private implementation will live at least as long as the public lifetime. Secondly, the deleter does not actually delete anything. It merely captures a copy of the impl pointer and runs a coroutine on the impl to completion before releasing the impl pointer. The idea is that this coroutine will not complete until all internal coroutines within the implementation have completed. The provides the fortunate side effect that operations running inside the impl do not have to capture the impl’s lifetime via shared_from_this. It turns out that this aids composability, since subordinate coroutines within the implementation can be written as free functions, and ported to other implementations that may not involve a shared_ptr. It also means that the impl itself can be composed, since it has no restrictions on lifetime semantics. i.e. If I wanted to implement a JSON-RPC connection by deriving from the websocket::connection_impl, I do not have to be concerned about translating shared_ptrs internally in the derived class. Once it’s all put together Finally, having created all the primitives (which I really should start collating into a library), we can test our little websocket chat client, which becomes a very simple program: Here’s main: int main() { net::io_context ioctx; net::co_spawn( ioctx.get_executor(), [] { return chat(); }, net::detached); ioctx.run(); } And here’s the chat() coroutine: net::awaitable&amp;lt; void &amp;gt; chat() { // connect the websocket auto ws = co_await websocket::connect(&quot;wss://echo.websocket.org&quot;); // spawn the coroutine to read console input and send it to the websocket auto stop_children = async::stop_source(); net::co_spawn( co_await net::this_coro::executor, [stop = async::stop_token(stop_children), ws]() mutable { return do_console(std::move(stop), std::move(ws)); }, net::detached); // read events from the websocket connection. for (;;) { auto event = co_await ws.consume(); if (event.is_error()) { if (event.error() == beast::websocket::error::closed) std::cerr &amp;lt;&amp;lt; &quot;peer closed connection: &quot; &amp;lt;&amp;lt; ws.reason() &amp;lt;&amp;lt; std::endl; else std::cerr &amp;lt;&amp;lt; &quot;connection error: &quot; &amp;lt;&amp;lt; event.error() &amp;lt;&amp;lt; std::endl; break; } else { std::cout &amp;lt;&amp;lt; &quot;message received: &quot; &amp;lt;&amp;lt; event.message() &amp;lt;&amp;lt; std::endl; } } // at this point, the stop_source goes out of scope, // which will cause the console coroutine to exit. } And finally, the do_console() coroutine. Note that I have used asio’s posix interface to collect console input. In order to run compile in a WIN32 environment, we’d need to do something different (suggestions welcome via PR!). net::awaitable&amp;lt; void &amp;gt; do_console(async::stop_token stop, websocket::connection ws) try { auto console = asio::posix::stream_descriptor( co_await net::this_coro::executor, ::dup(STDIN_FILENO)); auto stopconn = stop.connect([&amp;amp;] { console.cancel(); }); std::string console_chars; while (!stop.stopped()) { auto line_len = co_await net::async_read_until(console, net::dynamic_buffer(console_chars), '\n', net::use_awaitable); auto line = console_chars.substr(0, line_len - 1); console_chars.erase(0, line_len); std::cout &amp;lt;&amp;lt; &quot;you typed this: &quot; &amp;lt;&amp;lt; line &amp;lt;&amp;lt; std::endl; ws.send(line); } } catch(...) { // error handling here } If you’d like to look into the complete code, submit a PR or offer some (probably well-deserved) criticism, you will find the code repository here.</summary></entry><entry><title type="html">Richard’s October Update</title><link href="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html" rel="alternate" type="text/html" title="Richard’s October Update" /><published>2020-10-31T00:00:00+00:00</published><updated>2020-10-31T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/10/31/RichardsOctoberUpdate.html">&lt;h1 id=&quot;asio-coroutines-in-qt-applications&quot;&gt;Asio Coroutines in Qt applications!&lt;/h1&gt;

&lt;p&gt;I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end.
One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web 
technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the 
popular-but-so-far-unused-by-me C++ GUI framework, Qt.&lt;/p&gt;

&lt;p&gt;The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines.&lt;/p&gt;

&lt;p&gt;In the end it turned out to be easier than I had expected. Here’s how.&lt;/p&gt;

&lt;h2 id=&quot;a-simple-executor&quot;&gt;A simple Executor&lt;/h2&gt;

&lt;p&gt;As mentioned in a previous blog, Asio comes with a full implementation of the 
&lt;a href=&quot;http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p0443r12.html&quot;&gt;Unified Executors proposal&lt;/a&gt;. Asio coroutines
are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will
perform work in a Qt UI thread.&lt;/p&gt;

&lt;p&gt;The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it 
compatible with &lt;code&gt;asio::any_io_executor&lt;/code&gt;. This means it needs to have an associated 
&lt;a href=&quot;https://www.boost.org/doc/libs/1_74_0/doc/html/boost_asio/reference/execution_context.html&quot;&gt;execution context&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference
to the Application. Although Qt defines the macro &lt;code&gt;qApp&lt;/code&gt; to resolve to a pointer to the “current” application, for 
testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so 
that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient
when writing components to not have to specifically create and pass an an execution context to windows within the Qt 
application so it makes sense to be able to provide access to a default context which references the default application.
Here’s a first cut:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_execution_context : net::execution_context
    , boost::noncopyable
{
    qt_execution_context(QApplication *app = qApp)
        : app_(app)
    {
        instance_ = this;
    }

    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // todo
    }

    static qt_execution_context &amp;amp;
    singleton()
    {
        assert(instance_);
        return *instance_;
    }

private:
    static qt_execution_context *instance_;
    QApplication *app_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This class will provide two services. The first is to provide the asio service infrastructure so that we can create 
timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually
dispatch work in a Qt application. This is the purpose of the &lt;code&gt;post&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by 
children of the application. We can use this infrastructure to ensure that work dispatched by this execution context
actually takes place on the correct thread and at the correct time.&lt;/p&gt;

&lt;p&gt;In order for us to dispatch work to the application, we need to wrap our function into a QEvent:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_work_event_base : public QEvent
{
public:
    qt_work_event_base()
        : QEvent(generated_type())
    {
    }

    virtual void
    invoke() = 0;

    static QEvent::Type
    generated_type()
    {
        static int event_type = QEvent::registerEventType();
        return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type);
    }
};

template&amp;lt;class F&amp;gt;
struct basic_qt_work_event : qt_work_event_base
{
    basic_qt_work_event(F f)
        : f_(std::move(f))
    {}

    void
    invoke() override
    {
        f_();
    }

private:
    F f_;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As opposed to using a &lt;code&gt;std::function&lt;/code&gt;, the &lt;code&gt;basic_qt_work_event&lt;/code&gt; allows us to wrap a move-only function object, which is 
important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as 
it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in 
execution performance.&lt;/p&gt;

&lt;p&gt;Now we just need to fill out the code for &lt;code&gt;qt_execution_context::post&lt;/code&gt; and provide a mechanism in the Qt application to
detect and dispatch these messages:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    template&amp;lt;class F&amp;gt;
    void
    post(F f)
    {
        // c++20 auto template deduction
        auto event = new basic_qt_work_event(std::move(f));
        QApplication::postEvent(app_, event);
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class qt_net_application : public QApplication
{
    using QApplication::QApplication;

protected:
    bool
    event(QEvent *event) override;
};

bool
qt_net_application::event(QEvent *event)
{
    if (event-&amp;gt;type() == qt_work_event_base::generated_type())
    {
        auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event);
        p-&amp;gt;accept();
        p-&amp;gt;invoke();
        return true;
    }
    else
    {
        return QApplication::event(event);
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the 
&lt;code&gt;QEvent&lt;/code&gt;-derived event. This would mean no necessity of custom event handling in the &lt;code&gt;QApplication&lt;/code&gt; but there are two
problems that I can see with this approach:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;I don’t know enough about Qt to know that this is safe and correct, and&lt;/li&gt;
  &lt;li&gt;Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour
is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually
mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can 
be used in an &lt;code&gt;any_io_executor&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_executor
{
    qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
    {
    }

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template &amp;lt; typename OtherAllocator &amp;gt;
    static constexpr auto query(
    net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto
    query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        context_-&amp;gt;post(std::move(f));
    }

    bool
    operator==(qt_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_;
    }

    bool
    operator!=(qt_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
};


static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget : public QTextEdit
{
    Q_OBJECT
public:
    using QTextEdit::QTextEdit;

private:
    void
    showEvent(QShowEvent *event) override;

    void
    hideEvent(QHideEvent *event) override;

    net::awaitable&amp;lt;void&amp;gt;
    run_demo();
};

void
test_widget::showEvent(QShowEvent *event)
{
    net::co_spawn(
    qt_executor(), [this] {
        return run_demo();
    },
    net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    QWidget::hideEvent(event);
}

net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto timer = net::high_resolution_timer(co_await net::this_coro::executor);

    for (int i = 0; i &amp;lt; 10; ++i)
    {
        timer.expires_after(1s);
        co_await timer.async_wait(net::use_awaitable);
        this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
    }
    co_return;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-1&quot;&gt;stage 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here is a screenshot of the app running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-1.png&quot; alt=&quot;app running&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;all-very-well&quot;&gt;All very well…&lt;/h2&gt;

&lt;p&gt;OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven 
system in terms of procedural expression of code in a coroutine.&lt;/p&gt;

&lt;p&gt;But what if the user closes the window before the coroutine completes?&lt;/p&gt;

&lt;p&gt;This application has created the window on the stack, but in a larger application, there will be multiple windows and 
they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to 
run once the windows that’s hosting it is deleted, we are sure to get a segfault.&lt;/p&gt;

&lt;p&gt;One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the 
coroutine if destroyed. A &lt;code&gt;std::shared_ptr/weak_ptr&lt;/code&gt; pair would seem like a sensible solution. Let’s create an updated
version of the executor:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct qt_guarded_executor
{
    qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard,
                        qt_execution_context &amp;amp;context
                        = qt_execution_context::singleton()) noexcept
        : context_(std::addressof(context))
        , guard_(std::move(guard))
    {}

    qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept
    {
        return *context_;
    }

    static constexpr net::execution::blocking_t
    query(net::execution::blocking_t) noexcept
    {
        return net::execution::blocking.never;
    }

    static constexpr net::execution::relationship_t
    query(net::execution::relationship_t) noexcept
    {
        return net::execution::relationship.fork;
    }

    static constexpr net::execution::outstanding_work_t
    query(net::execution::outstanding_work_t) noexcept
    {
        return net::execution::outstanding_work.tracked;
    }

    template&amp;lt;typename OtherAllocator&amp;gt;
    static constexpr auto
    query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept
    {
        return std::allocator&amp;lt;void&amp;gt;();
    }

    template&amp;lt;class F&amp;gt;
    void
    execute(F f) const
    {
        if (auto lock1 = guard_.lock())
        {
            context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable {
                if (auto lock2 = guard.lock())
                    f();
            });
        }
    }

    bool
    operator==(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_)
            &amp;amp;&amp;amp; !other.guard_.owner_before(guard_);
    }

    bool
    operator!=(qt_guarded_executor const &amp;amp;other) const noexcept
    {
        return !(*this == other);
    }

private:
    qt_execution_context *context_;
    std::weak_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct has_guarded_executor
{
    using executor_type = qt_guarded_executor;

    has_guarded_executor(qt_execution_context &amp;amp;ctx
                         = qt_execution_context::singleton())
        : context_(std::addressof(ctx))
    {
        new_guard();
    }

    void
    new_guard()
    {
        static int x = 0;
        guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x),
                                      // no-op deleter
                                      [](auto *) {});
    }

    void
    reset_guard()
    {
        guard_.reset();
    }

    executor_type
    get_executor() const
    {
        return qt_guarded_executor(guard_, *context_);
    }

private:
    qt_execution_context *context_;
    std::shared_ptr&amp;lt;void&amp;gt; guard_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we can modify the &lt;code&gt;test_widget&lt;/code&gt; to use it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;class test_widget
    : public QTextEdit
    , public has_guarded_executor
{
    ...
};

void
test_widget::showEvent(QShowEvent *event)
{
    // stop all existing coroutines and create a new guard
    new_guard();

    // start our coroutine
    net::co_spawn(
        get_executor(), [this] { return run_demo(); }, net::detached);

    QTextEdit::showEvent(event);
}

void
test_widget::hideEvent(QHideEvent *event)
{
    // stop all coroutines
    reset_guard();
    QWidget::hideEvent(event);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow
and add a menu with an action to create new widgets.&lt;/p&gt;

&lt;p&gt;We are now able to create and destroy widgets at will, with no segfaults.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-october/stage-2.png&quot; alt=&quot;MDI app running&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to 
be cancelled early.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    // test_widget.hpp

    void
    listen_for_stop(std::function&amp;lt;void()&amp;gt; slot);

    void
    stop_all();

    std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_;
    bool stopped_ = false;

    // test_widget.cpp

    void
    test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot)
    {
        if (stopped_)
            return slot();
    
        stop_signals_.push_back(std::move(slot));
    }
    
    void
    test_widget::stop_all()
    {
        stopped_ = true;
        auto copy = std::exchange(stop_signals_, {});
        for (auto &amp;amp;slot : copy) slot();
    }
    
    void
    test_widget::closeEvent(QCloseEvent *event)
    {
        stop_all();
        QWidget::closeEvent(event);
    }

    net::awaitable&amp;lt;void&amp;gt;
    test_widget::run_demo()
    {
        using namespace std::literals;
    
        auto timer = net::high_resolution_timer(co_await net::this_coro::executor);
    
        auto done = false;
    
        listen_for_stop([&amp;amp;] {
            done = true;
            timer.cancel();
        });
    
        while (!done)
        {
            for (int i = 0; i &amp;lt; 10; ++i)
            {
                timer.expires_after(1s);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(
                    QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;));
            }
    
            for (int i = 10; i--;)
            {
                timer.expires_after(250ms);
                auto ec = boost::system::error_code();
                co_await timer.async_wait(
                    net::redirect_error(net::use_awaitable, ec));
                if (ec)
                {
                    done = true;
                    break;
                }
                this-&amp;gt;setText(QString::fromStdString(std::to_string(i)));
            }
        }
        co_return;
    }

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, 
nonsense.&lt;/p&gt;

&lt;p&gt;Here’s the code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-2&quot;&gt;stage 2&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;state-of-the-art&quot;&gt;State of the Art&lt;/h2&gt;

&lt;p&gt;It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. 
I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of 
Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting 
segfaults at run time.&lt;/p&gt;

&lt;p&gt;Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code 
on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install llvm
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clang will then be available in &lt;code&gt;/usr/local/opt/bin&lt;/code&gt; and you will need to set your &lt;code&gt;CMAKE_CXX_COMPILER&lt;/code&gt; CMake variable
appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to
set &lt;code&gt;Qt5_DIR&lt;/code&gt;. Something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;going-further&quot;&gt;Going further&lt;/h3&gt;

&lt;p&gt;Ok, so what if we want our Qt application to interact with some asio-based service running in another thread?&lt;/p&gt;

&lt;p&gt;For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running
and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that 
executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of
the threads assigned to it.&lt;/p&gt;

&lt;p&gt;It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise
with each other.&lt;/p&gt;

&lt;p&gt;In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”.&lt;/p&gt;

&lt;p&gt;In standard C++, we have the class &lt;code&gt;std::condition_variable&lt;/code&gt; which we can wait on for some condition to be fulfilled.
If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the 
basis of an asynchronous event queue.&lt;/p&gt;

&lt;p&gt;First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks
to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, 
write my own awaitable type!):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;struct async_condition_variable
{
private:
    using timer_type = net::high_resolution_timer;

public:
    using clock_type = timer_type::clock_type;
    using duration = timer_type::duration;
    using time_point = timer_type::time_point;
    using executor_type = timer_type::executor_type;

    /// Constructor
    /// @param exec is the executor to associate with the internal timer.
    explicit inline async_condition_variable(net::any_io_executor exec);

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    template&amp;lt;class Pred&amp;gt;
    [[nodiscard]]
    auto
    wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;;

    auto
    get_executor() noexcept -&amp;gt; executor_type
    {
        return timer_.get_executor();
    }

    inline void
    notify_one();

    inline void
    notify_all();

    /// Put the condition into a stop state so that all future awaits fail.
    inline void
    stop();

    auto
    error() const -&amp;gt; error_code const &amp;amp;
    {
        return error_;
    }

    void
    reset()
    {
        error_ = {};
    }

private:
    timer_type timer_;
    error_code error_;
    std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_;
};

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_until(Pred pred, time_point limit)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    assert(co_await net::this_coro::executor == timer_.get_executor());

    while (not error_ and not pred())
    {
        if (auto now = clock_type::now(); now &amp;gt;= limit)
            co_return std::cv_status::timeout;

        // insert our expiry time into the set and remember where it is
        auto where = wait_times_.insert(limit);

        // find the nearest expiry time and set the timeout for that one
        auto when = *wait_times_.begin();
        if (timer_.expiry() != when)
            timer_.expires_at(when);

        // wait for timeout or cancellation
        error_code ec;
        co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec));

        // remove our expiry time from the set
        wait_times_.erase(where);

        // any error other than operation_aborted is unexpected
        if (ec and ec != net::error::operation_aborted)
            if (not error_)
                error_ = ec;
    }

    if (error_)
        throw system_error(error_);

    co_return std::cv_status::no_timeout;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    auto stat = co_await wait_until(std::move(pred), time_point::max());
    boost::ignore_unused(stat);
    co_return;
}

template&amp;lt;class Pred&amp;gt;
auto
async_condition_variable::wait_for(Pred pred, duration d)
    -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;
{
    return wait_until(std::move(pred), clock_type::now() + d);
}

async_condition_variable::async_condition_variable(net::any_io_executor exec)
    : timer_(std::move(exec))
    , error_()
{}

void
async_condition_variable::notify_one()
{
    timer_.cancel_one();
}

void
async_condition_variable::notify_all()
{
    timer_.cancel();
}

void
async_condition_variable::stop()
{
    error_ = net::error::operation_aborted;
    notify_all();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple
coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments.
You will notice that I have marked the coroutines as &lt;code&gt;[[nodiscard]]&lt;/code&gt;. This is to ensure that I don’t forget to 
&lt;code&gt;co_await&lt;/code&gt; them at the call site. I can’t tell you how many times I have done that and then wondered why my program
mysteriously freezes mid run.&lt;/p&gt;

&lt;p&gt;Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some
shared state which contains an  &lt;code&gt;async_condition_variable&lt;/code&gt; and some kind of queue. I have made the implementation of the 
queue a template function (another over-complication for our purposes). The template represents the strategy for 
accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means 
that every message posted will be consumed in the order in which they were posted. But it could just as easily be a 
priority queue, or a latch - i.e. only storing the most recent message.&lt;/p&gt;

&lt;p&gt;The code to describe this machinery is a little long to put inline, but by all means look at the code:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_connection.hpp&quot;&gt;basic_connection&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_distributor.hpp&quot;&gt;basic_distributor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/basic_shared_state.hpp&quot;&gt;basic_shared_state&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less
a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time
via the basic_distributor.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.hpp&quot;&gt;message_service.hpp&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/blob/stage-3/src/message_service.cpp&quot;&gt;message_service.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself
non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer
than the handle, while it shuts itself down.&lt;/p&gt;

&lt;p&gt;The main coroutine on the impl is called &lt;code&gt;run()&lt;/code&gt; and it is initiated when the impl is created:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;message_service::message_service(const executor_type &amp;amp;exec)
    : exec_(exec)
    , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_))
{
    net::co_spawn(
        impl_-&amp;gt;get_executor(),
        [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); },
        net::detached);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;impl&lt;/code&gt; shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the
lambda is just a class who’s &lt;code&gt;operator()&lt;/code&gt; happens to be a coroutine. This means that the actual coroutine can outlive the
lambda that initiated it, which means that &lt;code&gt;impl&lt;/code&gt; could be destroyed before the coroutine finishes. For this reason
it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the 
coroutine state.
However, in this case we’re safe. &lt;code&gt;net::co_spawn&lt;/code&gt; will actually copy the lambda object before invoking it, guaranteeing&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with asio at least - that the impl will survive the execution of the coroutine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And here’s the &lt;code&gt;run()&lt;/code&gt; coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
message_service_impl::run()
{
    using namespace std::literals;

    auto timer
        = net::high_resolution_timer(co_await net::this_coro::executor);

    auto done = false;

    listen_for_stop([&amp;amp;] {
      done = true;
      timer.cancel();
    });

    while (!done)
    {
        for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i)
        {
            timer.expires_after(1s);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;);
        }

        for (int i = 10; i-- &amp;amp;&amp;amp; !done;)
        {
            timer.expires_after(250ms);
            auto ec = boost::system::error_code();
            co_await timer.async_wait(
                net::redirect_error(net::use_awaitable, ec));
            if (ec)
                break;
            message_dist_.notify_value(std::to_string(i));
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the &lt;code&gt;done&lt;/code&gt; machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The
first this coroutine will hear of it is when one of the timer &lt;code&gt;async_wait&lt;/code&gt; calls is canceled. Note that the lambda
passed to &lt;code&gt;listen_for_stop&lt;/code&gt; &lt;em&gt;is not actually part of the coroutine&lt;/em&gt;. It is a separate function that just happens to 
refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation
and the &lt;code&gt;done&lt;/code&gt; flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed 
by the same &lt;code&gt;strand&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finally we need to modify the widget:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;net::awaitable&amp;lt;void&amp;gt;
test_widget::run_demo()
{
    using namespace std::literals;

    auto service = message_service(ioexec_);
    auto conn = co_await service.connect();

    auto done = false;

    listen_for_stop([&amp;amp;] {
        done = true;
        conn.disconnect();
        service.reset();
    });

    while (!done)
    {
        auto message = co_await conn.consume();
        this-&amp;gt;setText(QString::fromStdString(message));
    }
    co_return;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when
the impl of the service is destroyed.&lt;/p&gt;

&lt;p&gt;Here is the final code for &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/tree/stage-3&quot;&gt;stage 3&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring
coroutines and the think-async mindset.&lt;/p&gt;

&lt;p&gt;There are a number of things I have not covered, the most important of which is improving the (currently very basic)
&lt;code&gt;qt_guarded_executor&lt;/code&gt; to improve its performance. At the present time, whether you call &lt;code&gt;dispatch&lt;/code&gt; or &lt;code&gt;post&lt;/code&gt; referencing
this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to
allow &lt;code&gt;net::dispatch(e, f)&lt;/code&gt; to offer straight-through execution if we’re already on the correct Qt thread.&lt;/p&gt;

&lt;p&gt;If you have any questions or suggestions I’m happy to hear them. You can generally find me in the &lt;code&gt;#beast&lt;/code&gt; channel 
on &lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;cpplang slack&lt;/a&gt; or if you prefer you can either email &lt;a href=&quot;mailto:hodges.r@gmail.com&quot;&gt;me&lt;/a&gt;
or create an issue on &lt;a href=&quot;https://github.com/madmongo1/blog-october-2020/issues&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Asio Coroutines in Qt applications! I started this train of thought when I wanted to hook up some back-end style code that I had written to a gui front end. One way to do this would be to have a web front end subscribing to a back-end service, but I am no expert in modern web technologies so rather than spend time learning something that wasn’t C++ I decided to reach for the popular-but-so-far-unused-by-me C++ GUI framework, Qt. The challenge was how to hook up Qt, which is an event driven framework to a service written with Asio C++ coroutines. In the end it turned out to be easier than I had expected. Here’s how. A simple Executor As mentioned in a previous blog, Asio comes with a full implementation of the Unified Executors proposal. Asio coroutines are designed to be initiated and continued within an executor’s execution context. So let’s build an executor that will perform work in a Qt UI thread. The executor I am going to build will have to invoke completion handlers to Asio IO objects, so we need to make it compatible with asio::any_io_executor. This means it needs to have an associated execution context. The execution context is going to ultimately perform work on a Qt Application, so it makes sense to capture a reference to the Application. Although Qt defines the macro qApp to resolve to a pointer to the “current” application, for testing and sanity purposes I prefer that all services I write allow dependency injection, so I’ll arrange things so that the execution_context’s constructor takes an optional pointer to an application. In addition, it will be convenient when writing components to not have to specifically create and pass an an execution context to windows within the Qt application so it makes sense to be able to provide access to a default context which references the default application. Here’s a first cut: struct qt_execution_context : net::execution_context , boost::noncopyable { qt_execution_context(QApplication *app = qApp) : app_(app) { instance_ = this; } template&amp;lt;class F&amp;gt; void post(F f) { // todo } static qt_execution_context &amp;amp; singleton() { assert(instance_); return *instance_; } private: static qt_execution_context *instance_; QApplication *app_; }; This class will provide two services. The first is to provide the asio service infrastructure so that we can create timers, sockets etc that use executors associated with this context and the second is to allow the executor to actually dispatch work in a Qt application. This is the purpose of the post method. Now a Qt application is itself a kind of execution context - in that it dispatches QEvent objects to be handled by children of the application. We can use this infrastructure to ensure that work dispatched by this execution context actually takes place on the correct thread and at the correct time. In order for us to dispatch work to the application, we need to wrap our function into a QEvent: class qt_work_event_base : public QEvent { public: qt_work_event_base() : QEvent(generated_type()) { } virtual void invoke() = 0; static QEvent::Type generated_type() { static int event_type = QEvent::registerEventType(); return static_cast&amp;lt;QEvent::Type&amp;gt;(event_type); } }; template&amp;lt;class F&amp;gt; struct basic_qt_work_event : qt_work_event_base { basic_qt_work_event(F f) : f_(std::move(f)) {} void invoke() override { f_(); } private: F f_; }; As opposed to using a std::function, the basic_qt_work_event allows us to wrap a move-only function object, which is important when that object is actually an Asio completion handler. Completion handlers benefit from being move-only as it means they can carry move-only state. This makes them more versatile, and can often lead to improvements in execution performance. Now we just need to fill out the code for qt_execution_context::post and provide a mechanism in the Qt application to detect and dispatch these messages: template&amp;lt;class F&amp;gt; void post(F f) { // c++20 auto template deduction auto event = new basic_qt_work_event(std::move(f)); QApplication::postEvent(app_, event); } class qt_net_application : public QApplication { using QApplication::QApplication; protected: bool event(QEvent *event) override; }; bool qt_net_application::event(QEvent *event) { if (event-&amp;gt;type() == qt_work_event_base::generated_type()) { auto p = static_cast&amp;lt;qt_work_event_base*&amp;gt;(event); p-&amp;gt;accept(); p-&amp;gt;invoke(); return true; } else { return QApplication::event(event); } } Note that I have seen on stack overflow the technique of invoking a function object in the destructor of the QEvent-derived event. This would mean no necessity of custom event handling in the QApplication but there are two problems that I can see with this approach: I don’t know enough about Qt to know that this is safe and correct, and Executors-TS executors can be destroyed while there are still un-invoked handlers within them. The correct behaviour is to destroy these handlers without invoking them. If we put invocation code in the destructors, they will actually mass-invoke when the executor is destroyed, leading most probably to annihilation of our program by segfault. However, that being done, we can now write the executor to meet the minimal expectations of an asio executor which can be used in an any_io_executor. struct qt_executor { qt_executor(qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) { } qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template &amp;lt; typename OtherAllocator &amp;gt; static constexpr auto query( net::execution::allocator_t&amp;lt; OtherAllocator &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt; void &amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { context_-&amp;gt;post(std::move(f)); } bool operator==(qt_executor const &amp;amp;other) const noexcept { return context_ == other.context_; } bool operator!=(qt_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; }; static_assert(net::execution::is_executor_v&amp;lt;qt_executor&amp;gt;); Now all that remains is to write a subclass of some Qt Widget so that we can dispatch some work against it. class test_widget : public QTextEdit { Q_OBJECT public: using QTextEdit::QTextEdit; private: void showEvent(QShowEvent *event) override; void hideEvent(QHideEvent *event) override; net::awaitable&amp;lt;void&amp;gt; run_demo(); }; void test_widget::showEvent(QShowEvent *event) { net::co_spawn( qt_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { QWidget::hideEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); co_await timer.async_wait(net::use_awaitable); this-&amp;gt;setText(QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } co_return; } Here is the code for stage 1 And here is a screenshot of the app running: All very well… OK, so we have a coroutine running in a Qt application. This is nice because it allows us to express an event-driven system in terms of procedural expression of code in a coroutine. But what if the user closes the window before the coroutine completes? This application has created the window on the stack, but in a larger application, there will be multiple windows and they may open and close at any time. It is not unusual in Qt to delete a closed window. If the coroutine continues to run once the windows that’s hosting it is deleted, we are sure to get a segfault. One answer to this is to maintain a sentinel in the Qt widget implementation, which prevents the continuation of the coroutine if destroyed. A std::shared_ptr/weak_ptr pair would seem like a sensible solution. Let’s create an updated version of the executor: struct qt_guarded_executor { qt_guarded_executor(std::weak_ptr&amp;lt;void&amp;gt; guard, qt_execution_context &amp;amp;context = qt_execution_context::singleton()) noexcept : context_(std::addressof(context)) , guard_(std::move(guard)) {} qt_execution_context &amp;amp;query(net::execution::context_t) const noexcept { return *context_; } static constexpr net::execution::blocking_t query(net::execution::blocking_t) noexcept { return net::execution::blocking.never; } static constexpr net::execution::relationship_t query(net::execution::relationship_t) noexcept { return net::execution::relationship.fork; } static constexpr net::execution::outstanding_work_t query(net::execution::outstanding_work_t) noexcept { return net::execution::outstanding_work.tracked; } template&amp;lt;typename OtherAllocator&amp;gt; static constexpr auto query(net::execution::allocator_t&amp;lt;OtherAllocator&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } static constexpr auto query(net::execution::allocator_t&amp;lt;void&amp;gt;) noexcept { return std::allocator&amp;lt;void&amp;gt;(); } template&amp;lt;class F&amp;gt; void execute(F f) const { if (auto lock1 = guard_.lock()) { context_-&amp;gt;post([guard = guard_, f = std::move(f)]() mutable { if (auto lock2 = guard.lock()) f(); }); } } bool operator==(qt_guarded_executor const &amp;amp;other) const noexcept { return context_ == other.context_ &amp;amp;&amp;amp; !guard_.owner_before(other.guard_) &amp;amp;&amp;amp; !other.guard_.owner_before(guard_); } bool operator!=(qt_guarded_executor const &amp;amp;other) const noexcept { return !(*this == other); } private: qt_execution_context *context_; std::weak_ptr&amp;lt;void&amp;gt; guard_; }; Now we’ll make a little boilerplate class that we can use as a base class in any executor-enabled object in Qt: struct has_guarded_executor { using executor_type = qt_guarded_executor; has_guarded_executor(qt_execution_context &amp;amp;ctx = qt_execution_context::singleton()) : context_(std::addressof(ctx)) { new_guard(); } void new_guard() { static int x = 0; guard_ = std::shared_ptr&amp;lt;int&amp;gt;(std::addressof(x), // no-op deleter [](auto *) {}); } void reset_guard() { guard_.reset(); } executor_type get_executor() const { return qt_guarded_executor(guard_, *context_); } private: qt_execution_context *context_; std::shared_ptr&amp;lt;void&amp;gt; guard_; }; And we can modify the test_widget to use it: class test_widget : public QTextEdit , public has_guarded_executor { ... }; void test_widget::showEvent(QShowEvent *event) { // stop all existing coroutines and create a new guard new_guard(); // start our coroutine net::co_spawn( get_executor(), [this] { return run_demo(); }, net::detached); QTextEdit::showEvent(event); } void test_widget::hideEvent(QHideEvent *event) { // stop all coroutines reset_guard(); QWidget::hideEvent(event); } Now we’ll update the application to allow the creation and deletion of our widget. For this I’ll use the QMdiWindow and add a menu with an action to create new widgets. We are now able to create and destroy widgets at will, with no segfaults. If you look at the code, you’ll also see that I’ve wired up a rudimentary signal/slot device to allow the coroutine to be cancelled early. // test_widget.hpp void listen_for_stop(std::function&amp;lt;void()&amp;gt; slot); void stop_all(); std::vector&amp;lt;std::function&amp;lt;void()&amp;gt;&amp;gt; stop_signals_; bool stopped_ = false; // test_widget.cpp void test_widget::listen_for_stop(std::function&amp;lt;void()&amp;gt; slot) { if (stopped_) return slot(); stop_signals_.push_back(std::move(slot)); } void test_widget::stop_all() { stopped_ = true; auto copy = std::exchange(stop_signals_, {}); for (auto &amp;amp;slot : copy) slot(); } void test_widget::closeEvent(QCloseEvent *event) { stop_all(); QWidget::closeEvent(event); } net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText( QString::fromStdString(std::to_string(i + 1) + &quot; seconds&quot;)); } for (int i = 10; i--;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) { done = true; break; } this-&amp;gt;setText(QString::fromStdString(std::to_string(i))); } } co_return; } Apparently I am told that it’s been a long-believed myth that Asio “doesn’t do cancellation”. This is of course, nonsense. Here’s the code for stage 2 State of the Art It’s worth mentioning that I wrote and tested this demo using clang-9 and the libc++ version of the standard library. I have also successfully tested clang-11 with coroutines (and concepts). As I understand it, recent versions of Visual Studio support both well. GCC 10 - although advertising support for coroutines - has given me trouble, exhibiting segfaults at run time. Apple Clang, of course, is as always well behind the curve with no support for coroutines. If you want to try this code on a mac, it’s entirely possible as long as you ditch the Apple compiler and use the homebrew’s clang: brew install llvm Clang will then be available in /usr/local/opt/bin and you will need to set your CMAKE_CXX_COMPILER CMake variable appropriately. For completeness, it’s worth mentioning that I also installed Qt5 using homebrew. You will need to set Qt5_DIR. Something like this: cmake -H. -Bmy_build_dir -DCMAKE_CXX_COMPILER=/usr/local/opt/llvm/clang++ -DQt5_DIR=/usr/local/opt/qt5/lib/cmake/Qt5 Going further Ok, so what if we want our Qt application to interact with some asio-based service running in another thread? For this I’m going to create a few boilerplate classes. The reason is that we’re going to have multiple threads running and each thread is going to be executing multiple coroutines. Each coroutine has an associated executor and that executor is dispatching completion handlers (which for our purposes advance the progress of the coroutines) in one of the threads assigned to it. It is important that coroutines are able to synchronise with each other, similar to the way that threads synchronise with each other. In fact, it’s reasonable to use the mental model that a coroutine is a kind of “thread”. In standard C++, we have the class std::condition_variable which we can wait on for some condition to be fulfilled. If we were to produce a similar class for coroutines, then coroutines could co_await on each other. This could form the basis of an asynchronous event queue. First the condition_variable, implemented in terms of cancellation of an Asio timer to indicate readiness (thanks to Chris Kohlhoff - the author of Asio - for suggesting this and saving me having reach for another library or worse, write my own awaitable type!): struct async_condition_variable { private: using timer_type = net::high_resolution_timer; public: using clock_type = timer_type::clock_type; using duration = timer_type::duration; using time_point = timer_type::time_point; using executor_type = timer_type::executor_type; /// Constructor /// @param exec is the executor to associate with the internal timer. explicit inline async_condition_variable(net::any_io_executor exec); template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; template&amp;lt;class Pred&amp;gt; [[nodiscard]] auto wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt;; auto get_executor() noexcept -&amp;gt; executor_type { return timer_.get_executor(); } inline void notify_one(); inline void notify_all(); /// Put the condition into a stop state so that all future awaits fail. inline void stop(); auto error() const -&amp;gt; error_code const &amp;amp; { return error_; } void reset() { error_ = {}; } private: timer_type timer_; error_code error_; std::multiset&amp;lt;timer_type::time_point&amp;gt; wait_times_; }; template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_until(Pred pred, time_point limit) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { assert(co_await net::this_coro::executor == timer_.get_executor()); while (not error_ and not pred()) { if (auto now = clock_type::now(); now &amp;gt;= limit) co_return std::cv_status::timeout; // insert our expiry time into the set and remember where it is auto where = wait_times_.insert(limit); // find the nearest expiry time and set the timeout for that one auto when = *wait_times_.begin(); if (timer_.expiry() != when) timer_.expires_at(when); // wait for timeout or cancellation error_code ec; co_await timer_.async_wait(net::redirect_error(net::use_awaitable, ec)); // remove our expiry time from the set wait_times_.erase(where); // any error other than operation_aborted is unexpected if (ec and ec != net::error::operation_aborted) if (not error_) error_ = ec; } if (error_) throw system_error(error_); co_return std::cv_status::no_timeout; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait(Pred pred) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { auto stat = co_await wait_until(std::move(pred), time_point::max()); boost::ignore_unused(stat); co_return; } template&amp;lt;class Pred&amp;gt; auto async_condition_variable::wait_for(Pred pred, duration d) -&amp;gt; net::awaitable&amp;lt;std::cv_status&amp;gt; { return wait_until(std::move(pred), clock_type::now() + d); } async_condition_variable::async_condition_variable(net::any_io_executor exec) : timer_(std::move(exec)) , error_() {} void async_condition_variable::notify_one() { timer_.cancel_one(); } void async_condition_variable::notify_all() { timer_.cancel(); } void async_condition_variable::stop() { error_ = net::error::operation_aborted; notify_all(); } For our purposes this one is a little too all-singing and all-dancing as it allows for timed waits from multiple coroutines. This is not needed in our example, but I happened to have the code handy from previous experiments. You will notice that I have marked the coroutines as [[nodiscard]]. This is to ensure that I don’t forget to co_await them at the call site. I can’t tell you how many times I have done that and then wondered why my program mysteriously freezes mid run. Having built the condition_variable, we now need some kind of waitable queue. I have implemented this in terms of some shared state which contains an async_condition_variable and some kind of queue. I have made the implementation of the queue a template function (another over-complication for our purposes). The template represents the strategy for accumulating messages before they have been consumed by the client. The strategy I have used here is a FIFO, which means that every message posted will be consumed in the order in which they were posted. But it could just as easily be a priority queue, or a latch - i.e. only storing the most recent message. The code to describe this machinery is a little long to put inline, but by all means look at the code: basic_connection basic_distributor basic_shared_state The next piece of machinery we need is the actual service that will be delivering messages. The code is more-or-less a copy/paste of the code that was in our widget because it’s doing the same job - delivering messages, but this time via the basic_distributor. message_service.hpp message_service.cpp Note that the message_service class is a pimpl. Although it uses a shared_ptr to hold the impl’s lifetime, it is itself non-copyable. When the message_service is destroyed, it will signal its impl to stop. The impl will last a little longer than the handle, while it shuts itself down. The main coroutine on the impl is called run() and it is initiated when the impl is created: message_service::message_service(const executor_type &amp;amp;exec) : exec_(exec) , impl_(std::make_shared&amp;lt;message_service_impl&amp;gt;(exec_)) { net::co_spawn( impl_-&amp;gt;get_executor(), [impl = impl_]() -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { co_await impl-&amp;gt;run(); }, net::detached); } Note that the impl shared_ptr has been captured in the lambda. Normally we’d need to be careful here because the lambda is just a class who’s operator() happens to be a coroutine. This means that the actual coroutine can outlive the lambda that initiated it, which means that impl could be destroyed before the coroutine finishes. For this reason it’s generally safer to pass the impl to the coroutine as an argument, so that it gets decay_copied into the coroutine state. However, in this case we’re safe. net::co_spawn will actually copy the lambda object before invoking it, guaranteeing with asio at least - that the impl will survive the execution of the coroutine. And here’s the run() coroutine: net::awaitable&amp;lt;void&amp;gt; message_service_impl::run() { using namespace std::literals; auto timer = net::high_resolution_timer(co_await net::this_coro::executor); auto done = false; listen_for_stop([&amp;amp;] { done = true; timer.cancel(); }); while (!done) { for (int i = 0; i &amp;lt; 10 &amp;amp;&amp;amp; !done; ++i) { timer.expires_after(1s); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i + 1) + &quot; seconds&quot;); } for (int i = 10; i-- &amp;amp;&amp;amp; !done;) { timer.expires_after(250ms); auto ec = boost::system::error_code(); co_await timer.async_wait( net::redirect_error(net::use_awaitable, ec)); if (ec) break; message_dist_.notify_value(std::to_string(i)); } } } Notice the done machinery allowing detection of a stop event. Remember that a stop event can arrive at any time. The first this coroutine will hear of it is when one of the timer async_wait calls is canceled. Note that the lambda passed to listen_for_stop is not actually part of the coroutine. It is a separate function that just happens to refer to the same state that the coroutine refers to. The communication between the two is via the timer cancellation and the done flag. This communication is guaranteed not to race because both the coroutine and the lambda are executed by the same strand. Finally we need to modify the widget: net::awaitable&amp;lt;void&amp;gt; test_widget::run_demo() { using namespace std::literals; auto service = message_service(ioexec_); auto conn = co_await service.connect(); auto done = false; listen_for_stop([&amp;amp;] { done = true; conn.disconnect(); service.reset(); }); while (!done) { auto message = co_await conn.consume(); this-&amp;gt;setText(QString::fromStdString(message)); } co_return; } This coroutine will exit via exception when the distributor feeding the connection is destroyed. This will happen when the impl of the service is destroyed. Here is the final code for stage 3. I’ve covered quite a few topics here and I hope this has been useful and interesting for people interested in exploring coroutines and the think-async mindset. There are a number of things I have not covered, the most important of which is improving the (currently very basic) qt_guarded_executor to improve its performance. At the present time, whether you call dispatch or post referencing this executor type, a post will actually be performed. Perhaps next month I’ll revisit and add the extra machinery to allow net::dispatch(e, f) to offer straight-through execution if we’re already on the correct Qt thread. If you have any questions or suggestions I’m happy to hear them. You can generally find me in the #beast channel on cpplang slack or if you prefer you can either email me or create an issue on this repo.</summary></entry><entry><title type="html">Richard’s September Update</title><link href="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html" rel="alternate" type="text/html" title="Richard’s September Update" /><published>2020-09-30T00:00:00+00:00</published><updated>2020-09-30T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/30/RichardsSeptemberUpdate.html">&lt;h1 id=&quot;cancellation-in-beastasio-and-better-compile-performance-with-beastwebsocket&quot;&gt;Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket&lt;/h1&gt;

&lt;p&gt;This month I will be discussing two issues. One of interest to many people who come to us with questions on the 
&lt;a href=&quot;https://github.com/boostorg/beast/issues&quot;&gt;Github Issue Tracker&lt;/a&gt; and the #beast channel of 
&lt;a href=&quot;https://cppalliance.org/slack/&quot;&gt;Cpplang Slack&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;compile-times-and-separation-of-concerns&quot;&gt;Compile Times and Separation of Concerns&lt;/h2&gt;

&lt;p&gt;A common complaint about Boost.Beast is that compilation units that use the &lt;code&gt;websocket::stream&lt;/code&gt; template class
often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can
become viral in an application.&lt;/p&gt;

&lt;p&gt;This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket
stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s 
intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a 
few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions.
In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance.
The branch is &lt;a href=&quot;https://github.com/vinniefalco/beast/tree/async-an&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will be continuing work in this area in the coming days.&lt;/p&gt;

&lt;p&gt;In the meantime, our general response is to suggest that users create a base class to handle the transport, and 
communicate important events such as frame received, connection state and the close notification to a derived 
application-layer class through a private polymorphic interface.&lt;/p&gt;

&lt;p&gt;In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once
since the transport layer will rarely change during the development life of an application. Whenever there is a change
to the application layer, the transport layer is not affected so websocket-related code is not affected.&lt;/p&gt;

&lt;p&gt;This approach has a number of benefits. Not least of which is that developing another client implementation over 
a different websocket connection in the same application becomes trivial.&lt;/p&gt;

&lt;p&gt;Another benefit is that the application can be designed such that application-level concerns are agnostic of the 
transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, 
unix sockets and so on.&lt;/p&gt;

&lt;p&gt;In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user &lt;code&gt;@elegracer&lt;/code&gt;
who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question
which prompted me to finally conjure up a demo. &lt;code&gt;@elegracer&lt;/code&gt;’s problem was needing to connect to multiple cryptocurrency
exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to
the public FMex market data feed since that was the subject of the original question.&lt;/p&gt;

&lt;h2 id=&quot;correct-cancellation&quot;&gt;Correct Cancellation&lt;/h2&gt;

&lt;p&gt;Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application
in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response
to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want 
active objects in the program to write data to disk, we may want to ensure that the underlying websocket is 
shut down cleanly and we may want to give the user an opportunity to prevent the shutdown.&lt;/p&gt;

&lt;p&gt;I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the 
first SIGINT with another within 5 seconds to confirm.&lt;/p&gt;

&lt;h1 id=&quot;designing-the-application&quot;&gt;Designing the application&lt;/h1&gt;

&lt;p&gt;When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the 
responsibility of monitoring signals and starting the initial connection objects. It also provides the communication
between the two.&lt;/p&gt;

&lt;p&gt;The construction and configuration of the &lt;code&gt;io_context&lt;/code&gt; and &lt;code&gt;ssl::context&lt;/code&gt; stay in &lt;code&gt;main()&lt;/code&gt;. The executor and ssl context
are passed to the application by reference as dependencies. The application can then pass on these refrences as 
required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic &lt;code&gt;any_io_executor&lt;/code&gt; 
type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, 
then each individual io_enabled object such as a connection or the application will need to have its &lt;em&gt;own&lt;/em&gt; strand.
Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so 
for top level objects I pass the executor as &lt;code&gt;io_context::executor_type&lt;/code&gt;. It is then up to each object to create its own
strand internally which will have the type &lt;code&gt;strand&amp;lt;io_context::executor_type&amp;gt;&lt;/code&gt;. The &lt;code&gt;strand&lt;/code&gt; type provides the method
&lt;code&gt;get_inner_executor&lt;/code&gt; which allows the application to extract the underlying &lt;code&gt;io_context::executor_type&lt;/code&gt; and pass it to
the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own
strands from this.&lt;/p&gt;

&lt;h2 id=&quot;step-1---a-simple-application-framework-that-supports-ctrl-c&quot;&gt;Step 1 - A Simple Application Framework That Supports ctrl-c&lt;/h2&gt;

&lt;p&gt;OK, let’s get started and build the framework. Here’s a link to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ssl.hpp&lt;/code&gt; and &lt;code&gt;net.hpp&lt;/code&gt; simply configure the project to use boost.asio. The idea of these little configuration headers
is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking
if it ever arrives.&lt;/p&gt;

&lt;p&gt;As a matter of style, I like to ensure that no names are created in the global namespace other than &lt;code&gt;main&lt;/code&gt;. This saves
headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name
was already in use by the native system libraries.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;main.cpp&lt;/code&gt; simply creates the io execution context and a default ssl context, creates the application, starts it and
runs the io context.&lt;/p&gt;

&lt;p&gt;At the moment, the only interesting part of our program is the &lt;code&gt;signit_state&lt;/code&gt;. This is a state machine which handles the
behaviour of the program when a &lt;code&gt;SIGINT&lt;/code&gt; is received. Our state machine is doing something a little fancy. Here is the 
state diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-sigint-state.png&quot; alt=&quot;sigint_state&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rather than reproduce the code here, please refer to 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-1/pre-cxx20/blog-2020-09&quot;&gt;step 1&lt;/a&gt; 
to see the source code.&lt;/p&gt;

&lt;p&gt;At this point the program will run and successfully handle ctrl-c:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
Interrupt unconfirmed. Ignoring
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-2---connecting-to-an-exchange&quot;&gt;Step 2 - Connecting to an Exchange&lt;/h2&gt;

&lt;p&gt;Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it.
For now we won’t worry about cancellation - we’ll retrofit that in Step 3.&lt;/p&gt;

&lt;p&gt;Here is the code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-2/pre-cxx20/blog-2020-09&quot;&gt;step 2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This section introduces two new main classes - the &lt;code&gt;wss_transport&lt;/code&gt; and the &lt;code&gt;fmex_connection&lt;/code&gt;. In addition, the connection
phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually 
makes the code easier to read than continuation-passing style code)&lt;/p&gt;

&lt;p&gt;Here is the implementation of the connect coroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    struct wss_transport::connect_op : asio::coroutine
    {
        using executor_type = wss_transport::executor_type;
        using websock       = wss_transport::websock;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we define the &lt;em&gt;implementation&lt;/em&gt; of the coroutine - this is an object which will not be moved for the duration of the
execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely
on knowing the address of the resolver (and later perhaps other io objects).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        struct impl_data
        {
            impl_data(websock &amp;amp;   ws,
                      std::string host,
                      std::string port,
                      std::string target)
            : ws(ws)
            , resolver(ws.get_executor())
            , host(host)
            , port(port)
            , target(target)
            {
            }

            layer_0 &amp;amp;
            tcp_layer() const
            {
                return ws.next_layer().next_layer();
            }

            layer_1 &amp;amp;
            ssl_layer() const
            {
                return ws.next_layer();
            }

            websock &amp;amp;                            ws;
            net::ip::tcp::resolver               resolver;
            net::ip::tcp::resolver::results_type endpoints;
            std::string                          host, port, target;
        };
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The constructor merely forwards the arguments to the construction of the &lt;code&gt;impl_data&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        connect_op(websock &amp;amp;   ws,
                   std::string host,
                   std::string port,
                   std::string target)
        : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target))
        {
        }

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an
&lt;code&gt;operator()&lt;/code&gt; interface matching the requirements of each sub-operation. During the lifetime of this coroutine we 
will be using the resolver and calling &lt;code&gt;async_connect&lt;/code&gt; on the &lt;code&gt;tcp_stream&lt;/code&gt;. We therefore provide conforming member
functions which store or ignore the and forward the &lt;code&gt;error_code&lt;/code&gt; to the main implementation of the coroutine.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;                               self,
                   error_code                           ec,
                   net::ip::tcp::resolver::results_type results)
        {
            impl_-&amp;gt;endpoints = results;
            (*this)(self, ec);
        }

        template &amp;lt; class Self &amp;gt;
        void
        operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;)
        {
            (*this)(self, ec);
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order
to allow this member function to match the completion handler signatures of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;void()&lt;/code&gt; - invoked during async_compose in order to start the coroutine.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code)&lt;/code&gt; - invoked by the two functions above and by the async handshakes.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;void(error_code, std::size_t)&lt;/code&gt; - invoked by operations such as async_read and async_write although not strictly 
necessary here.
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;      template &amp;lt; class Self &amp;gt;
      void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0)
      {
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to
omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, 
including the first entry (at which time &lt;code&gt;ec&lt;/code&gt; is guaranteed to be default constructed).&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;          if (ec)
              return self.complete(ec);

          auto &amp;amp;impl = *impl_;
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;Note the use of the asio yield and unyield headers to create the fake ‘keywords’ &lt;code&gt;reenter&lt;/code&gt; and &lt;code&gt;yield&lt;/code&gt; in avery limited
scope.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;boost/asio/yield.hpp&amp;gt;
          reenter(*this)
          {
              yield impl.resolver.async_resolve(
                  impl.host, impl.port, std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.tcp_layer().async_connect(impl.endpoints,
                                                   std::move(self));

              if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                            impl.host.c_str()))
                  return self.complete(
                      error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                 net::error::get_ssl_category()));

              impl.tcp_layer().expires_after(15s);
              yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                     std::move(self));

              impl.tcp_layer().expires_after(15s);
              yield impl.ws.async_handshake(
                  impl.host, impl.target, std::move(self));
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been
caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the 
&lt;code&gt;async_handshake&lt;/code&gt; initiating function, we are guaranteed to be executing in the correct executor. Therefore we can
simply call &lt;code&gt;complete&lt;/code&gt; directly without needing to post to an executor. Note that the &lt;code&gt;async_compose&lt;/code&gt; call which will
encapsulate the use of this class embeds this object into a wrapper which provides the &lt;code&gt;executor_type&lt;/code&gt; and 
&lt;code&gt;get_executor()&lt;/code&gt; mechanism which asio uses to determine on which executor to invoke completion handlers.&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;              impl.tcp_layer().expires_never();
              yield self.complete(ec);
          }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
      }

      std::unique_ptr&amp;lt; impl_data &amp;gt; impl_;
  };
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;wss_connection&lt;/code&gt; class provides the bare bones required to connect a websocket and maintain the connection. It 
provides a protected interface so that derived classes can send text frames and it will call private virtual functions
in order to notify the derived class of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;transport up (websocket connection established).&lt;/li&gt;
  &lt;li&gt;frame received.&lt;/li&gt;
  &lt;li&gt;connection error (either during connection or operation).&lt;/li&gt;
  &lt;li&gt;websocket close - the server has requested or agreed to a graceful shutdown.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach
the derived class.&lt;/p&gt;

&lt;p&gt;One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one &lt;code&gt;async_write&lt;/code&gt;
is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple 
transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // send_state - data to control sending data

        std::deque&amp;lt;std::string&amp;gt; send_queue_;
        enum send_state
        {
            not_sending,
            sending
        } send_state_ = not_sending;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will note that I have used a &lt;code&gt;std::deque&lt;/code&gt; to hold the pending messages. Although a deque has theoretically better
complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data
structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items
are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to
send them. Remember that during an &lt;code&gt;async_write&lt;/code&gt;, the data to which the supplied buffer sequence refers must have a 
stable address.&lt;/p&gt;

&lt;p&gt;Here are the functions that deal with the send state transitions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::send_text_frame(std::string frame)
    {
        if (state_ != connected)
            return;

        send_queue_.push_back(std::move(frame));
        start_sending();
    }

    void
    wss_transport::start_sending()
    {
        if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp;
            !send_queue_.empty())
        {
            send_state_ = sending;
            websock_.async_write(net::buffer(send_queue_.front()),
                                 [this](error_code const &amp;amp;ec, std::size_t bt) {
                                     handle_send(ec, bt);
                                 });
        }
    }

    void
    wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t)
    {
        send_state_ = not_sending;

        send_queue_.pop_front();

        if (ec)
            event_transport_error(ec);
        else
            start_sending();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, we can implement our specific exchange protocol on top of the &lt;code&gt;wss_connection&lt;/code&gt;. In this case, FMex eschews 
the ping/pong built into websockets and requires a json ping/pong to be initiated by the client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::ping_enter_state()
    {
        BOOST_ASSERT(ping_state_ == ping_not_started);
        ping_enter_wait();
    }

    void
    fmex_connection::ping_enter_wait()
    {
        ping_state_ = ping_wait;

        ping_timer_.expires_after(5s);

        ping_timer_.async_wait([this](error_code const &amp;amp;ec) {
            if (!ec)
                ping_event_timeout();
        });
    }

    void
    fmex_connection::ping_event_timeout()
    {
        ping_state_ = ping_waiting_pong;

        auto  frame = json::value();
        auto &amp;amp;o     = frame.emplace_object();
        o[&quot;cmd&quot;]    = &quot;ping&quot;;
        o[&quot;id&quot;]     = &quot;my_ping_ident&quot;;
        o[&quot;args&quot;].emplace_array().push_back(timestamp());
        send_text_frame(json::serialize(frame));
    }

    void
    fmex_connection::ping_event_pong(json::value const &amp;amp;frame)
    {
        ping_enter_wait();
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no
need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application 
developer’s life easy.&lt;/p&gt;

&lt;p&gt;Finally, we implement &lt;code&gt;on_text_frame&lt;/code&gt; and write a little message parser and switch. Note that this function may throw.
The base class will catch any exceptions thrown here and ensure that the &lt;code&gt;on_transport_error&lt;/code&gt; event will be called at
the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about
handling exceptions in an asynchronous environment.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    fmex_connection::on_text_frame(std::string_view frame)
    try
    {
        auto jframe =
            json::parse(json::string_view(frame.data(), frame.size()));

        // dispatch on frame type

        auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;);
        if (type == &quot;hello&quot;)
        {
            on_hello();
        }
        else if (type == &quot;ping&quot;)
        {
            ping_event_pong(jframe);
        }
        else if (type.as_string().starts_with(&quot;ticker.&quot;))
        {
            fmt::print(stdout,
                       &quot;fmex: tick {} : {}\n&quot;,
                       type.as_string().subview(7),
                       jframe.as_object().at(&quot;ticker&quot;));
        }
    }
    catch (...)
    {
        fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame);
        throw;
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Compiling and running the program produces output similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4]
fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4]
fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4]
fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response.
This is because we have not wired up a mechanism to communicate the &lt;code&gt;stop()&lt;/code&gt; event to the implementation of the 
connection:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4]
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4]
^CInterrupt confirmed. Shutting down
fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4]
fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4]
fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4]
^Z
[1]+  Stopped                 ./blog_2020_09
$ kill %1

[1]+  Stopped                 ./blog_2020_09
$ 
[1]+  Terminated              ./blog_2020_09
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;step-3---re-enabling-cancellation&quot;&gt;Step 3 - Re-Enabling Cancellation&lt;/h2&gt;

&lt;p&gt;You will remember from step 1 that we created a little class called &lt;code&gt;sigint_state&lt;/code&gt; which notices that the application 
has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the 
signal to the fmex connection:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            fmex_connection_.start();
            sigint_state_.add_slot([this]{
                fmex_connection_.stop();
            });
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But we didn’t put any code in &lt;code&gt;wss_transport::stop&lt;/code&gt;. Now all we have to do is provide a function object within 
&lt;code&gt;wss_transport&lt;/code&gt; that we can adjust whenever the current state changes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;        // stop signal
        std::function&amp;lt;void()&amp;gt; stop_signal_;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;    void
    wss_transport::stop()
    {
        net::dispatch(get_executor(), [this] {
            if (auto sig = boost::exchange(stop_signal_, nullptr))
                sig();
        });
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will also need to provide a way for the connect operation to respond to the stop signal (the user might press
ctrl-c while resolving for example).&lt;/p&gt;

&lt;p&gt;The way I have done this here is a simple approach, merely pass a reference to the &lt;code&gt;wss_transport&lt;/code&gt; into the composed
operation so that the operation can modify the function directly. There are other more scalable ways to do this, but
this is good enough for now.&lt;/p&gt;

&lt;p&gt;The body of the coroutine then becomes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;            auto &amp;amp;impl = *impl_;

            if(ec)
                impl.error = ec;

            if (impl.error)
                return self.complete(impl.error);

#include &amp;lt;boost/asio/yield.hpp&amp;gt;
            reenter(*this)
            {
                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.resolver.cancel();
                    impl.error = net::error::operation_aborted;
                };
                yield impl.resolver.async_resolve(
                    impl.host, impl.port, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] {
                    impl.tcp_layer().cancel();
                    impl.error = net::error::operation_aborted;
                };

                impl.tcp_layer().expires_after(15s);
                yield impl.tcp_layer().async_connect(impl.endpoints,
                                                     std::move(self));

                //

                if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(),
                                              impl.host.c_str()))
                    return self.complete(
                        error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()),
                                   net::error::get_ssl_category()));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ssl_layer().async_handshake(ssl::stream_base::client,
                                                       std::move(self));

                //

                impl.tcp_layer().expires_after(15s);
                yield impl.ws.async_handshake(
                    impl.host, impl.target, std::move(self));

                //

                transport_-&amp;gt;stop_signal_ = nullptr;
                impl.tcp_layer().expires_never();
                yield self.complete(impl.error);
            }
#include &amp;lt;boost/asio/unyield.hpp&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The final source code for 
&lt;a href=&quot;https://github.com/test-scenarios/boost_beast_websocket_echo/tree/blog-2020-09-step-3/pre-cxx20/blog-2020-09&quot;&gt;step 3 is here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stopping the program while connecting:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
^CInterrupt confirmed. Shutting down
fmex: transport error : system : 125 : Operation canceled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And stopping the program while connected:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./blog_2020_09 
Application starting
Press ctrl-c to interrupt.
fmex: initiating connection
fmex: transport up
fmex: hello
^CInterrupt detected. Press ctrl-c again within 5 seconds to exit
fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4]
^CInterrupt confirmed. Shutting down
closing websocket
fmex: closed
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;future-development&quot;&gt;Future development&lt;/h1&gt;

&lt;p&gt;Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing
event based systems easier and/or more maintainable.&lt;/p&gt;

&lt;p&gt;Thanks for reading.&lt;/p&gt;</content><author><name></name></author><category term="richard" /><summary type="html">Cancellation in Beast/Asio and Better Compile Performance with Beast.Websocket This month I will be discussing two issues. One of interest to many people who come to us with questions on the Github Issue Tracker and the #beast channel of Cpplang Slack. Compile Times and Separation of Concerns A common complaint about Boost.Beast is that compilation units that use the websocket::stream template class often take a long time to compile, and that because websocket::stream is a template, this compilation overhead can become viral in an application. This is a valid complaint and we believe there are some reasonable tradeoffs we can make by refactoring the websocket stream to use fewer templates internally. Vinnie has started work to express the WebSocket’s intermediate completion handlers, buffer sequence and executor in terms of a polymorphic object. This would mean a few indirect jumps in the compiled code but would significantly reduce the number of internal template expansions. In the scheme of things, we don’t believe that the virtual function calls will materially affect runtime performance. The branch is here I will be continuing work in this area in the coming days. In the meantime, our general response is to suggest that users create a base class to handle the transport, and communicate important events such as frame received, connection state and the close notification to a derived application-layer class through a private polymorphic interface. In this way, the websocket transport compilation unit may take a while to compile, but it needs to be done only once since the transport layer will rarely change during the development life of an application. Whenever there is a change to the application layer, the transport layer is not affected so websocket-related code is not affected. This approach has a number of benefits. Not least of which is that developing another client implementation over a different websocket connection in the same application becomes trivial. Another benefit is that the application can be designed such that application-level concerns are agnostic of the transport mechanism. Such as when the server can be accessed by multiple means - WSS, WS, long poll, direct connection, unix sockets and so on. In this blog I will present a simplified implementation of this idea. My thanks to the cpplang Slack user @elegracer who most recently asked for guidance on reducing compile times. It was (his/her? Slack is silent on the matter) question which prompted me to finally conjure up a demo. @elegracer’s problem was needing to connect to multiple cryptocurrency exchanges in the same app over websocket. In this particular example I’ll demonstrate a simplified connection to the public FMex market data feed since that was the subject of the original question. Correct Cancellation Our examples in the Beast Repository are rudimentary and don’t cover the issue of graceful shutdown of an application in response to a SIGINT (i.e. the user pressing ctrl-c). It is common for simple programs to exit suddenly in response to this signal, which is the default behaviour. For many applications, this is perfectly fine but not all. We may want active objects in the program to write data to disk, we may want to ensure that the underlying websocket is shut down cleanly and we may want to give the user an opportunity to prevent the shutdown. I will further annotate the example by providing this ability to prevent the shutdown. The user will have to confirm the first SIGINT with another within 5 seconds to confirm. Designing the application When I write IO applications involving Asio and Beast, I prefer to create an “application” object. This has the responsibility of monitoring signals and starting the initial connection objects. It also provides the communication between the two. The construction and configuration of the io_context and ssl::context stay in main(). The executor and ssl context are passed to the application by reference as dependencies. The application can then pass on these refrences as required. It is also worth mentioning that I don’t pass the io_context’s executor as a polymorphic any_io_executor type at this stage. The reason is that I may want in future to upgrade my program to be multi-threaded. If I do this, then each individual io_enabled object such as a connection or the application will need to have its own strand. Getting the strand out of an any_io_executor is not possible in the general case as it will have been type-erased, so for top level objects I pass the executor as io_context::executor_type. It is then up to each object to create its own strand internally which will have the type strand&amp;lt;io_context::executor_type&amp;gt;. The strand type provides the method get_inner_executor which allows the application to extract the underlying io_context::executor_type and pass it to the constructor of any subordinate but otherwise self-contained io objects. The subordinates can then build their own strands from this. Step 1 - A Simple Application Framework That Supports ctrl-c OK, let’s get started and build the framework. Here’s a link to step 1. ssl.hpp and net.hpp simply configure the project to use boost.asio. The idea of these little configuration headers is that they could be generated by the cmake project if necessary to allow the option of upgrading to std networking if it ever arrives. As a matter of style, I like to ensure that no names are created in the global namespace other than main. This saves headaches that could occur if I wrote code on one platform, but then happened to port it to another where the name was already in use by the native system libraries. main.cpp simply creates the io execution context and a default ssl context, creates the application, starts it and runs the io context. At the moment, the only interesting part of our program is the signit_state. This is a state machine which handles the behaviour of the program when a SIGINT is received. Our state machine is doing something a little fancy. Here is the state diagram: Rather than reproduce the code here, please refer to step 1 to see the source code. At this point the program will run and successfully handle ctrl-c: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit Interrupt unconfirmed. Ignoring ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down Step 2 - Connecting to an Exchange Now we need to create our WebSocket transport class and our FMex exchange protocol class that will derive from it. For now we won’t worry about cancellation - we’ll retrofit that in Step 3. Here is the code for step 2. This section introduces two new main classes - the wss_transport and the fmex_connection. In addition, the connection phase of the wss_transport is expressed as a composed operation for exposition purposes (and in my opinion it actually makes the code easier to read than continuation-passing style code) Here is the implementation of the connect coroutine: struct wss_transport::connect_op : asio::coroutine { using executor_type = wss_transport::executor_type; using websock = wss_transport::websock; Here we define the implementation of the coroutine - this is an object which will not be moved for the duration of the execution of the coroutine. This address stability is important because intermediate asynchronous operations will rely on knowing the address of the resolver (and later perhaps other io objects). struct impl_data { impl_data(websock &amp;amp; ws, std::string host, std::string port, std::string target) : ws(ws) , resolver(ws.get_executor()) , host(host) , port(port) , target(target) { } layer_0 &amp;amp; tcp_layer() const { return ws.next_layer().next_layer(); } layer_1 &amp;amp; ssl_layer() const { return ws.next_layer(); } websock &amp;amp; ws; net::ip::tcp::resolver resolver; net::ip::tcp::resolver::results_type endpoints; std::string host, port, target; }; The constructor merely forwards the arguments to the construction of the impl_data. connect_op(websock &amp;amp; ws, std::string host, std::string port, std::string target) : impl_(std::make_unique&amp;lt; impl_data &amp;gt;(ws, host, port, target)) { } This coroutine is both a composed operation and a completion handler for sub-operations. This means it must have an operator() interface matching the requirements of each sub-operation. During the lifetime of this coroutine we will be using the resolver and calling async_connect on the tcp_stream. We therefore provide conforming member functions which store or ignore the and forward the error_code to the main implementation of the coroutine. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp; self, error_code ec, net::ip::tcp::resolver::results_type results) { impl_-&amp;gt;endpoints = results; (*this)(self, ec); } template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec, net::ip::tcp::endpoint const &amp;amp;) { (*this)(self, ec); } Here is the main implementation of the coroutine. Note that the last two parameters provide defaults. This is in order to allow this member function to match the completion handler signatures of: void() - invoked during async_compose in order to start the coroutine. void(error_code) - invoked by the two functions above and by the async handshakes. void(error_code, std::size_t) - invoked by operations such as async_read and async_write although not strictly necessary here. template &amp;lt; class Self &amp;gt; void operator()(Self &amp;amp;self, error_code ec = {}, std::size_t = 0) { Note that here we are checking the error code before re-entering the coroutine. This is a shortcut which allows us to omit error checking after each sub-operation. This check will happen on every attempt to re-enter the coroutine, including the first entry (at which time ec is guaranteed to be default constructed). if (ec) return self.complete(ec); auto &amp;amp;impl = *impl_; Note the use of the asio yield and unyield headers to create the fake ‘keywords’ reenter and yield in avery limited scope. #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); If the coroutine is re-entered here, it must be because there was no error (if there was an error, it would have been caught by the pre-reentry error check above). Since execution has resumed here in the completion handler of the async_handshake initiating function, we are guaranteed to be executing in the correct executor. Therefore we can simply call complete directly without needing to post to an executor. Note that the async_compose call which will encapsulate the use of this class embeds this object into a wrapper which provides the executor_type and get_executor() mechanism which asio uses to determine on which executor to invoke completion handlers. impl.tcp_layer().expires_never(); yield self.complete(ec); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; } std::unique_ptr&amp;lt; impl_data &amp;gt; impl_; }; The wss_connection class provides the bare bones required to connect a websocket and maintain the connection. It provides a protected interface so that derived classes can send text frames and it will call private virtual functions in order to notify the derived class of: transport up (websocket connection established). frame received. connection error (either during connection or operation). websocket close - the server has requested or agreed to a graceful shutdown. Connection errors will only be notified once, and once a connection error has been indicated, no other event will reach the derived class. One of the many areas that trips up asio/beast beginners is that care must be taken to ensure that only one async_write is in progress at a time on the WebSocket (or indeed any async io object). For this reason we implement a simple transmit queue state which can be considered to be an orthogonal region (parallel task) to the read state. // send_state - data to control sending data std::deque&amp;lt;std::string&amp;gt; send_queue_; enum send_state { not_sending, sending } send_state_ = not_sending; You will note that I have used a std::deque to hold the pending messages. Although a deque has theoretically better complexity when inserting or removing items at the ends than a vector, this is not the reason for choosing this data structure. The actual reason is that items in a deque are guaranteed to have a stable address, even when other items are added or removed. This is useful as it means we don’t have to move frames out of the transmit queue in order to send them. Remember that during an async_write, the data to which the supplied buffer sequence refers must have a stable address. Here are the functions that deal with the send state transitions. void wss_transport::send_text_frame(std::string frame) { if (state_ != connected) return; send_queue_.push_back(std::move(frame)); start_sending(); } void wss_transport::start_sending() { if (state_ == connected &amp;amp;&amp;amp; send_state_ == not_sending &amp;amp;&amp;amp; !send_queue_.empty()) { send_state_ = sending; websock_.async_write(net::buffer(send_queue_.front()), [this](error_code const &amp;amp;ec, std::size_t bt) { handle_send(ec, bt); }); } } void wss_transport::handle_send(const error_code &amp;amp;ec, std::size_t) { send_state_ = not_sending; send_queue_.pop_front(); if (ec) event_transport_error(ec); else start_sending(); } Finally, we can implement our specific exchange protocol on top of the wss_connection. In this case, FMex eschews the ping/pong built into websockets and requires a json ping/pong to be initiated by the client. void fmex_connection::ping_enter_state() { BOOST_ASSERT(ping_state_ == ping_not_started); ping_enter_wait(); } void fmex_connection::ping_enter_wait() { ping_state_ = ping_wait; ping_timer_.expires_after(5s); ping_timer_.async_wait([this](error_code const &amp;amp;ec) { if (!ec) ping_event_timeout(); }); } void fmex_connection::ping_event_timeout() { ping_state_ = ping_waiting_pong; auto frame = json::value(); auto &amp;amp;o = frame.emplace_object(); o[&quot;cmd&quot;] = &quot;ping&quot;; o[&quot;id&quot;] = &quot;my_ping_ident&quot;; o[&quot;args&quot;].emplace_array().push_back(timestamp()); send_text_frame(json::serialize(frame)); } void fmex_connection::ping_event_pong(json::value const &amp;amp;frame) { ping_enter_wait(); } Note that since we have implemented frame transmission in the base class in terms of a queue, the fmex class has no need to worry about ensuring the one-write-at-a-time rule. The base class handles it. This makes the application developer’s life easy. Finally, we implement on_text_frame and write a little message parser and switch. Note that this function may throw. The base class will catch any exceptions thrown here and ensure that the on_transport_error event will be called at the appropriate time. Thus again, the application developer’s life is improved as he doesn’t need to worry about handling exceptions in an asynchronous environment. void fmex_connection::on_text_frame(std::string_view frame) try { auto jframe = json::parse(json::string_view(frame.data(), frame.size())); // dispatch on frame type auto &amp;amp;type = jframe.as_object().at(&quot;type&quot;); if (type == &quot;hello&quot;) { on_hello(); } else if (type == &quot;ping&quot;) { ping_event_pong(jframe); } else if (type.as_string().starts_with(&quot;ticker.&quot;)) { fmt::print(stdout, &quot;fmex: tick {} : {}\n&quot;, type.as_string().subview(7), jframe.as_object().at(&quot;ticker&quot;)); } } catch (...) { fmt::print(stderr, &quot;text frame is not json : {}\n&quot;, frame); throw; } Compiling and running the program produces output similar to this: Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0879E4,1.407E3,1.0879E4,2.28836E5,1.08795E4,1.13E2,1.0701E4,1.0939E4,1.0663E4,2.51888975E8,2.3378048830533768E4] fmex: tick btcusd_p : [1.08795E4,1E0,1.0879E4,3.79531E5,1.08795E4,3.518E3,1.0701E4,1.0939E4,1.0663E4,2.51888976E8,2.3378048922449758E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.7747E5,1.08795E4,7.575E3,1.0701E4,1.0939E4,1.0663E4,2.51888978E8,2.3378049106290182E4] fmex: tick btcusd_p : [1.0879E4,2E0,1.0879E4,3.77468E5,1.08795E4,9.229E3,1.0701E4,1.0939E4,1.0663E4,2.5188898E8,2.337804929013061E4] fmex: tick btcusd_p : [1.0879E4,1E0,1.0879E4,1.0039E4,1.08795E4,2.54203E5,1.0701E4,1.0939E4,1.0663E4,2.51888981E8,2.3378049382050827E4] Note however, that although pressing ctrl-c is noticed by the application, the fmex feed does not shut down in response. This is because we have not wired up a mechanism to communicate the stop() event to the implementation of the connection: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.8663E4,1.08595E4,4.1457E4,1.07125E4,1.0939E4,1.0667E4,2.58585817E8,2.3968266005011003E4] ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9942E4,1.08595E4,4.3727E4,1.07125E4,1.0939E4,1.0667E4,2.58585819E8,2.3968266189181537E4] ^CInterrupt confirmed. Shutting down fmex: tick btcusd_p : [1.08595E4,2E0,1.0859E4,5.9932E4,1.08595E4,4.0933E4,1.07125E4,1.0939E4,1.0667E4,2.58585821E8,2.396826637335208E4] fmex: tick btcusd_p : [1.0859E4,1E0,1.0859E4,6.2722E4,1.08595E4,4.0943E4,1.07125E4,1.0939E4,1.0667E4,2.58585823E8,2.3968266557531104E4] fmex: tick btcusd_p : [1.08595E4,1.58E2,1.0859E4,6.2732E4,1.08595E4,3.7953E4,1.07125E4,1.0939E4,1.0667E4,2.58585981E8,2.3968281107003917E4] ^Z [1]+ Stopped ./blog_2020_09 $ kill %1 [1]+ Stopped ./blog_2020_09 $ [1]+ Terminated ./blog_2020_09 Step 3 - Re-Enabling Cancellation You will remember from step 1 that we created a little class called sigint_state which notices that the application has received a sigint and checks for a confirming sigint before taking action. We also added a slot to this to pass the signal to the fmex connection: fmex_connection_.start(); sigint_state_.add_slot([this]{ fmex_connection_.stop(); }); But we didn’t put any code in wss_transport::stop. Now all we have to do is provide a function object within wss_transport that we can adjust whenever the current state changes: // stop signal std::function&amp;lt;void()&amp;gt; stop_signal_; void wss_transport::stop() { net::dispatch(get_executor(), [this] { if (auto sig = boost::exchange(stop_signal_, nullptr)) sig(); }); } We will also need to provide a way for the connect operation to respond to the stop signal (the user might press ctrl-c while resolving for example). The way I have done this here is a simple approach, merely pass a reference to the wss_transport into the composed operation so that the operation can modify the function directly. There are other more scalable ways to do this, but this is good enough for now. The body of the coroutine then becomes: auto &amp;amp;impl = *impl_; if(ec) impl.error = ec; if (impl.error) return self.complete(impl.error); #include &amp;lt;boost/asio/yield.hpp&amp;gt; reenter(*this) { transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.resolver.cancel(); impl.error = net::error::operation_aborted; }; yield impl.resolver.async_resolve( impl.host, impl.port, std::move(self)); // transport_-&amp;gt;stop_signal_ = [&amp;amp;impl] { impl.tcp_layer().cancel(); impl.error = net::error::operation_aborted; }; impl.tcp_layer().expires_after(15s); yield impl.tcp_layer().async_connect(impl.endpoints, std::move(self)); // if (!SSL_set_tlsext_host_name(impl.ssl_layer().native_handle(), impl.host.c_str())) return self.complete( error_code(static_cast&amp;lt; int &amp;gt;(::ERR_get_error()), net::error::get_ssl_category())); // impl.tcp_layer().expires_after(15s); yield impl.ssl_layer().async_handshake(ssl::stream_base::client, std::move(self)); // impl.tcp_layer().expires_after(15s); yield impl.ws.async_handshake( impl.host, impl.target, std::move(self)); // transport_-&amp;gt;stop_signal_ = nullptr; impl.tcp_layer().expires_never(); yield self.complete(impl.error); } #include &amp;lt;boost/asio/unyield.hpp&amp;gt; The final source code for step 3 is here. Stopping the program while connecting: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit ^CInterrupt confirmed. Shutting down fmex: transport error : system : 125 : Operation canceled And stopping the program while connected: $ ./blog_2020_09 Application starting Press ctrl-c to interrupt. fmex: initiating connection fmex: transport up fmex: hello ^CInterrupt detected. Press ctrl-c again within 5 seconds to exit fmex: tick btcusd_p : [1.0882E4,1E0,1.0882E4,3.75594E5,1.08825E4,5.103E3,1.07295E4,1.0939E4,1.06785E4,2.58278146E8,2.3907706652603207E4] ^CInterrupt confirmed. Shutting down closing websocket fmex: closed Future development Next month I’ll refactor the application to use C++20 coroutines and we can see whether this makes developing event based systems easier and/or more maintainable. Thanks for reading.</summary></entry><entry><title type="html">Krystian’s September Update</title><link href="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html" rel="alternate" type="text/html" title="Krystian’s September Update" /><published>2020-09-29T00:00:00+00:00</published><updated>2020-09-29T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/29/KrystiansSeptemberUpdate.html">&lt;h1 id=&quot;reviewing-the-review&quot;&gt;Reviewing the review&lt;/h1&gt;

&lt;p&gt;The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find.&lt;/p&gt;

&lt;p&gt;Other points of contention were the use of a push parser as opposed to a pull parser, the use of &lt;code&gt;double&lt;/code&gt;, &lt;code&gt;uint64_t&lt;/code&gt;, and &lt;code&gt;int64_t&lt;/code&gt; without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review.&lt;/p&gt;

&lt;h1 id=&quot;customizing-the-build&quot;&gt;Customizing the build&lt;/h1&gt;

&lt;p&gt;I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal.&lt;/p&gt;

&lt;p&gt;To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well.&lt;/p&gt;

&lt;p&gt;Here’s a table of the new Travis configurations that will be added:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Compiler&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Library&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;C++ Standard&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Variant&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OS&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Architecture&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Job&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Documentation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Coverage&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Valgrind&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Address Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;UB Sanitizer&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;msvc 14.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MS STL&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Windows&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;icc 2021.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Bionic)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.8.5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 4.9.4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 5.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 6.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 7.5.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 8.4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 9.3.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc 10.2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gcc (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Focal)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 3.8.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Trusty)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 4.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 5.0.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11, 14&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 6.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14, 17&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 7.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 9.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 10.0.1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang 11.0.0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boost&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;clang (trunk)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;libstdc++&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;17, 2a&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Standalone&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Linux (Xenial)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;x86-64&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;—&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge.&lt;/p&gt;

&lt;h1 id=&quot;binary-size&quot;&gt;Binary size&lt;/h1&gt;

&lt;p&gt;It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables.&lt;/p&gt;

&lt;p&gt;Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much.&lt;/p&gt;

&lt;p&gt;I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration.&lt;/p&gt;

&lt;p&gt;Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb).&lt;/p&gt;

&lt;p&gt;Here is a boiled down version of how we currently perform the conversion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;double calculate_float(
    std::uint64_t mantissa, 
    std::uint32_t exponent, 
    bool sign)
{
    constexpr static double table[618] = 
    { 
        1e-308, 1e-307, 
        ..., 
        1e307, 1e308 
    };
    double power;
    if(exponent &amp;lt; -308 || exponent &amp;gt; 308)
        power = std::pow(10.0, exponent);
    else
        power = table[exponent + 308]
    double result = mantissa * power;
    return sign ? -result : result;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To further reduce the size of the binary, Peter suggested that we instead calculate &lt;code&gt;power&lt;/code&gt; as &lt;code&gt;10^floor(exponent / 8) * 10^(exponent mod 8)&lt;/code&gt;. Yes, the division operations there might look expensive, but any decent optimizing compiler will transform &lt;code&gt;exponent / 8&lt;/code&gt; to &lt;code&gt;exponent &amp;gt;&amp;gt; 3&lt;/code&gt;, and &lt;code&gt;exponent mod 8&lt;/code&gt; to &lt;code&gt;exponent &amp;amp; 7&lt;/code&gt;. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Reviewing the review The review period for Boost.JSON has come and gone, and we got some great feedback on the design of the library. Glancing over the results, it appears that the general mood was to accept the library. This doesn’t mean that there weren’t any problem areas – most notably the documentation, which often did contain the information people wanted, but it was difficult to find. Other points of contention were the use of a push parser as opposed to a pull parser, the use of double, uint64_t, and int64_t without allowing for users to change them, and the value conversion interface. Overall some very good points were made, and I’d like to thank everyone for participating in the review. Customizing the build I put a bit of work into improving our CI matrix, as it had several redundant configurations and did not test newer compiler versions (e.g. GCC 10, clang 11), nor did we have any 32-bit jobs. The most difficult thing about working on the build matrix is balancing how exhaustive it is with the turnaround time – sure, we could add 60 configurations that test x86, x86-64, and ARM on every major compiler version released since 2011, but the turnaround would be abysmal. To alleviate this, I only added 32-bit jobs for the sanitizers that use a recent version of GCC. It’s a less common configuration in the days of 64-bit universality, and if 64 bit works then it’s highly likely that 32 bit will “just work” as well. Here’s a table of the new Travis configurations that will be added: Compiler Library C++ Standard Variant OS Architecture Job — — — Boost Linux (Xenial) x86-64 Documentation gcc 8.4.0 libstdc++ 11 Boost Linux (Xenial) x86-64 Coverage clang 6.0.1 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 Valgrind clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 Address Sanitizer clang 11.0.0 libstdc++ 17 Boost Linux (Xenial) x86-64 UB Sanitizer msvc 14.1 MS STL 11, 14, 17 Boost Windows x86-64 — msvc 14.1 MS STL 17, 2a Standalone Windows x86-64 — msvc 14.2 MS STL 17, 2a Boost Windows x86-64 — msvc 14.2 MS STL 17, 2a Standalone Windows x86-64 — icc 2021.1 libstdc++ 11, 14, 17 Boost Linux (Bionic) x86-64 — gcc 4.8.5 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 4.9.4 libstdc++ 11 Boost Linux (Trusty) x86-64 — gcc 5.5.0 libstdc++ 11 Boost Linux (Xenial) x86-64 — gcc 6.5.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — gcc 7.5.0 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — gcc 8.4.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — gcc 9.3.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc 10.2.0 libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Boost Linux (Focal) x86-64 — gcc (trunk) libstdc++ 17, 2a Standalone Linux (Focal) x86-64 — clang 3.8.0 libstdc++ 11 Boost Linux (Trusty) x86-64 — clang 4.0.0 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 5.0.2 libstdc++ 11, 14 Boost Linux (Xenial) x86-64 — clang 6.0.1 libstdc++ 14, 17 Boost Linux (Xenial) x86-64 — clang 7.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 9.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 10.0.1 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang 11.0.0 libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Boost Linux (Xenial) x86-64 — clang (trunk) libstdc++ 17, 2a Standalone Linux (Xenial) x86-64 — I think it strikes a good balance between exhaustiveness and turnaround time, and we now test the most recent compiler versions to make sure they won’t cause problems on the cutting edge. Binary size It doesn’t matter how good a library is if it’s too big to use within your environment. As with all things in computer science, there is a trade-off between size and speed; seldom can you have both. We have been exploring options to reduce the size of the binary, and this mostly involved removing a lot of the pre-written tables we have (such as the ever-controversial jump table), since it allows the compiler to take into account the specific options it was past and optimize for those constraints (i.e. size and speed) rather than hard-coding in a set configuration as we did with the jump tables. Peter Dimov also helped out by transitioning our compile-time system of generating unique parse functions for each permutation of extensions to a runtime system, which drastically decreases the binary size without affecting performance too much. I must admit I’m not the biggest fan of these changes, but it’s important to support the use of Boost.JSON in embedded environments. As Peter has said time and time again: don’t overfit for a particular use-case or configuration. Another place with room for improvement is with string to float-point conversions. Right now we calculate a mantissa and base-10 exponent, then lookup the value in a massive table that contains pre-calculated powers of 10 from 1e-308 to 1e+308. As you can surmise, this takes up a substantial amount of space (8 bytes * 618 elements = 4.95 kb). Here is a boiled down version of how we currently perform the conversion: double calculate_float( std::uint64_t mantissa, std::uint32_t exponent, bool sign) { constexpr static double table[618] = { 1e-308, 1e-307, ..., 1e307, 1e308 }; double power; if(exponent &amp;lt; -308 || exponent &amp;gt; 308) power = std::pow(10.0, exponent); else power = table[exponent + 308] double result = mantissa * power; return sign ? -result : result; } To further reduce the size of the binary, Peter suggested that we instead calculate power as 10^floor(exponent / 8) * 10^(exponent mod 8). Yes, the division operations there might look expensive, but any decent optimizing compiler will transform exponent / 8 to exponent &amp;gt;&amp;gt; 3, and exponent mod 8 to exponent &amp;amp; 7. This does introduce another multiplication instruction, but at the same time, it makes our table 8 times smaller. In theory, the slight drop in performance is worth the significant reduction in binary size.</summary></entry><entry><title type="html">Krystian’s August Update</title><link href="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html" rel="alternate" type="text/html" title="Krystian’s August Update" /><published>2020-09-06T00:00:00+00:00</published><updated>2020-09-06T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/09/06/KrystiansAugustUpdate.html">&lt;h1 id=&quot;boostjson&quot;&gt;Boost.JSON&lt;/h1&gt;

&lt;p&gt;Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline.&lt;/p&gt;

&lt;h2 id=&quot;optimize-optimize-optimize&quot;&gt;Optimize, optimize, optimize&lt;/h2&gt;

&lt;p&gt;Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return &lt;code&gt;const char*&lt;/code&gt; instead of &lt;code&gt;result&lt;/code&gt; (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing &lt;code&gt;basic_parser&lt;/code&gt; was complete (for now…), save for a few more minor changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the &lt;code&gt;this&lt;/code&gt; pointer for &lt;code&gt;basic_parser&lt;/code&gt; is the &lt;code&gt;this&lt;/code&gt; pointer for the handler, which eliminates some register spills.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as &lt;code&gt;max_depth - actual_depth&lt;/code&gt;, meaning that we don’t have to read &lt;code&gt;max_depth&lt;/code&gt; from memory each time a structure is parsed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code&gt;parse_string&lt;/code&gt; was split into two functions: &lt;code&gt;parse_unescaped&lt;/code&gt; and &lt;code&gt;parse_escaped&lt;/code&gt;. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;the-dom-parser&quot;&gt;The DOM parser&lt;/h3&gt;

&lt;p&gt;Our old implementation of &lt;code&gt;parser&lt;/code&gt; was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects.&lt;/p&gt;

&lt;p&gt;Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a &lt;em&gt;lot&lt;/em&gt; of redundancy in the implementation. For example, &lt;code&gt;basic_parser&lt;/code&gt; already keeps track of the current object/array/string/key size, so there is no reason to so within &lt;code&gt;parser&lt;/code&gt;. The state information we were tracking was also not needed – &lt;code&gt;basic_parser&lt;/code&gt; already checks the syntactic correctness of the input. That left one more thing: strings and keys.&lt;/p&gt;

&lt;p&gt;My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from &lt;code&gt;basic_parser&lt;/code&gt;. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work.&lt;/p&gt;

&lt;p&gt;His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising.&lt;/p&gt;

&lt;h2 id=&quot;more-utf-8-validation-malarkey&quot;&gt;More UTF-8 validation malarkey&lt;/h2&gt;

&lt;p&gt;Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, &lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;an issue was opened&lt;/a&gt;; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called.&lt;/p&gt;

&lt;p&gt;This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% &lt;code&gt;// KRYSTIAN TODO: this can be optimized&lt;/code&gt;. The tests provided in the issue finally passed, so the patch was merged.&lt;/p&gt;

&lt;p&gt;I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/CPPAlliance/json/issues/162&quot;&gt;Handler not invoked correctly in multi-byte UTF8 sequences, part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">Boost.JSON Boost.JSON is officially scheduled for review! It starts on September 14th, so there isn’t much time left to finish up polishing the library – but it looks like we will make the deadline. Optimize, optimize, optimize Boost.JSON’s performance has significantly increased in the past month. The change to the parsing functions where we pass and return const char* instead of result (detailed in my last post) was merged, bringing large gains across the board. After this, my work on optimizing basic_parser was complete (for now…), save for a few more minor changes: The handler is stored as the first data member as opposed to passing a reference to each parse function. This means that the this pointer for basic_parser is the this pointer for the handler, which eliminates some register spills. The parser’s depth (i.e. nesting level of objects/arrays) is now tracked as max_depth - actual_depth, meaning that we don’t have to read max_depth from memory each time a structure is parsed. parse_string was split into two functions: parse_unescaped and parse_escaped. The former is much cheaper to call as it doesn’t have to store the string within a local buffer, and since unescaped strings are vastly more common in JSON documents, this increases performance considerably. The DOM parser Our old implementation of parser was pretty wasteful. It stored state information (such as whether we were parsing an object or array), keys, and values, all on one stack. This proved to be quite a pain when it came to unwinding it and also required us to align the stack when pushing arrays and objects. Several months ago, Vinnie and I tried to figure out how to make the homogeneous but came to a dead end. I decided to revisit the idea, and after some experimentation, it became apparent that there was a lot of redundancy in the implementation. For example, basic_parser already keeps track of the current object/array/string/key size, so there is no reason to so within parser. The state information we were tracking was also not needed – basic_parser already checks the syntactic correctness of the input. That left one more thing: strings and keys. My rudimentary implementation required two stacks: one for keys and strings, and the other for values. Other information, such as the sizes of objects and arrays, were obtained from basic_parser. My implementation, though primitive, gave some promising results on the benchmarks: up to 10% for certain documents. After some brainstorming with Vinnie, he had the idea of storing object keys as values; the last piece of the puzzle we needed to make this thing work. His fleshed-out implementation was even faster. In just a week’s time, Boost.JSON’s performance increased by some 15%. I’m still working on the finishing touches, but the results are looking promising. More UTF-8 validation malarkey Out of all the things I’ve worked on, nothing has proved as frustrating as UTF-8 validation. The validation itself is trivial; but making it work with an incremental parser is remarkably difficult. Shortly after merging the feature, an issue was opened; while validation worked just fine when a document was parsed without suspending, I neglected to write tests for incremental parsing, and that’s precisely where the bug was. Turns out, if parsing suspended while validating a UTF-8 byte sequence, the handler just would not be called. This was… quite a problem to say the least, and required me to reimplement UTF-8 validation from scratch – but with a twist. We don’t want to pass partial UTF-8 sequences because it just transfers the burden of assembling incomplete sequences to the handler. This means that we need to store the sequences, append to them until we get a complete codepoint, and only then can we validate and send it off to the handler. Doing this in an efficient manner proved to be quite challenging, so I ended up with a “fix” that was 50% code and 50% // KRYSTIAN TODO: this can be optimized. The tests provided in the issue finally passed, so the patch was merged. I thought my woes with validation were over, but I was wrong. Just over a week later, a new issue rolled in: Handler not invoked correctly in multi-byte UTF8 sequences, part 2 Luckily, fixing this didn’t require another rewrite. This taught me a fine lesson in exhaustive testing.</summary></entry><entry><title type="html">Richard’s August Update</title><link href="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html" rel="alternate" type="text/html" title="Richard’s August Update" /><published>2020-09-01T00:00:00+00:00</published><updated>2020-09-01T00:00:00+00:00</updated><id>http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate</id><content type="html" xml:base="http://cppalliance.org/richard/2020/09/01/RichardsAugustUpdate.html">&lt;h1 id=&quot;new-debugging-feature-in-asio-and-beast&quot;&gt;New Debugging Feature in Asio and Beast&lt;/h1&gt;

&lt;p&gt;As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio.&lt;/p&gt;

&lt;p&gt;Support for this is not the only thing that is new in Beast.&lt;/p&gt;

&lt;p&gt;Chris Kohlhoff recently submitted a &lt;a href=&quot;https://github.com/boostorg/beast/pull/2053&quot;&gt;PR&lt;/a&gt; to Beast’s repository 
demonstrating how to annotate source code with the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro. I have since followed up and 
annotated all asynchronous operations in Beast this way.&lt;/p&gt;

&lt;p&gt;In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro 
&lt;code&gt;BOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt; will cause these macros to generate code which will emit handler tracking
log data to stdout in a very specific format.&lt;/p&gt;

&lt;p&gt;The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation
in linear terms. i.e. the asynchronous events are flattened and linked to show causality.&lt;/p&gt;

&lt;p&gt;Here is an example of the output:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@asio|1597543084.233257|&amp;gt;33|
@asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel
@asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait
@asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331)
@asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201)
@asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64)
@asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223)
@asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277)
@asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send
@asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103
@asio|1597543084.233949|&amp;lt;33|
@asio|1597543084.233983|&amp;lt;31|
@asio|1597543084.234031|&amp;gt;30|ec=system:89
@asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234054|&amp;gt;36|
@asio|1597543084.234064|&amp;lt;36|
@asio|1597543084.234072|&amp;lt;30|
@asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103
@asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234109|&amp;gt;37|
@asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel
@asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321)
@asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait
@asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373)
@asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168)
@asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212)
@asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297)
@asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101)
@asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive
@asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0
@asio|1597543084.234353|&amp;lt;37|
@asio|1597543084.234364|&amp;lt;35|
@asio|1597543084.234380|&amp;gt;34|ec=system:89
@asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute
@asio|1597543084.234401|&amp;gt;40|
@asio|1597543084.234408|&amp;lt;40|
@asio|1597543084.234416|&amp;lt;34|
@asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534
@asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far, so good. But not very informative or friendly to the native eye.&lt;/p&gt;

&lt;p&gt;Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source
tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as
PNG, BMP, SVG and many others.&lt;/p&gt;

&lt;p&gt;Here is an example of a visualisation of a simple execution graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/richard/2020-09-01-handler-tracking-example.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The tool you need to do this is in the &lt;code&gt;asio&lt;/code&gt; subproject of the Boost repo. The full path is 
&lt;code&gt;libs/asio/tools/handlerviz.pl&lt;/code&gt;. The command is self-documenting but for clarity, the process would be like this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Compile and link your program with the compiler flag &lt;code&gt;-DBOOST_ASIO_ENABLE_HANDLER_TRACKING&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;run your program, capturing stdout to a file (say &lt;code&gt;mylog.txt&lt;/code&gt;) (or you can pipe it to the next step)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;You should now be able to view your graph in a web browser, editor or picture viewer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The documentation for dot is &lt;a href=&quot;https://linux.die.net/man/1/dot&quot;&gt;here&lt;/a&gt; dot is usually available in the graphviz package 
of your linux distro/brew cask. Windows users can download an executable suite 
&lt;a href=&quot;https://www.graphviz.org/download/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your
handler locations to the graph output, you can do so by inserting the &lt;code&gt;BOOST_ASIO_HANDLER_LOCATION&lt;/code&gt; macro just before
each asynchronous suspension point (i.e. just before the call to &lt;code&gt;async_xxx&lt;/code&gt;). If you’re doing this in an Asio 
&lt;code&gt;coroutine&lt;/code&gt; (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the 
YIELD macro, for example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    ...

    // this marks a suspension point of the coroutine
    BOOST_ASIO_CORO_YIELD
    {
        // This macro creates scoped variables so must be in a private scope
        BOOST_ASIO_HANDLER_LOCATION((           // note: double open brackets
            __FILE__, __LINE__,                 // source location
            &quot;websocket::tcp::async_teardown&quot;    // name of the initiating function
        ));

        // this is the initiation of the next inner asynchronous operation
        s_.async_wait(
            net::socket_base::wait_read,
                beast::detail::bind_continuation(std::move(*this)));

        // there is an implied return statement here
    }

    ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When writing applications, people historically have used Continuation Passing Style when calling asynchronous 
operations, capturing a shared_ptr to the connection implementation in each handler (continuation).&lt;/p&gt;

&lt;p&gt;When using this macro in user code with written in continuation passing style, you might do so like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void send_request(http::request&amp;lt;http::string_body&amp;gt; req)
{
    send_queue_.push_back(std::move(req));
    if (!sending_)
    {
        sending_ = true;
        maybe_initiate_send();
    }
}

void my_connection_impl::maybe_initiate_send()
{
    if (send_queue_.empty())
    {
        sending_ = false;
        return;
    }

    // assume request_queue_ is a std::deque so elements will have stable addresses
    auto&amp;amp; current_request = request_queue_.front(); 

    BOOST_ASIO_HANDLER_LOCATION((
        __FILE__, __LINE__,
        &quot;my_connection_impl::maybe_initiate_send&quot;
    ));

    // suspension point

    boost::beast::http::async_write(stream_, current_request_, 
        [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t)
        {
            // continuation

            if (!ec)
            {
                self-&amp;gt;request_queue_.pop_front();
                self-&amp;gt;maybe_initiate_send();
            }
            else
            {
                // handle error
            }
        });
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking
state to be destroyed after the asynchronous initiation function but before the coroutine continuation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;namespace net = boost::asio;
namespace http = boost::beast::http;

auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    // suspension point coming up

    auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable));
    }
    auto results = co_await std::move(*oresults);

    auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;();
    {
        BOOST_ASIO_HANDLER_LOCATION((
            __FILE__, __LINE__,
            &quot;my_connection_impl::connect_and_send&quot;
        ));
        oconnect.emplace(net::async_connect(stream, results, net::use_awaitable));
    }
    auto ep = co_await *std::move(oconnect);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which might look a little unwieldy compared to the unannotated code, which could look like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;auto connect_and_send(
    boost::asio::ip::tcp::socket&amp;amp; stream, 
    std::string host, 
    std::string port, 
    http::request&amp;lt;http::string_body&amp;gt; req) 
-&amp;gt; net::awaitable&amp;lt;void&amp;gt;
{
    namespace net = boost::asio;
    
    auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor);

    auto ep = co_await net::async_connect(stream, 
                            co_await resolver.async_resolve(host, port, net::use_awaitable), 
                            net::use_awaitable);

    // ... and so on ...

}
&lt;/code&gt;&lt;/pre&gt;</content><author><name></name></author><category term="richard" /><summary type="html">New Debugging Feature in Asio and Beast As covered previously, Boost 1.74 brought an implementation of the new unified executors model to Boost.Asio. Support for this is not the only thing that is new in Beast. Chris Kohlhoff recently submitted a PR to Beast’s repository demonstrating how to annotate source code with the BOOST_ASIO_HANDLER_LOCATION macro. I have since followed up and annotated all asynchronous operations in Beast this way. In a normal build, there is no effect (and zero extra code generation). However, defining the preprocessor macro BOOST_ASIO_ENABLE_HANDLER_TRACKING will cause these macros to generate code which will emit handler tracking log data to stdout in a very specific format. The output is designed to describe the flow of asynchronous events in a format suitable for generating a visualisation in linear terms. i.e. the asynchronous events are flattened and linked to show causality. Here is an example of the output: @asio|1597543084.233257|&amp;gt;33| @asio|1597543084.233273|33|deadline_timer@0x7fa6cac25218.cancel @asio|1597543084.233681|33^34|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.233681|33^34|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233681|33^34|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233681|33^34|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233681|33^34|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233681|33^34|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233681|33*34|deadline_timer@0x7fa6cac25298.async_wait @asio|1597543084.233801|33^35|in 'basic_stream::async_write_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.233801|33^35|called from 'async_write' (../../../../../../boost/asio/impl/write.hpp:331) @asio|1597543084.233801|33^35|called from 'ssl::stream&amp;lt;&amp;gt;::async_write_some' (../../../../../../boost/asio/ssl/detail/io.hpp:201) @asio|1597543084.233801|33^35|called from 'http::async_write_some' (../../../../../../boost/beast/http/impl/write.hpp:64) @asio|1597543084.233801|33^35|called from 'http::async_write' (../../../../../../boost/beast/http/impl/write.hpp:223) @asio|1597543084.233801|33^35|called from 'http::async_write(msg)' (../../../../../../boost/beast/http/impl/write.hpp:277) @asio|1597543084.233801|33*35|socket@0x7fa6cac251c8.async_send @asio|1597543084.233910|.35|non_blocking_send,ec=system:0,bytes_transferred=103 @asio|1597543084.233949|&amp;lt;33| @asio|1597543084.233983|&amp;lt;31| @asio|1597543084.234031|&amp;gt;30|ec=system:89 @asio|1597543084.234045|30*36|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234054|&amp;gt;36| @asio|1597543084.234064|&amp;lt;36| @asio|1597543084.234072|&amp;lt;30| @asio|1597543084.234086|&amp;gt;35|ec=system:0,bytes_transferred=103 @asio|1597543084.234100|35*37|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234109|&amp;gt;37| @asio|1597543084.234119|37|deadline_timer@0x7fa6cac25298.cancel @asio|1597543084.234198|37^38|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:321) @asio|1597543084.234198|37^38|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234198|37^38|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234198|37^38|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234198|37^38|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234198|37*38|deadline_timer@0x7fa6cac25218.async_wait @asio|1597543084.234288|37^39|in 'basic_stream::async_read_some' (../../../../../../boost/beast/core/impl/basic_stream.hpp:373) @asio|1597543084.234288|37^39|called from 'ssl::stream&amp;lt;&amp;gt;::async_read_some' (../../../../../../boost/asio/ssl/detail/io.hpp:168) @asio|1597543084.234288|37^39|called from 'http::async_read_some' (../../../../../../boost/beast/http/impl/read.hpp:212) @asio|1597543084.234288|37^39|called from 'http::async_read' (../../../../../../boost/beast/http/impl/read.hpp:297) @asio|1597543084.234288|37^39|called from 'http::async_read(msg)' (../../../../../../boost/beast/http/impl/read.hpp:101) @asio|1597543084.234288|37*39|socket@0x7fa6cac251c8.async_receive @asio|1597543084.234334|.39|non_blocking_recv,ec=system:35,bytes_transferred=0 @asio|1597543084.234353|&amp;lt;37| @asio|1597543084.234364|&amp;lt;35| @asio|1597543084.234380|&amp;gt;34|ec=system:89 @asio|1597543084.234392|34*40|strand_executor@0x7fa6cac24bd0.execute @asio|1597543084.234401|&amp;gt;40| @asio|1597543084.234408|&amp;lt;40| @asio|1597543084.234416|&amp;lt;34| @asio|1597543084.427594|.39|non_blocking_recv,ec=system:0,bytes_transferred=534 @asio|1597543084.427680|&amp;gt;39|ec=system:0,bytes_transferred=534 So far, so good. But not very informative or friendly to the native eye. Fortunately as of Boost 1.74 there is a tool in the Asio source tree to convert this data into something consumable by the open source tool dot, which can then output the resulting execution graph in one of a number of common graphical formats such as PNG, BMP, SVG and many others. Here is an example of a visualisation of a simple execution graph: The tool you need to do this is in the asio subproject of the Boost repo. The full path is libs/asio/tools/handlerviz.pl. The command is self-documenting but for clarity, the process would be like this: Compile and link your program with the compiler flag -DBOOST_ASIO_ENABLE_HANDLER_TRACKING run your program, capturing stdout to a file (say mylog.txt) (or you can pipe it to the next step) handlerviz.pl &amp;lt; mylog.txt | dot -Tpng mygraph.png You should now be able to view your graph in a web browser, editor or picture viewer. The documentation for dot is here dot is usually available in the graphviz package of your linux distro/brew cask. Windows users can download an executable suite here. If you have written your own asynchronous operations to compliment Beast or Asio, or indeed you just wish you add your handler locations to the graph output, you can do so by inserting the BOOST_ASIO_HANDLER_LOCATION macro just before each asynchronous suspension point (i.e. just before the call to async_xxx). If you’re doing this in an Asio coroutine (not to be confused with C++ coroutines) then be sure to place the macro in curly braces after the YIELD macro, for example: ... // this marks a suspension point of the coroutine BOOST_ASIO_CORO_YIELD { // This macro creates scoped variables so must be in a private scope BOOST_ASIO_HANDLER_LOCATION(( // note: double open brackets __FILE__, __LINE__, // source location &quot;websocket::tcp::async_teardown&quot; // name of the initiating function )); // this is the initiation of the next inner asynchronous operation s_.async_wait( net::socket_base::wait_read, beast::detail::bind_continuation(std::move(*this))); // there is an implied return statement here } ... When writing applications, people historically have used Continuation Passing Style when calling asynchronous operations, capturing a shared_ptr to the connection implementation in each handler (continuation). When using this macro in user code with written in continuation passing style, you might do so like this: void send_request(http::request&amp;lt;http::string_body&amp;gt; req) { send_queue_.push_back(std::move(req)); if (!sending_) { sending_ = true; maybe_initiate_send(); } } void my_connection_impl::maybe_initiate_send() { if (send_queue_.empty()) { sending_ = false; return; } // assume request_queue_ is a std::deque so elements will have stable addresses auto&amp;amp; current_request = request_queue_.front(); BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::maybe_initiate_send&quot; )); // suspension point boost::beast::http::async_write(stream_, current_request_, [self = this-&amp;gt;shared_from_this()](boost::beast::error_code ec, std::size_t) { // continuation if (!ec) { self-&amp;gt;request_queue_.pop_front(); self-&amp;gt;maybe_initiate_send(); } else { // handle error } }); } If you’re using c++ coroutines it becomes a little more complicated as you want the lifetime of the tracking state to be destroyed after the asynchronous initiation function but before the coroutine continuation: namespace net = boost::asio; namespace http = boost::beast::http; auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); // suspension point coming up auto oresults = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::resolver::results_type&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oresults.emplace(resolver.async_resolve(host, port, net::use_awaitable)); } auto results = co_await std::move(*oresults); auto oconnect = std::optional&amp;lt;net::awaitable&amp;lt;net::ip::tcp::endpoint&amp;gt;&amp;gt;(); { BOOST_ASIO_HANDLER_LOCATION(( __FILE__, __LINE__, &quot;my_connection_impl::connect_and_send&quot; )); oconnect.emplace(net::async_connect(stream, results, net::use_awaitable)); } auto ep = co_await *std::move(oconnect); // ... and so on ... } Which might look a little unwieldy compared to the unannotated code, which could look like this: auto connect_and_send( boost::asio::ip::tcp::socket&amp;amp; stream, std::string host, std::string port, http::request&amp;lt;http::string_body&amp;gt; req) -&amp;gt; net::awaitable&amp;lt;void&amp;gt; { namespace net = boost::asio; auto resolver = net::ip::tcp::resolver(co_await net::this_coro::executor); auto ep = co_await net::async_connect(stream, co_await resolver.async_resolve(host, port, net::use_awaitable), net::use_awaitable); // ... and so on ... }</summary></entry><entry><title type="html">Krystian’s July Update</title><link href="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html" rel="alternate" type="text/html" title="Krystian’s July Update" /><published>2020-08-01T00:00:00+00:00</published><updated>2020-08-01T00:00:00+00:00</updated><id>http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate</id><content type="html" xml:base="http://cppalliance.org/krystian/2020/08/01/KrystiansJulyUpdate.html">&lt;h1 id=&quot;what-ive-been-doing&quot;&gt;What I’ve been doing&lt;/h1&gt;

&lt;p&gt;I’ve been spending a &lt;em&gt;lot&lt;/em&gt; of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled).&lt;/p&gt;

&lt;h2 id=&quot;utf-8-validation&quot;&gt;UTF-8 validation&lt;/h2&gt;

&lt;p&gt;Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is &lt;em&gt;fast&lt;/em&gt;, it technically does not conform to the JSON standard.&lt;/p&gt;

&lt;p&gt;As per Section 2 of the &lt;a href=&quot;http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf&quot;&gt;JSON Data Interchange Syntax Standard&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post.&lt;/p&gt;

&lt;p&gt;After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional.&lt;/p&gt;

&lt;p&gt;Then came my favorite part: optimization.&lt;/p&gt;

&lt;p&gt;The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid.&lt;/p&gt;

&lt;p&gt;Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is &lt;code&gt;0xE1&lt;/code&gt;, then the byte sequence will be composed of three bytes, the latter two having a valid range of &lt;code&gt;0x80&lt;/code&gt; to &lt;code&gt;0xBF&lt;/code&gt;. Thus, our fast-path routine to verify this sequence can be written as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;uint32_t v;
// this is reversed on big-endian
std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load

switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit
{
...
case 3:
	if ((v &amp;amp; 0x00C0C000) == 0x00808000)
		return result::ok;
	return result::fail;
...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with &lt;code&gt;0xF0&lt;/code&gt; can have a second byte between &lt;code&gt;0x90&lt;/code&gt; and &lt;code&gt;0xBF&lt;/code&gt; which requires the check to be done as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;(v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It’s a weird little outlier that I spent way too much time trying to figure out.&lt;/p&gt;

&lt;p&gt;Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often.&lt;/p&gt;

&lt;h2 id=&quot;other-optimizations&quot;&gt;Other optimizations&lt;/h2&gt;

&lt;p&gt;I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a &lt;code&gt;const char*&lt;/code&gt; parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or &lt;code&gt;nullptr&lt;/code&gt; upon failure or partial parsing.&lt;/p&gt;

&lt;p&gt;Since I’m not great at explaining things, here’s the before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;result parse_array(const_stream&amp;amp;);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and here’s the after:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;const char* parse_array(const char*);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the &lt;code&gt;const_stream&lt;/code&gt; object at the top of the call stack (created within &lt;code&gt;basic_parser::write_some&lt;/code&gt;), nor read it each time a nested value is parsed. This yields an &lt;em&gt;8%&lt;/em&gt; boost in performance across the board.&lt;/p&gt;

&lt;p&gt;More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within &lt;code&gt;count_whitespace&lt;/code&gt;, we were able to get rid of a &lt;code&gt;_mm_cmpeq_epi8&lt;/code&gt; (&lt;code&gt;PCMPEQB&lt;/code&gt;) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with &lt;code&gt;'\r'&lt;/code&gt;, as the ASCII value of tab (&lt;code&gt;'\t'&lt;/code&gt;) only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers.&lt;/p&gt;

&lt;p&gt;For &lt;code&gt;count_unescaped&lt;/code&gt; (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the &lt;code&gt;_mm_min_epu8&lt;/code&gt; (&lt;code&gt;PMINUB&lt;/code&gt;) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the &lt;code&gt;strings.json&lt;/code&gt; benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark.&lt;/p&gt;

&lt;h2 id=&quot;the-important-but-boring-stuff&quot;&gt;The important but boring stuff&lt;/h2&gt;

&lt;p&gt;After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.&lt;/p&gt;</content><author><name></name></author><category term="krystian" /><summary type="html">What I’ve been doing I’ve been spending a lot of time working on optimizing the parser; perhaps a bit too much. Nevertheless, it’s very enjoyable and in doing so I’ve learned more than I could hope to ever learn in school. In addition to the optimization, comment and trailing comma support finally got merged, and I implemented UTF-8 validation (enabled by default, but it can be disabled). UTF-8 validation Prior to implementing this extension (or rather, feature which can be disabled), the parser considers any character appearing within a string to be valid, so long as it wasn’t a control character or formed an illegal escape. While this is fast, it technically does not conform to the JSON standard. As per Section 2 of the JSON Data Interchange Syntax Standard: A conforming JSON text is a sequence of Unicode code points that strictly conforms to the JSON grammar defined by this specification. As with most standardese, this particular requirement for conformance is not outright stated, but rather implied. Anyways, that’s enough standardese talk for this post. After working on this parser so much, I’ve pretty much got the suspend/resume idiom we use nailed down, so integrating it with the string parsing function was trivial… the actual validation, not so much. I hadn’t the slightest clue about any of the terminology used in the Unicode standard, so it took a good couple of hours to point myself in the right direction. Anyways, a lot of Googling and a messy python script for generating valid and invalid byte sequences later, I had something functional. Then came my favorite part: optimization. The first byte within a UTF-8 byte sequence determines how many bytes will follow, as well as the valid ranges for these following bytes. Since this byte has such a large valid range, I settled on using a lookup table to check whether the first byte is valid. Luckily, the following bytes have ranges that can be trivially checked using a mask. For example, if the first byte is 0xE1, then the byte sequence will be composed of three bytes, the latter two having a valid range of 0x80 to 0xBF. Thus, our fast-path routine to verify this sequence can be written as: uint32_t v; // this is reversed on big-endian std::memcpy(&amp;amp;v, bytes, 4); // 4 bytes load switch (lookup_table[v &amp;amp; 0x7F]) // mask out the most significant bit { ... case 3: if ((v &amp;amp; 0x00C0C000) == 0x00808000) return result::ok; return result::fail; ... } This works well for all but one byte sequence combination. For whatever reason, UTF-8 byte sequences that start with 0xF0 can have a second byte between 0x90 and 0xBF which requires the check to be done as: (v &amp;amp; 0xC0C0FF00) + 0x7F7F7000 &amp;lt;= 0x00002F00 It’s a weird little outlier that I spent way too much time trying to figure out. Since our parser supports incremental parsing, we only take the fast path if the input stream has four or more bytes remaining. If this condition isn’t met, we have to check each byte individually. It’s slower, but shouldn’t happen often. Other optimizations I’ve been trying out a number of different optimizations to squeeze all the performance we can get out of the parser. Most recently, I rewrote the parser functions to take a const char* parameter indicating the start of the value, and return a pointer to the end of the value (if parsing succeeds) or nullptr upon failure or partial parsing. Since I’m not great at explaining things, here’s the before: result parse_array(const_stream&amp;amp;); and here’s the after: const char* parse_array(const char*); This allows us to keep the pointer to the current position in the stream entirely within the registers when parsing a document. Since the value is local to the function, the compiler no longer needs to write it to the const_stream object at the top of the call stack (created within basic_parser::write_some), nor read it each time a nested value is parsed. This yields an 8% boost in performance across the board. More time was spent optimizing the SSE2 functions used for parsing unescaped strings and whitespace as well. Within count_whitespace, we were able to get rid of a _mm_cmpeq_epi8 (PCMPEQB) instruction by performing a bitwise or with 4 after testing for spaces, and then comparing the result with '\r', as the ASCII value of tab ('\t') only differs from that of the carriage return by the third least significant bit. This was something that clang was doing for us, but it’s nice to implement it for all other compilers. For count_unescaped (used to parse unescaped strings), we were able to again reduce the length of the hot path, this time a bit more significantly. Instead of checking for control characters by means of relational comparison, we can instead check for quotes and backslash first, and once that’s done, the _mm_min_epu8 (PMINUB) instruction can be used to set all control characters (0 - 31) to 31, and then test for equality. This brought our performance on the strings.json benchmark past the 8 GB/s mark from around 7.7 GB/s. Combined with the optimization of how the stream pointer is passed around, we now hit just a hair under 8.5 GB/s on this benchmark. The important but boring stuff After merging the parser extensions, there was a bunch of housekeeping to do such as improving coverage and writing documentation. Though these are far from being my favorite tasks, they are integral to writing a good library, so it must be done. My initial approach to writing tests for the parser extensions was to run each test on every parser configuration we have, but this soon proved to be a nonoptimal approach when the time taken to run the test suite quadrupled. I ended up doing the right thing by making the tests more surgical in nature, and in doing so we even got 100% coverage on the parser.</summary></entry></feed>